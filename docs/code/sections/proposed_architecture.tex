\begin{figure*}[t]
    \centering
    \begin{subfigure}[c]{0.65\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Generator.png}
        \caption{Generator Network}
        \label{fig:gen_only}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[c]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/Critic.png}
        \caption{Critic Network}
        \label{fig:critic_only}
    \end{subfigure}
    
    \caption{Overall structure of the proposed networks used in this work. The larger Generator model is shown in (a), and the smaller Critic model is shown in (b).}
    \label{fig:gen_critic_structure_full}
\end{figure*}

\subsection{GAN Overview}
% GAN in general
Denoising EEG can be formulated as a conditional signal-to-signal reconstruction problem, in which a model learns to map a noisy EEG input to a clean EEG output. 
While this task can be addressed using conventional regression-based models trained with pointwise loss functions (e.g., L1 or L2 loss), such approaches tend to produce over-smoothed reconstructions that fail to preserve high-frequency temporal structures that are critical for EEG analysis.

Generative Adversarial Networks (GANs) provide a suitable framework for this task by modeling the conditional distribution of clean EEG signals given noisy observations. 
In a GAN-based denoising setup, a generator is trained to reconstruct clean EEG signals from noisy inputs, while a discriminator learns to distinguish between generated synthetic signals and ground-truth clean EEG. 
The adversarial objective encourages the generator to produce outputs that not only minimize reconstruction error, but also to consider the realism of the generated signals. 
This property is particularly important for EEG denoising, as the notion of a single “correct” clean signal is inherently ambiguous. Multiple plausible clean EEG realizations may exist for a given noisy input.

To capture the temporal patterns in EEG signals effectively, convolutional neural networks (CNNs) were used rather than fully connected layers. EEG signals exhibit strong local temporal correlations, and convolutions naturally exploit this locality by learning filters that detect meaningful patterns across short temporal windows. 
CNNs also reduce the number of parameters compared to fully connected layers, improving training stability and reducing the risk of overfitting, which is especially important when working with limited EEG datasets. 
Furthermore, convolutional architectures are well-suited to hierarchical feature extraction, allowing the model to capture both fine-grained and longer-range temporal dependencies essential for accurate denoising. 
The reduced parameter count also makes CNNs more suitable for deployment on hardware with limited memory for real-time EEG denoising.

Furthermore, GANs offer practical advantages in terms of computational efficiency. Once trained, the generator produces denoised EEG signals in a single forward pass, making GAN-based models suitable for real-time or near–real-time applications. 
These characteristics make GANs a suitable choice for wearable EEG motion artifact removal.

\subsection{Generator and Critic Structure}
Using a GAN-Based model, the generator model will be unique compared to the traditional models\cite{anotherPaper2020} that generates based on a seed. The generator will need to accept a complete noisy EEG as an input for it to fully denoise motion artifact, while the output will be the clean EEG. 
To implement this, compared to the usual generator architecture that only requires a decoder, the generator that will be implemented will also have an encoder to detect the patterns of the noisy EEG input to map into the clean EEG. 
The model comprises four primary components: the encoder (Layer A), the bottleneck (Layer B), the decoder (Layer C), and the output adjustment (Layer D).

Along with the generator model, the discriminator model will just follow the traditional pattern of using only an encoder\cite{anotherPaper2020}. 
For the purpose of training using multiple discriminators, the term "critic" will be adopted for the “n critic” training method. 
The model itself is relatively simpler than the generator, with an append module, encoder (Layer E), and output adjustment (Layer F).

The figures in Fig.\ref{fig:gen_critic_structure_full} depict the logical architecture utilized during training, explicitly showing Batch Normalization layers in the encoder, bottleneck, and decoder. 
However, it is important to note that for inference, these layers are mathematically fused with the preceding Convolution or Transposed Convolution operations. 
Thus, while visible here for structural clarity, the normalization parameters are effectively absorbed into the weights and biases during deployment.

\subsection{Data-Driven Model}
% Already will be mentioned in intorduction, but will expand a little
% Explains that the model built can be used for variations of EEG artifacts
For the model to be flexible enough to distinguish EEG from a variety of artifacts which each having unique signal-value characteristics, the model was made to accept data with the general labels of “noisy EEG” and “clean EEG”.

Through this approach, the model can be trained to denoise any artifact, as long as a sufficient dataset exists for the target artifacts. The pure clean EEG itself can use the same data across any artifact variety. This is possible by artificially making noisy EEG from combining the raw artifacts and the raw EEG.

A benefit of this approach is that since the noisy EEG is artificially created, we can make variations using just a single pair of artifact and EEG so that the model can have plenty of data to be trained and tested on. This idea is an industry standard that has been tried from popular datasets like EEGdenoiseNet \cite{eegdenoise}.

\subsection{Training Setup}
\label{subsec:training_setup}
% Data and training setup
The main data used for this implementation is EEGdenoiseNet\cite{eegdenoise}. This dataset is a dataset of clean EEG, alongside pure EOG and EMG artifacts. 
The approach employed on this dataset being a weighted summation of the clean EEG and the selected artifacts.

This method offers several distinct advantages for our implementation. 
First, it resolves the issue in EEG manual denoising that produces different results based on the person performing it, this dataset provides a base truth of clean EEG and avoids that problem.
Second, the approach allows us to use any artifacts as long as a sufficient dataset exits, not just the artifacts provided in the original EEGdenoiseNet\cite{denoisenet}; This is suitable for our implementation that aims to be data-driven and flexible.
Finally, the weighted superimposition of artifacts serves as an effective data augmentation strategy, where varying the mixing weights significantly diversifies the training data.

To evaluate the model's suitability for hardware implementation, we utilized five distinct data configurations. 
The first two datasets, Data 1 (EEG + EOG) and Data 2 (EEG + EMG), strictly follow the generation protocols and sizes demonstrated in the original EEGdenoiseNet paper \cite{denoisenet}. 
To improve robustness, we generated three expanded custom datasets: Data 3 and Data 4, which mirror the EOG and EMG mixtures respectively but are scaled up to 10,000 training and 2,000 test samples. 
Finally, Data 5 combines all artifacts (EEG + EOG + EMG) with the same expanded sample size.

Mainly for hardware, we use Data 5 for our implementation. The other data modes serves as evaluation on how the model performs for different data.

\subsection{Training Procedure}
\label{subsec:training_procedure}
% Training specifics
The model was trained using the training and testing splits defined in Section \ref{subsec:training_setup}, employing the architecture illustrated in Fig.\ref{fig:gen_critic_structure_full}. We adopted the Conditional Wasserstein GAN with Gradient Penalty (cWGAN-GP) framework to ensure training stability and convergence.

Let $x$ denote the ground truth (clean EEG) and $y$ denote the input (noisy EEG). The generator $G$ learns a mapping $G(y) \rightarrow x$, while the critic $D$ is a conditional PatchGAN that discriminates between the real pair $(y, x)$ and the generated pair $(y, G(y))$.

The objective function for the critic is defined to approximate the Wasserstein distance between the real and generated distributions. To satisfy the 1-Lipschitz constraint required by WGAN, we apply a gradient penalty. The total critic loss $L_D$ is formulated as:

\vspace{-1.0em}
\begin{equation}
\begin{split}
    L_D = & \underbrace{\mathbb{E}[D(y, G(y))] - \mathbb{E}[D(y, x)]}_{\text{Wasserstein Estimate}} \\
    & + \lambda_{gp} \underbrace{\mathbb{E}_{\hat{x}} [(\|\nabla_{\hat{x}} D(y, \hat{x})\|_2 - 1)^2]}_{\text{Gradient Penalty}}
\end{split}
\label{eq:critic_loss}
\end{equation}

\noindent
where $\lambda_{gp}$ is the penalty coefficient, and $\hat{x}$ represents random samples interpolated between the real samples $x$ and generated samples $G(y)$.

The generator is trained to simultaneously fool the critic and reconstruct the clean signal. The total generator loss $L_G$ is a weighted sum of the adversarial loss and the L1 reconstruction loss:

\vspace{-1.0em}
\begin{equation}
    L_G = \underbrace{-\mathbb{E}[D(y, G(y))]}_{\text{Adversarial Loss}} + \alpha_{rec} \underbrace{\| G(y) - x \|_1}_{\text{Reconstruction Loss}}
\end{equation}

\noindent
where $\alpha_{rec}$ controls the weight of the reconstruction term.

We implemented the framework using the PyTorch library. The network parameters were optimized using the Adam optimizer for both the generator and the critic, configured with a learning rate of $1 \times 10^{-4}$ and momentum terms $\beta_1 = 0.0$ and $\beta_2 = 0.9$. The training process spanned 50 epochs with a batch size of 16 samples. To ensure stable convergence, we updated the critic $n_{critic}=5$ times for every single generator update. Regarding the loss balancing, the hyperparameters were set to $\lambda_{gp} = 10.0$ for the gradient penalty and $\alpha_{rec} = 10.0$ for the reconstruction term.

\subsection{Normalization Strategy}
% BN over GN motivation
% Design level table
To effectively stabilize model training, normalization is needed to control training values to make learning rate effective. 
For EEG datasets, the usual choice for normalization would be GroupNorm\cite{somepaper} as it normalizes within a sample's channels to capture the relatively lower variance of data within a single sample which is more stable.

However, for our implementation, BatchNorm was used. 
BatchNorm has the exact same normalization equation as GroupNorm, with the biggest difference being that it gathers statistics from different samples. 
This is not ideal for EEG datasets that usually have high variance across different samples. 
Despite that, for the model implementation, BatchNorm brings considerable benefits. 
One of which is the unique trait that BatchNorm statistics are constant after training and is utilized to fuse normalization with Convolution as explained in the next section.
\begin{table}[H] % <--- The H forces it to stay right here
    \centering
    \caption{Comparison of GroupNorm and BatchNorm for Hardware Implementation}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Normalization} & \textbf{Runtime Ops} & \textbf{Folding} & \textbf{HW Cost} \\
        \midrule
        GroupNorm & Yes & No  & High \\
        BatchNorm & No  & Yes & Low \\
        \bottomrule
    \end{tabular}
    \label{tab:norm_comparison_logic} % <--- Label must be AFTER caption
\end{table}
Aside from that, we have tested the performance between GroupNorm and BatchNorm, results can be seen in Fig. \ref{fig:bn_vs_gn}. 
From those results it can be seen that BatchNorm does not bring much detriment to the model's performance.

\subsection{BatchNorm Fusion}
% Mathematical derivation and just explain why
As mentioned earlier, Batch Normalization utilizes fixed statistics (mean and variance) derived from training. 
In the generator architecture, these normalization blocks invariably follow a linear projection (whether standard or transposed convolution), allowing them to be absorbed into the preceding operation.

Given a layer output z, the normalization process for each channel is defined by:

\begin{equation}
\mathrm{BN}(z) = \gamma \cdot \frac{z - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
\label{eq:batchnorm}
\end{equation}
where $\mu$ and $\sigma^2$ denote the running mean and variance, $\epsilon$ is a small constant for numerical stability, and $\gamma$, $\beta$ are learnable parameters.

A convolution layer computes:
\begin{equation}
z = \sum_i w_i x_i + b
\label{eq:conv_equation}
\end{equation}
where $w_i$ are the convolution weights, $x_i$ are the input activations, and $b$ is the convolution bias.

By substituting the convolution expression into the Batch Normalization equation, we obtain:
\begin{equation}
\mathrm{BN}(x) =
\gamma \cdot
\frac{\left(\sum_i w_i x_i + b\right) - \mu}
{\sqrt{\sigma^2 + \epsilon}}
+ \beta
\label{eq:sub_conv_to_batchnorm}
\end{equation}

Rearranging and separating terms yields:
\begin{equation}
\mathrm{BN}(x) =
\sum_i
\left(
\frac{\gamma}{\sqrt{\sigma^2 + \epsilon}} w_i
\right) x_i
+
\left(
\frac{\gamma (b - \mu)}{\sqrt{\sigma^2 + \epsilon}} + \beta
\right)
\label{eq:fuse_batchnorm_rearrangement}
\end{equation}

We then define the fused convolution parameters as:
\begin{minipage}{.45\linewidth}
    \begin{equation}
        w_i' = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}} \cdot w_i
        \label{eq:fused_weight}
    \end{equation}
\end{minipage}%
\hfill
\begin{minipage}{.45\linewidth}
    \begin{equation}
        b' = \frac{\gamma (b - \mu)}{\sqrt{\sigma^2 + \epsilon}} + \beta
        \label{eq:fused_bias}
    \end{equation}
\end{minipage}
\\

Using the fused parameters, the convolution can be rewritten as:
\begin{equation}
y = \sum_i w_i' x_i + b'
\label{eq:fused_batch_norm_conv}
\end{equation}

Thus, the original convolution followed by Batch Normalization can be replaced by a single convolution with modified weights and bias, producing an identical output during inference.

From a hardware perspective, this fusion significantly reduces complexity by eliminating the need for dedicated normalization circuitry. The network effectively retains the benefits of normalization, yet requires no specific hardware logic to execute it.
To ensure that the model still performs well, an evaluation with the results shown on Fig. \ref{} was conducted.

\subsection{Fixed-Point Representation}
\label{subsec:fixed_point}
To optimize hardware resource utilization and minimize latency, our implementation uses fixed-point arithmetic for inference. We adopted the \textbf{Q9.14} format, which allocates 1 implicit sign bit, 9 bits for the integer part, and 14 bits for the fractional part, resulting in a total bit-width of 24 bits.

This format provides a dynamic range of approximately $\pm 512$ with a resolution of $2^{-14}$ ($\approx 6.1 \times 10^{-5}$). Our range analysis on intermediate network activations indicated that values rarely exceed a magnitude of 10. Consequently, the chosen integer width provides ample headroom to prevent overflow, while the 14-bit fractional precision is sufficient to preserve the model's accuracy.

The primary motivation for using fixed-point representation is hardware efficiency. Unlike floating-point arithmetic, which requires complex logic for exponent alignment and normalization, fixed-point operations can be realized using standard integer adders and multipliers. Furthermore, operations such as scaling and truncation can be efficiently handled in hardware through bit-slicing and wiring, significantly reducing the logic gate count.

The quantization process is modeled in software using Python. We initially compute network parameters in standard 32-bit floating-point (NumPy \texttt{float32}). These values are then quantized to the Q9.14 format and exported as hexadecimal strings into \texttt{.mem} files, which serve as the initialization memory for the Verilog testbench. 

To validate this approach, we compared the signal reconstruction quality of the fixed-point approximation against the original floating-point model. As illustrated in Fig. \ref{fig:quant_vs_float} (placeholder), the discrepancy between the two is negligible, confirming that the Q9.14 format maintains high fidelity without the computational overhead of floating-point arithmetic.
