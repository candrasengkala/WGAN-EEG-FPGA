\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{subcaption}
\usepackage{algpseudocode}
\usepackage{algorithm}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{float}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\raggedbottom
\usepackage[numbers]{natbib}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Area-Efficient Unified Systolic Array for GAN-Based EEG Artifact Removal\\
}

\author{\IEEEauthorblockN{Dharma Anargya Jowandy}
\IEEEauthorblockA{\textit{Electrical Engineering} \\
\textit{Bandung Institute of Technology}\\
Bandung, Indonesia \\
13223075@std.stei.itb.ac.id}
\and
\IEEEauthorblockN{Rizmi Ahmad Raihan}
\IEEEauthorblockA{\textit{Electrical Engineering} \\
\textit{Bandung Institute of Technology}\\
Bandung, Indonesia \\
13223051@std.stei.itb.ac.id}
 \and
\IEEEauthorblockN{Aryo Wisanggeni}
\IEEEauthorblockA{\textit{Informatics Engineering} \\
\textit{Bandung Institute of Technology}\\
Bandung, Indonesia \\
13523100@std.stei.itb.ac.id}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
}

\maketitle

\begin{abstract}
In this work, we present a specialized hardware architecture for EEG artifact removal based on a GAN-driven temporal U-Net model. The proposed design features a unified systolic array core capable of executing both one-dimensional convolution and transposed convolution, enabling efficient support for encoder–decoder architectures without runtime reconfiguration. To improve hardware efficiency, Batch Normalization is fused into convolution operations, eliminating inference-time overhead, and a Q9.14 fixed-point quantization scheme is adopted to reduce complexity while preserving numerical fidelity.

The architecture employs mode-multiplexed datapaths controlled by hierarchical finite state machines, removing the need for dynamic parameter loading. Implemented on the Pynq-Z1 platform, the unified accelerator achieves high hardware utilization with full DSP usage while consuming only 25\% of available Block RAM, drawing an estimated 0.793 W of on-chip power. Experimental evaluation against floating-point software models demonstrates high similarity indicated by Pearson correlation and low RRMSE across both convolution and transposed convolution layers. Using the EEGdenoiseNet benchmark with constructed ground truth, the system exhibits robust generalization across multiple artifact types, including EOG and EMG signals. These results validate the proposed architecture as an efficient solution for EEG denoising in embedded hardware environments. 
\end{abstract}

\begin{IEEEkeywords}
Generative Adversarial Network, Systolic Array, EEG artifact Removal
\end{IEEEkeywords}

\section{Introduction}
Electroencephalography (EEG) is a widely used method to record electrical activity of the brain, spanning applications from medical diagnostics and brain-computer interfaces (BCIs) to sleep analysis \cite{niedermeyer2011}, \cite{wolpaw2002bci}. However, a critical bottleneck in deploying EEG systems is the extraction of clean neural signals, which requires filtering out various noise sources and artifacts that often obscure pure brain activity \cite{uriguen2015artifact}. Traditionally, data denoising relies on manual expert analysis, which is labor-intensive, inherently subjective, and unscalable for real-time applications. While numerous machine learning models have emerged to automate this process \cite{jiang2019deep}, they face significant challenges: the lack of deterministic "ground truth" in manually cleaned datasets, the diverse characteristics of EEG artifacts \cite{islam2016artifact}, and the high computational power needed for state-of-the-art models \cite{roy2019deep} makes them unsuitable for resource-constrained edge devices. To address these problems, we propose a Generative Adversarial Network (GAN) driven temporal U-Net to perform signal reconstruction. The network is then implemented on hardware level to support on edge-device application.

Previous hardware implementations of U-Net such as \cite{10585793}, \cite{10965410}, \cite{posso2025realtimesemanticsegmentationaerial} typically instantiate separate modules for encoder and decoder paths, prioritizing algorithmic equivalence over architectural considerations. This result in poor scalability and excessive resource consumption. For that reason, we propose a specialized systolic array architecture implemented in hardware, designed to deliver computational efficiency and scalability. Unlike prior implementations that often suffer from scalability issues or excessive resource consumption due to modular redundancy, our design introduces a unified systolic array core capable of executing both one-dimensional convolution and transposed convolution without runtime reconfiguration.

Our approach leverages a data-driven model capable of generalizing across diverse artifacts, adopting the EEGdenoiseNet \cite{zhang2021eegdenoisenetbenchmarkdatasetendtoend} methodology to resolve consistency issues through constructed ground truth. To bridge the gap between model complexity and hardware constraints, we implement a suite of software-guided optimizations, including Batch Normalization fusion \cite{jacob2018quantization} and fixed-point quantization \cite{jacob2018quantization}, \cite{sze2017efficient}. These enhancements significantly improve hardware efficiency by eliminating inference-time overhead without compromising model accuracy. This paper is organized as follows. Section II discusses the proposed design including algorithm and architectural optimization. Section III discusses hardware architecture and related works,  Section IV discusses implementation, and Section V analyzes the experimental results and performance comparison. Finally, conclusions are drawn in Section IV.  
\section{Proposed Architecture}
\begin{figure*}[t]
    \centering
    \begin{subfigure}[c]{0.65\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Generator.png}
        \caption{Generator Network}
        \label{fig:gen_only}
    \end{subfigure}
    \hfill 
    \begin{subfigure}[c]{0.33\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Critic.png}
        \caption{Critic Network}
        \label{fig:critic_only}
    \end{subfigure}
    
    \caption{Overall structure of the proposed networks used in this work. The larger Generator model is shown in (a), and the smaller Critic model is shown in (b).}
    \label{fig:gen_critic_structure_full}
\end{figure*}

\subsection{GAN Overview}
% GAN in general
Denoising EEG is effectively formulated as a conditional signal-to-signal reconstruction problem. 
While conventional regression models (using L1/L2 loss) often yield over-smoothed results, Generative Adversarial Networks (GANs) overcome this by modeling the conditional distribution of clean signals. 
The adversarial objective forces the generator to preserve high-frequency structures and realistic signal characteristics, handling the inherent ambiguity of reconstructing clean EEG from noisy observations.

To capture the strong local temporal correlations in EEG, we employ Convolutional Neural Networks (CNNs) rather than fully connected layers. 
CNNs naturally exploit temporal locality through learnable filters and enable hierarchical feature extraction for both fine-grained and long-range dependencies. 
Additionally, their parameter efficiency reduces the risk of overfitting and memory usage, making them ideal for hardware deployment. Once trained, the generator performs denoising in a single forward pass, ensuring the low latency required for real-time applications.

\subsection{Generator and Critic Structure}
Using a GAN-Based model, the generator model will be unique compared to the traditional models \cite{goodfellow2014gan} that generate based on a seed. The generator will need to accept a complete noisy EEG as an input to fully denoise motion artifact, while the output will be the clean EEG. 

To implement this, compared to the usual generator architecture that only requires a decoder \cite{radford2015dcgan}, the generator that will be implemented will also have an encoder to detect the patterns of the noisy EEG input to map into the clean EEG. 
The model comprises four primary components: the encoder (Layer A), the bottleneck (Layer B), the decoder (Layer C), and the output adjustment (Layer D).

Along with the generator model, the discriminator model will follow the traditional pattern of using only an encoder \cite{goodfellow2014gan}. 
For the purpose of training using multiple discriminators, the term "critic" will be adopted for the “n critic” training method. 
The model itself is relatively simpler than the generator, with an append module, encoder (Layer E), and output adjustment (Layer F).

The figures in Fig.\ref{fig:gen_critic_structure_full} depict the logical architecture utilized during training, explicitly showing Batch Normalization layers in the encoder, bottleneck, and decoder. 
However, it is important to note that for inference, these layers are mathematically fused with the preceding Convolution or Transposed Convolution operations. 
Thus, while visible here for structural clarity, the normalization parameters are effectively absorbed into the weights and biases during deployment.

\subsection{Data Strategy and Training Setup}
\label{subsec:data_strategy}

To ensure robust generalization, we adopted a data-driven approach using the EEGdenoiseNet benchmark \cite{zhang2021eegdenoisenetbenchmarkdatasetendtoend}. By synthetically superimposing artifacts onto clean EEG, we establish an objective ground truth, eliminating manual labeling variability while utilizing weighted mixing as an effective data augmentation strategy. This framework is highly extensible, allowing the model to learn to suppress any artifact type provided a corresponding data set exists.

We constructed five different dataset configurations for evaluation. Data 1 (EEG+EOG) and Data 2 (EEG+EMG) adhere to the original EEGdenoiseNet protocols \cite{zhang2021eegdenoisenetbenchmarkdatasetendtoend}. To improve model robustness, we generated expanded datasets—Data 3 (EOG) and Data 4 (EMG)—scaled to 10,000 training and 2,000 test samples. Finally, Data 5 combines all artifacts (EEG+EOG+EMG) at the expanded scale. 

Our hardware implementation primarily utilizes Data 5 to handle complex multi-artifact scenarios, while the other subsets serve as baselines for performance benchmarking.

\subsection{Training Procedure}
\label{subsec:training_procedure}
The model was trained using the training and testing splits defined in Section \ref{subsec:data_strategy}, employing the architecture illustrated in Fig.\ref{fig:gen_critic_structure_full}. We adopted the Conditional Wasserstein GAN with Gradient Penalty (cWGAN-GP) framework to ensure training stability and convergence.

Let $x$ denote the ground truth (clean EEG) and $y$ denote the input (noisy EEG). The generator $G$ learns a mapping $G(y) \rightarrow x$, while the critic $D$ is a conditional PatchGAN that discriminates between the real pair $(y, x)$ and the generated pair $(y, G(y))$.

The objective function for the critic is defined to approximate the Wasserstein distance between the real and generated distributions. To satisfy the 1-Lipschitz constraint required by WGAN, we apply a gradient penalty. The total critic loss $L_D$ is formulated as:

\vspace{-1.0em}
\begin{equation}
\begin{split}
    L_D = & \underbrace{\mathbb{E}[D(y, G(y))] - \mathbb{E}[D(y, x)]}_{\text{Wasserstein Estimate}} \\
    & + \lambda_{gp} \underbrace{\mathbb{E}_{\hat{x}} [(\|\nabla_{\hat{x}} D(y, \hat{x})\|_2 - 1)^2]}_{\text{Gradient Penalty}}
\end{split}
\label{eq:critic_loss}
\end{equation}

\noindent
where $\lambda_{gp}$ is the penalty coefficient, and $\hat{x}$ represents random samples interpolated between the real samples $x$ and the generated samples $G(y)$.

The generator is trained to simultaneously fool the critic and reconstruct the clean signal. The total generator loss $L_G$ is a weighted sum of the adversarial loss and the L1 reconstruction loss:

\vspace{-1.0em}
\begin{equation}
    L_G = \underbrace{-\mathbb{E}[D(y, G(y))]}_{\text{Adversarial Loss}} + \alpha_{rec} \underbrace{\| G(y) - x \|_1}_{\text{Reconstruction Loss}}
\end{equation}

\noindent
where $\alpha_{rec}$ controls the weight of the reconstruction term.

We implemented the framework using the PyTorch library. The network parameters were optimized using the Adam optimizer for both the generator and the critic, configured with a learning rate of $1 \times 10^{-4}$ and momentum terms $\beta_1 = 0.0$ and $\beta_2 = 0.9$. The training process spanned 50 epochs with a batch size of 16 samples. To ensure stable convergence, we updated the critic $n_{critic}=5$ times for every single generator update. Regarding the loss balancing, the hyperparameters were set to $\lambda_{gp} = 10.0$ for the gradient penalty and $\alpha_{rec} = 10.0$ for the reconstruction term.

\subsection{Normalization Strategy}
\label{subsec:normalization_strat}
% BN over GN motivation
% Design level table
Normalization is essential to stabilize training dynamics. 
For EEG datasets, GroupNorm \cite{wu2018group} is typically preferred as it normalizes within individual samples to mitigate the high variance often observed across different subjects. 
However, our implementation utilizes Batch Normalization due to its significant advantages for hardware deployment.
Although BatchNorm gathers statistics across samples, which is theoretically less robust for EEG, it possesses the unique property of having constant statistics during inference.
This allows the normalization operations to be mathematically fused with the preceding convolution layers (as detailed in Subsection \ref{subsec:batchnorm_fusion}), thereby eliminating the runtime computational cost while still preserving the necessary training stability.
\begin{table}[H]
    \centering
    \caption{Comparison of GroupNorm and BatchNorm for Hardware Implementation}
    \begin{tabular}{lccc}
        \toprule
        \textbf{Normalization} & \textbf{Runtime Ops} & \textbf{Folding} & \textbf{HW Cost} \\
        \midrule
        GroupNorm & Yes & No  & High \\
        BatchNorm & No  & Yes & Low \\
        \bottomrule
    \end{tabular}
    \label{tab:norm_comparison_logic} 
\end{table}
Aside from that, we have tested the performance between GroupNorm and BatchNorm, results can be seen in Table. \ref{tab:norm_heatmap}. 
From those results it can be seen that BatchNorm does not bring much detriment to the model's performance.

\begin{figure*}[t]
    \centering
    \includegraphics[
        width=0.90\textwidth
    ]{AXON-Conv_Transconv_Unified_Core_Paper.drawio.png}
    \caption{Overall view of the WGAN-EEG FPGA accelerator. Systolic array is interfaced with different input buffers and output buffers based on mode given by the architecture FSM. Blue colored paths indicate transposed convolution paths while red colored paths indicate 1D convolution paths. Black colored paths are used by both convolution and transposed convolution operations. Thick lines indicate $Depth \times DW$ (Datawidth) wide signals (flattened BRAM input/output). All non-control modules are controlled by the control logic blocks.}
    \label{fig:Unified_Core} 
    \vspace{-15pt}
\end{figure*}

\subsection{BatchNorm Fusion}
\label{subsec:batchnorm_fusion}
As mentioned earlier, Batch Normalization utilizes fixed statistics (mean and variance) derived from training. 
In the generator architecture, these normalization blocks invariably follow a linear projection (whether standard or transposed convolution), allowing them to be absorbed into the preceding operation \cite{jacob2018quantization}.

Given a layer output z, the normalization process for each channel is defined by:

\begin{equation}
\mathrm{BN}(z) = \gamma \cdot \frac{z - \mu}{\sqrt{\sigma^2 + \epsilon}} + \beta
\label{eq:batchnorm}
\end{equation}
where $\mu$ and $\sigma^2$ denote the running mean and variance, $\epsilon$ is a small constant for numerical stability, and $\gamma$, $\beta$ are learnable parameters.

A convolution layer computes:
\begin{equation}
z = \sum_i w_i x_i + b
\label{eq:conv_equation}
\end{equation}
where $w_i$ are the convolution weights, $x_i$ are the input activations, and $b$ is the convolution bias.

By substituting the convolution expression into the Batch Normalization equation, we obtain:
\begin{equation}
\mathrm{BN}(x) =
\gamma \cdot
\frac{\left(\sum_i w_i x_i + b\right) - \mu}
{\sqrt{\sigma^2 + \epsilon}}
+ \beta
\label{eq:sub_conv_to_batchnorm}
\end{equation}

Rearranging and separating terms yields:
\begin{equation}
\mathrm{BN}(x) =
\sum_i
\left(
\frac{\gamma}{\sqrt{\sigma^2 + \epsilon}} w_i
\right) x_i
+
\left(
\frac{\gamma (b - \mu)}{\sqrt{\sigma^2 + \epsilon}} + \beta
\right)
\label{eq:fuse_batchnorm_rearrangement}
\end{equation}

We then define the fused convolution parameters as:
\begin{minipage}{.45\linewidth}
    \begin{equation}
        w_i' = \frac{\gamma}{\sqrt{\sigma^2 + \epsilon}} \cdot w_i
        \label{eq:fused_weight}
    \end{equation}
\end{minipage}%
\hfill
\begin{minipage}{.45\linewidth}
    \begin{equation}
        b' = \frac{\gamma (b - \mu)}{\sqrt{\sigma^2 + \epsilon}} + \beta
        \label{eq:fused_bias}
    \end{equation}
\end{minipage}
\\

Using the fused parameters, the convolution can be rewritten as:
\begin{equation}
y = \sum_i w_i' x_i + b'
\label{eq:fused_batch_norm_conv}
\end{equation}

Thus, the original convolution followed by Batch Normalization can be replaced by a single convolution with modified weights and bias, producing an identical output during inference.

From a hardware perspective, this fusion significantly reduces complexity by eliminating the need for dedicated normalization circuitry. The network effectively retains the benefits of normalization, yet requires no specific hardware logic to execute it.


\section{Hardware Architecture}

\subsection{Related Works} \label{subsec: related works}
Several related works on hardware implementations of U-Net focus primarily on algorithmic equivalence and functional correctness, rather than on explicit architectural design considerations. Implementations such as \cite{10965410} and \cite{posso2025realtimesemanticsegmentationaerial} rely on High-Level Synthesis (HLS) to translate the computational model into hardware, but provide limited architectural discussion on architectural aspects such as data flow organization, memory hierarchy, and resource reuse. In particular, \cite{10585793} adopts a direct algorithm-to-hardware mapping approach, where each network layer is instantiated as a dedicated hardware block, resulting in a one-layer–to–one-hardware mapping without further architectural optimization. While \cite{CHENG2022105492} includes architectural discussions, it does not address implementation considerations for transposed convolution operations or layers.

Furthermore, there is a lack of literature on hardware implementations of U-Net architectures for temporal data that employ both one-dimensional convolution and one-dimensional transposed convolution layers. To address this gap, we propose a unified architecture capable of efficiently support both convolutional and transposed convolutional operations within a temporal U-Net framework.


\subsection{Overview of Proposed Hardware Design}

\begin{figure*}[t]
    \centering
    \includegraphics[width=0.9\textwidth]{Matrix Multiplicator.jpg}
    \caption{AXON \cite{nayan2025axonnovelsystolicarray} output-stationary systolic array and processing element (PE). The $D$ array denotes diagonal processing units, while $H$ denotes horizontal units. The illustration shows a $4 \times 4$ array, whereas the proposed system employs a $16 \times 16$ array. Yellow paths indicate control paths. On systolic array illustration, black paths indicate weight and ifmap values propagation direction.}
    \label{fig:Matrix_Multiplicator}
    \vspace{-15pt}
\end{figure*}


Our design implements the generator part of the GAN network mentioned earlier. The design is based on the output-stationary AXON systolic array \cite{nayan2025axonnovelsystolicarray}, which is tailored specifically for both convolutional and transposed convolutional layers. For convolutional operations, the systolic array multiplies input feature maps (IFMAPs) and kernels, with IFMAP formation (\textit{im2col}) performed in the Programmable Logic. This \textit{im2col} approach reduces computation in the Processing System.

Our system implements the U-Net architecture directly within the system control logic. As a result, no convolution or transposed convolution parameters (e.g., padding, input length, kernel size) are provided by the Processing System (PS). Instead, these parameters are stored internally and managed by finite state machines (FSMs) that configure the system according to the required operation.

Consequently, the PS is only responsible for supplying input data (filter weights and input channels) and receiving the output results in a fixed and deterministic order. In general, three FSMs govern the overall operation of the system, as summarized below:

\begin{itemize}
    \item \textbf{System FSM}: Controls the execution of U-Net layer groups, with states corresponding to the encoder, bottleneck, decoder, and output stage (i.e., the final convolution layer at the U-Net tail). This FSM determines whether the system performs convolution or transposed convolution and selects the appropriate mode-multiplexed datapaths, as shown in Fig.~\ref{fig:Unified_Core}.
    
    \item \textbf{One-Dimensional Convolution FSM}: Controls all convolution operations, including the final tail layer. This FSM consists of two main components: a sequencer and a datapath controller. The sequencer manages convolution parameters (e.g., stride, padding, kernel size), initial input loading, layer transitions, and AXI-based data output and bias configuration. The datapath controller governs the computation flow for a single convolution layer.
    
    \item \textbf{One-Dimensional Transposed Convolution FSM}: Manages the transposed convolution control flow using a hierarchical scheduling mechanism that tightly coordinates data loading, address mapping, and computation. A global auto-scheduler controls layer and batch progression, while a main scheduler FSM issues deterministic control signals for weight loading, input feature map (ifmap) streaming, and diagonal Processing Element (PE) activation. Within this process, a Memory-Mapped-to-Image (MM2IM) mapper generates localized output addresses synchronized with the scheduler’s row- and tile-level execution. For each PE, the mapper computes a BRAM bank index and intra-bank address using fixed channel interleaving and temporal positioning. A Channel Map (CMAP) signal explicitly gates accumulation to ensure that only valid outputs trigger memory updates. This integrated control and mapping scheme aligns the systolic array’s inherent latency with multi-bank memory addressing, enabling deterministic and stall-free write-back cycles.
\end{itemize}

While AXON is originally optimized for two-dimensional convolution with a stride of 1, we generalize its usage to support one-dimensional convolution and transposed convolution operations with stride values greater than 1. To achieve this, we design an AXON-based unified core capable of supporting both operations.

The unified core employs a $16 \times 16$ AXON-based matrix systolic array with separate buffers, interfaces, and control units for convolution and transposed convolution modes. The $16 \times 16$ dimension is selected because 16 is the largest kernel size used in the U-Net architecture. For convolution operations, each kernel can therefore fit entirely within the systolic array, with smaller kernels zero-padded as needed. The same matrix multiplier is reused for the transposed convolution process by activating only the diagonal PEs, with input feature map values injected from the left-most PE (corresponding to coordinate $(0,0)$ in Fig.~\ref{fig:Matrix_Multiplicator}). The datapaths involved in convolution and transposed convolution operations are described in Sections~\ref{sec:DatapathConvolution} and~\ref{sec:DatapathTransposeConvolution}, respectively.

\subsection{Datapath for Convolution Process}
\label{sec:DatapathConvolution}

Convolution is performed in batches of 64 input channels. Upon completion of each batch, the system requests a new set of filter weights from the Processing System. A new weight batch is also requested after processing every group of 16 filters. The sequencer first waits for the AXI interface to confirm that all required input channels have been loaded into the BRAMs before requesting the initial weight batch. Subsequent weight batches are processed without reloading the input channels, making the design constrained by the available input BRAM capacity.

Each BRAM stores a subset of input channels according to the following pattern: BRAM[0] stores $\{\text{channel}[0], \text{channel}[16], \text{channel}[32]\}$, BRAM[1] stores $\{\text{channel}[1], \text{channel}[17], \text{channel}[33]\}$, and so on until the final BRAM. As a result, the number of operable input channels is limited by the temporal length of each channel. For correct operation, the system must satisfy the constraint
$num\_channels \times temporal\_length \leq BRAM\_depth \times num\_BRAMs$.

The \textit{im2col} is performed internally by streaming input channels through shift registers whose enable signals follow a shifting of predefined pattern. The enable signal is derived from a "ghost" shift register according to $en\_shift\_reg = en\_shift\_reg\_ghost[2 \times Dimension - 1 : Dimension]$ with ghost shift register initialized starting from the first padding value and is shifted every stride amount of input based on the overlap pattern. Specifically, an overlap of one sample set from $\text{en\_shift\_reg\_ghost}[\text{DW}]$ to $\text{en\_shift\_reg\_ghost}[\text{DW} - (\text{overlap} - 1)]$, where the overlap is defined as $overlap = \frac{kernel\_size}{stride}$ (i.e., all values on these indices are made one, shown in Table.~\ref{tab:timing_sequence}). As such, due the design, the convolution engine supports only configurations in which the kernel size divided by the stride yields an integer value. Additionally, the system cannot support kernel sizes larger than 16. Bias values are applied as initial output values, with all subsequent systolic array outputs accumulated onto these initialized values.

\begin{figure}[H]
    \centering
    \includegraphics[
        width=0.7\linewidth
    ]{AXON-Input_Buffer_Diagram.jpg}
    \caption{\textit{im2col} supporting hardware. The enable signals are shifted as shown in Table.~\ref{tab:timing_sequence}.}
    \label{fig:Input_Buffer}
\end{figure}

\begin{table}[H]
\caption{Timed Sequence for Data Streaming and Shift Register Enable (LSB denotes enable signal for 0th shift register in Fig.~\ref{fig:Input_Buffer})}
\label{tab:timing_sequence}
\centering
\small
\begin{tabular}{@{}ccc@{}}
\toprule
\textbf{Time} & \textbf{Data Streamed} & \textbf{Shift Register Enable} \\ \midrule
$t$           & 1                      & 00000001                       \\
$t+1$         & 2                      & 00000001                       \\
$t+2$         & 3                      & 00000011                       \\
$t+3$         & 4                      & 00000011                       \\
$t+4$         & 5                      & 00000110                       \\
$t+5$         & 6                      & 00000110                       \\
$t+6$         & 7                      & 00001100                       \\
$t+7$         & 8                      & 00001100                       \\
$t+8$         & 9                      & 00011000                       \\
$t+9$         & 10                     & 00011000                       \\ \bottomrule
\end{tabular}
\end{table}

After 16 ifmaps are evaluated, the system is then designed to start its calculation appropriately, according to last streamed value of an input channel. The input channel transition is done control algorithm accounting for "stacks" available on the BRAMs.

\subsection{Datapath for Transpose Convolution Process} 
\label{sec:DatapathTransposeConvolution}

In the implementation of the transpose convolution process, we utilize diagonal Processing Elements (PEs) in the proposed design. Unlike conventional Weight Stationary designs where data propagates through neighbor connections, this architecture injects inputs directly from on-chip BRAMs into each diagonal PE. The scheduler manages this injection via a deterministic staggered activation scheme, effectively bypassing the standard systolic propagation path. Weight addresses are generated through a wavefront pipeline with jumpstart logic, where each successive BRAM is activated one cycle after the previous, eliminating startup dead cycles and achieving full utilization within 16 clock cycles. Meanwhile, the ifmap counter activates only a single selected BRAM bank per pass, reducing dynamic power consumption during transpose convolution operation. To accommodate the resulting time-skewed output wavefront, the Memory-Mapped-to-Image (MM2IM) Mapper employs a linear FIFO delay that aligns write addresses with valid data arrival. Finally, the Accumulation Unit integrates bias addition within a pipelined read-modify-write cycle, using pre-loaded biases as initial partial sums to eliminate additional latency. The accumulation operates as a six-stage pipeline — latch, address setup, alignment, read return, accumulate, and write-back — hiding BRAM read latency while maintaining single-cycle throughput.

For the transpose convolution process, we implement the MM2IM mapper that operates synchronously with the scheduler and generates one mapping vector per cycle corresponding to the active transposed-convolution row and tile. Each processing element independently computes its target output channel and temporal position, which are then translated into a BRAM bank index and intra-bank address using fixed channel interleaving. The resulting CMAP signal explicitly gates accumulation, ensuring that only valid mapper outputs contribute to BRAM updates. This approach guarantees deterministic alignment between systolic array latency, accumulation timing, and multi-bank output memory addressing. The mapper supports four decoder layers with different output dimensions — ranging from 64 to 512 time steps and 128 down to 16 channels — through layer-aware combinational parameter selection that requires no reconfiguration overhead between layers. The MM2IM mapper algorithm is shown in Algorithm \ref{alg:mm2im}.

% \clearpage
\begin{algorithm}[H]
    \caption{MM2IM Mapper Algorithm}
    \label{alg:mm2im}
    \small % Shrinks font to prevent wrapping
    \begin{algorithmic}[1]
        \State \textbf{Input:} $layer_{id}, row_{id}, tile_{id}$ \textbf{Output:} $CMap, OMap$
        \Statex
        \Procedure{MM2IMMapper}{$layer_{id}, row_{id}, tile_{id}$}
            \State \textbf{load} $\{O_{h}, O_{c}, T_{max}\}$ for $layer_{id}$
            \For{$i = 0$ \textbf{to} $15$} \Comment{Iterate through PEs}
                \State $k \gets i \pmod 4, \ offset \gets \lfloor i/4 \rfloor$
                \State $ch \gets (tile_{id} \times 4) + offset, \ t_{pos} \gets base_{pos} + k$
                
                \State \textbf{let} $v \gets (tile_{id} < T_{max}) \land (ch < O_{h}) \land (0 \le t_{pos} < O_{h})$
                
                \If{$v$}
                    \State $bank \gets ch \pmod{16}, \ page \gets \lfloor ch/16 \rfloor$
                    \State $addr \gets (page \times O_{h}) + t_{pos}$
                    \State $CMap_{i} \gets \text{VAL}, \ OMap_{i} \gets \{bank, addr\}$ 
                \Else
                    \State $CMap_{i} \gets \text{INV}, \ OMap_{i} \gets \text{NULL}$
                \EndIf
            \EndFor
            \State \textbf{return} $CMap, OMap$
        \EndProcedure
    \end{algorithmic}
\end{algorithm}

\section{Implementation}
\subsection{Utilization Report}
We implement our design using Pynq-Z1. Our synthesis result is as follows: 

\begin{table}[H]
\centering
\caption{FPGA Resource Utilization}
\begin{tabular}{lrrr}
\hline
\textbf{Resource} & \textbf{Estimation} & \textbf{Available} & \textbf{Utilization (\%)} \\
\hline
LUT     & 27091 & 53200  & 50.92 \\
LUTRAM  & 537   & 17400  & 3.09  \\
FF      & 18735 & 106400 & 17.61 \\
BRAM    & 35    & 140    & 25.00 \\
DSP     & 220   & 220    & 100.00 \\
\hline
\end{tabular}
\label{tab:resource_utilization}
\end{table}
Table.~\ref{tab:resource_utilization} shows integrated Convolution and Transpose Convolution system utilization report on Pynq-Z1 board. Other implementation parameters are presented modularity. 
\subsection{Convolution System Implementation Parameters}
\subsubsection{Latency}
Convolution is done per "block" of 64 input channels with new filter weights reloaded after the process is completed or during filter change as the system operates 16 filters at the same time. As such, convolution latency can be measured as the amount of "blocks" needed for a convolution process. Time needed for an entire block of convolution is 0.00359ms for an entire process with 0.00343ms loading process time both without AXI interface assuming that \textbf{all} input channels are loaded. 
\subsubsection{Timing Constrain}
For the convolution process, timing is constrained at 12ns clock period with results as follows:
\begin{table}[H]
\centering
\caption{Convolution Timing Analysis Summary}
\begin{tabular}{lrrr}
\hline
\textbf{Metric} & \textbf{Setup} & \textbf{Hold} \\
\hline
Worst Negative Slack (WNS)        & 1.211 ns  & --- \\
Worst Hold Slack (WHS)             & ---       & 0.137 ns  \\
\hline
\end{tabular}
\label{tab:timing_analysis}
\end{table}

\textbf{All user specified timing constraints are met.}
\subsubsection{Power Consumption}
Power analysis result done after synthesis for convolution system: \textbf{0.577W}.

\subsection{Transpose Convolution Process}
\subsubsection{Latency}
For transpose convolutino process, latency is measured as the time needed for the entire decoder process to be done including AXI interface. The results are as follows:
\begin{table}[H]
\centering
\caption{Latency Summary Table}
\begin{tabular}{lll}
\hline
\textbf{Layer} & \textbf{Latency (cycles)} \\
\hline
d1    & 419311  \\
d2    & 353543  \\
d3    & 171100  \\
d4    & 107613  \\
\hline
TOTAL & 1051864 \\
\hline
\end{tabular}
\label{tab:latency_summary}
\end{table}
\subsubsection{Timing Constrain}
For the transpose convolution process, timing is constrained at 10ns clock period. Results as follows:
\begin{table}[H]
\centering
\caption{Transpose Convolution Timing Analysis Summary}
\begin{tabular}{lrrr}
\hline
\textbf{Metric} & \textbf{Setup} & \textbf{Hold} \\
\hline
Worst Negative Slack (WNS)        & 1.892 ns  & ---         \\
Worst Hold Slack (WHS)             & ---       & 0.045 ns  
\hline
\end{tabular}
\label{tab:timing_analysis}
\end{table}

\subsubsection{Power Consumption}
Power analysis result done after synthesis for transpose convolution system: \textbf{0.216W}.
\subsection{Analysis}
Compared to implementations \cite{10585793} and \cite{10965410}, our implementation results in lower power consumption and utilization. At the same time, latency and clock speed worsen. This is due to the fact that our design highly configurable and scalable as it doesn't implement U-Net as layer-by-layer algorithmic equivalence.
\begin{table}[H]
\centering
\caption{Performance Comparison}
\begin{tabular}{lrrr}
\hline
\textbf{Metric} & & \textbf{\cite{10965410}} & \textbf{\cite{10585793}} \\
                & \textbf{Current Work} & \textbf{(Rui Ma et al.)} & \textbf{(Khalil et al.)} \\
\hline
Target Device      & Xilinx Pynq-Z1 & Xilinx Artix-7 & Altera FPGA \\
Logic (LUT)        & 27,068         & 653,670        & 10,417      \\
Registers (FF)     & 18,759         & 923,059        & 2,657       \\
Arithmetic (DSP)   & 220            & 11,830         & 168         \\
Memory (BRAM)      & 35             & 38,108         & 173         \\
Est. Slices        & 29,295         & 1,374,810      & 19,807      \\
\hline
\end{tabular}
\label{tab:performance_comparison}
\end{table}
\section{Evaluation}
\label{sec:conv_evaluation}

To comprehensively evaluate the performance of the proposed EEG denoising model, two complementary evaluation metrics were employed: Pearson correlation accuracy (ACC) and relative root mean square error (RRMSE).

Pearson correlation accuracy (ACC) is used to evaluate how well the denoised signal preserves the temporal structure and morphology of the original clean EEG signal. By measuring the linear correlation between the reconstructed signal and the ground truth, Pearson ACC is insensitive to absolute amplitude scaling and instead focuses on waveform similarity.

Relative root mean square error (RRMSE), on the other hand, quantifies the absolute reconstruction error normalized by the energy of the clean signal. Unlike correlation-based metrics, RRMSE directly penalizes amplitude mismatches and residual noise, providing a complementary perspective on denoising performance.

\subsection{GroupNorm vs BatchNorm Comparison}

By experimenting with both normalization strategies, the following results were obtained.

\begin{table}[H]
    \centering
    \caption{Comparison of GroupNorm and BatchNorm performance.}
    \label{tab:norm_heatmap}
    \renewcommand{\arraystretch}{1.1} % Slightly reduced stretch to save space
    \vspace{-5pt}
    \begin{tabular}{c|cc|cc}
    \toprule
    \multirow{2}{*}{Data} &
    \multicolumn{2}{c|}{GroupNorm} &
    \multicolumn{2}{c}{BatchNorm} \\
    & RRMSE $\downarrow$ & ACC $\uparrow$ & RRMSE $\downarrow$ & ACC $\uparrow$ \\
    \midrule
    1 & \cellcolor{red!25}0.4562 & \cellcolor{green!25}0.8849 & \cellcolor{green!35}0.4243 & \cellcolor{green!40}0.8992 \\
    2 & \cellcolor{red!35}0.6604 & \cellcolor{red!30}0.7409 & \cellcolor{red!25}0.6308 & \cellcolor{red!20}0.7609 \\
    3 & \cellcolor{green!40}0.2805 & \cellcolor{green!45}0.9522 & \cellcolor{green!45}0.2665 & \cellcolor{green!50}0.9564 \\
    4 & \cellcolor{red!30}0.5815 & \cellcolor{red!20}0.7991 & \cellcolor{red!25}0.5594 & \cellcolor{red!15}0.8070 \\
    5 & \cellcolor{red!30}0.5900 & \cellcolor{red!20}0.7961 & \cellcolor{red!25}0.5592 & \cellcolor{red!15}0.8056 \\
    \bottomrule
    \end{tabular}
    \vspace{-10pt}
\end{table}

As discussed in Section~\ref{subsec:normalization_strat}, the test data were generated according to the selected data modes (1--5) alongside the training dataset. The numerical results demonstrate consistently satisfactory performance across all modes. Notably, Mode~5 includes both EEG and EOG artifacts, yet the model is still able to reliably detect and suppress noise, achieving Pearson correlation values exceeding 0.8, this demonstrates that BatchNorm is a valid replacement for GroupNorm.

In addition to numerical evaluation, Fig.~\ref{fig:model_out} illustrates representative examples comparing clean, noisy, and generated EEG signals from the software models using BatchNorm which shows that the generated EEG is very close the the original EEG without the noise.

\begin{figure}[H]
    \centering
    \vspace{-5pt}
    \includegraphics[width=0.85\linewidth]{model_out.png} % Rescaled
    \vspace{-5pt}
    \caption{Comparison of clean, noisy, and generated EEG signals.}
    \label{fig:model_out}
    \vspace{-10pt}
\end{figure}

\subsection{Hardware vs Software Model}

To validate the hardware implementation, we compare
the hardware output with the original software model output for ten different data using the evaluation metric ACC and RRMSE as mentioned above. 

To evaluate the hardware, we tested transpose convolution layer C4 followed by convolution layer D1 (see Fig.~\ref{fig:gen_critic_structure_full}). We provide a standalone output comparison for C4 to validate individual layer performance, followed by the integrated results of the C4-D1 pipeline.

\begin{table}[htbp]
\caption{TransConv(C4): Similarity Comparison between Hardware and Software Outputs}
\label{tab:d4_metrics}
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Seed ID} & \textbf{Mean RRMSE} & \textbf{Mean Correlation} \\ \midrule
1234             & 0.102               & 0.991                     \\
420              & 0.111               & 0.990                     \\
67               & 0.112               & 0.990                     \\
69               & 0.118               & 0.988                     \\
13523100         & 0.106               & 0.990                     \\
13223051         & 0.104               & 0.991                     \\
13223075         & 0.106               & 0.991                     \\
42               & 0.111               & 0.990                     \\
21               & 0.104               & 0.991                     \\
20               & 0.115               & 0.989                     \\ \midrule
\textbf{Overall Mean} & \textbf{0.109}      & \textbf{0.990}            \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
\caption{Conv(C4-D1): Similarity Comparison between Hardware and Software Outputs}
\label{tab:c4_d1_metrics}
\centering
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Seed ID} & \textbf{Mean RRMSE} & \textbf{Mean Correlation} \\ \midrule
1234             & 0.077               & 0.996                     \\
420              & 0.086               & 0.996                     \\
67               & 0.076               & 0.996                     \\
69               & 0.097               & 0.995                     \\
13523100         & 0.085               & 0.996                     \\
13223051         & 0.084               & 0.996                     \\
13223075         & 0.082               & 0.996                     \\
42               & 0.087               & 0.996                     \\
21               & 0.083               & 0.996                     \\
20               & 0.086               & 0.995                     \\ \midrule
\textbf{Overall Mean} & \textbf{0.084}      & \textbf{0.996}            \\ \bottomrule
\end{tabular}
\end{table}


The hardware implementation demonstrates high fidelity to the software model, achieving a high mean Pearson Correlation score of 0.990 and 0.996, and a low RRMSE of 0.109 and 0.084, respectively for Transpose Convolution and Convolution. These results confirm that quantization noise remains within acceptable limits even when implemented in hardware.

\section{Conclusion}

This work presented a specialized, unified systolic array architecture for the removal of EEG artifacts using a GAN-based model. We introduce a unified  hardware core capable of efficiently executing both 1D convolution and transposed convolution, effectively supporting temporal U-Net architectures. Our contributions include the validation of Batch Normalization fusion to eliminate runtime overhead and a hardware-efficient Q9.14 fixed-point design that fidelity (using RRMSE and ACC as metrics) while reducing complexity. The architecture utilizes mode-multiplexed datapaths controlled by hierarchical FSMs, eliminating the need for runtime parameter configuration. 

The implementation achieves exceptional fidelity compared to software-based golden models; experimental results demonstrate a high mean Pearson Correlation of 0.990 and 0.996, alongside low RRMSE scores of 0.109 and 0.084 for transposed convolution and convolution layers, respectively. These metrics confirm that the proposed hardware core maintains high signal integrity while providing the low-latency execution required for closed-loop brain-computer interfaces. Future iterations will explore dynamic precision scaling and multi-core synchronization to further extend the system’s capacity for high-density EEG arrays.

\bibliographystyle{IEEEtranN}
\bibliography{reference}

\end{document}
