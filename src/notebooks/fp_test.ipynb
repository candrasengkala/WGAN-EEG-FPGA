{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48f1f687",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd7928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31ec525",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e06e3bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV = \"local\" # or \"local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e9ea995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\data\\eeg_denoise_net\n",
      "EEG shape: (4514, 512)\n",
      "EOG shape: (3400, 512)\n",
      "EMG shape: (5598, 512)\n"
     ]
    }
   ],
   "source": [
    "if ENV == \"kaggle\":\n",
    "    EEG = np.load(\"/kaggle/input/eeg-denoise-net/EEG_all_epochs.npy\")\n",
    "    EOG = np.load(\"/kaggle/input/eeg-denoise-net/EOG_all_epochs.npy\")\n",
    "    EMG = np.load(\"/kaggle/input/eeg-denoise-net/EMG_all_epochs.npy\")\n",
    "else:\n",
    "    data_path = os.path.abspath(\"../../data/eeg_denoise_net/\")\n",
    "    print(data_path)\n",
    "    \n",
    "    EEG = np.load(os.path.join(data_path, \"EEG_all_epochs.npy\"))\n",
    "    EOG = np.load(os.path.join(data_path, \"EOG_all_epochs.npy\"))\n",
    "    EMG = np.load(os.path.join(data_path, \"EMG_all_epochs.npy\"))\n",
    "\n",
    "print(\"EEG shape:\", EEG.shape)\n",
    "print(\"EOG shape:\", EOG.shape)\n",
    "print(\"EMG shape:\", EMG.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b8ce29",
   "metadata": {},
   "source": [
    "# Data Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4395d0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rms(x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Root Mean Square (RMS) of a 1D signal.\n",
    "    Implements Formula (3).\n",
    "    \"\"\"\n",
    "    x = np.asarray(x)\n",
    "    return np.sqrt(np.mean(x ** 2))\n",
    "\n",
    "def snr_db(clean: np.ndarray, noise: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute SNR in decibels.\n",
    "    Implements Formula (2).\n",
    "    \"\"\"\n",
    "    return 10 * np.log10(rms(clean) / rms(noise))\n",
    "\n",
    "def compute_lambda(clean: np.ndarray, noise: np.ndarray, target_snr_db: float) -> float:\n",
    "    \"\"\"\n",
    "    Compute lambda such that the mixed signal has the desired SNR (in dB).\n",
    "    \"\"\"\n",
    "    return rms(clean) / rms(noise) * 10 ** (-target_snr_db / 10)\n",
    "\n",
    "def mix_signals(clean: np.ndarray, noise: np.ndarray, target_snr_db: float):\n",
    "    \"\"\"\n",
    "    Mix clean signal with noise at a target SNR (dB).\n",
    "    \n",
    "    Returns:\n",
    "        mixed_signal\n",
    "        lambda_used\n",
    "    \"\"\"\n",
    "    lam = compute_lambda(clean, noise, target_snr_db)\n",
    "    mixed = clean + lam * noise\n",
    "    return mixed, lam\n",
    "\n",
    "def noisy_eeg_eog(eeg: np.ndarray, eog: np.ndarray, target_snr_db: float):\n",
    "    \"\"\"\n",
    "    EEG contaminated by ocular artifacts (EOG).\n",
    "    \"\"\"\n",
    "    return mix_signals(eeg, eog, target_snr_db)\n",
    "\n",
    "def noisy_eeg_emg(eeg: np.ndarray, emg: np.ndarray, target_snr_db: float):\n",
    "    \"\"\"\n",
    "    EEG contaminated by myogenic artifacts (EMG).\n",
    "    \"\"\"\n",
    "    return mix_signals(eeg, emg, target_snr_db)\n",
    "\n",
    "def noisy_eeg_eog_emg(\n",
    "    eeg: np.ndarray,\n",
    "    eog: np.ndarray,\n",
    "    emg: np.ndarray,\n",
    "    target_snr_db: float,\n",
    "    eog_weight: float = 1.0,\n",
    "    emg_weight: float = 1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    EEG contaminated by both ocular (EOG) and myogenic (EMG) artifacts.\n",
    "    \n",
    "    eog_weight / emg_weight allow control of relative artifact dominance.\n",
    "    \"\"\"\n",
    "    combined_noise = eog_weight * eog + emg_weight * emg\n",
    "    return mix_signals(eeg, combined_noise, target_snr_db)\n",
    "\n",
    "def make_noisy_sample(\n",
    "    noisy_signal,\n",
    "    eeg_idx,\n",
    "    eog_idx=None,\n",
    "    emg_idx=None,\n",
    "    snr_db=None,\n",
    "    lambda_used=None\n",
    "):\n",
    "    return {\n",
    "        \"noisy\": noisy_signal,      # np.ndarray (512,)\n",
    "        \"eeg_idx\": eeg_idx,         # int\n",
    "        \"eog_idx\": eog_idx,         # int or None\n",
    "        \"emg_idx\": emg_idx,         # int or None\n",
    "        \"snr_db\": snr_db,           # float\n",
    "        \"lambda\": lambda_used       # float\n",
    "    }\n",
    "\n",
    "def sample_snr_uniform(low, high, rng):\n",
    "    return rng.uniform(low, high)\n",
    "\n",
    "def fixed_snr_list(snrs, rng):\n",
    "    return rng.choice(snrs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d70357b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_eeg_eog_paper(\n",
    "    EEG,\n",
    "    EOG,\n",
    "    seed=42,\n",
    "    mode=\"random\"  # \"exhaustive\" | \"random\"\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    eeg_train_idx = np.arange(3000)\n",
    "    eeg_test_idx  = np.arange(3000, 3400)\n",
    "\n",
    "    snr_grid = np.array([-7, -6, -5, -4, -3, -2, -1, 0, 1, 2])\n",
    "\n",
    "    train_samples = []\n",
    "    test_samples = []\n",
    "\n",
    "    if mode == \"exhaustive\":\n",
    "        # ---- training: 10x uniform SNR ----\n",
    "        for _ in range(10):\n",
    "            perm = rng.permutation(eeg_train_idx)\n",
    "            for i in perm:\n",
    "                snr = sample_snr_uniform(-7, 2, rng)\n",
    "                noisy, lam = noisy_eeg_eog(EEG[i], EOG[i], snr)\n",
    "\n",
    "                train_samples.append(\n",
    "                    make_noisy_sample(\n",
    "                        noisy,\n",
    "                        eeg_idx=i,\n",
    "                        eog_idx=i,\n",
    "                        snr_db=snr,\n",
    "                        lambda_used=lam\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # ---- testing: fixed SNR grid ----\n",
    "        for snr in snr_grid:\n",
    "            for i in eeg_test_idx:\n",
    "                noisy, lam = noisy_eeg_eog(EEG[i], EOG[i], snr)\n",
    "\n",
    "                test_samples.append(\n",
    "                    make_noisy_sample(\n",
    "                        noisy,\n",
    "                        eeg_idx=i,\n",
    "                        eog_idx=i,\n",
    "                        snr_db=snr,\n",
    "                        lambda_used=lam\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    elif mode == \"random\":\n",
    "        # ---- training: single pass, random SNR from grid ----\n",
    "        for i in eeg_train_idx:\n",
    "            snr = rng.choice(snr_grid)\n",
    "            noisy, lam = noisy_eeg_eog(EEG[i], EOG[i], snr)\n",
    "\n",
    "            train_samples.append(\n",
    "                make_noisy_sample(\n",
    "                    noisy,\n",
    "                    eeg_idx=i,\n",
    "                    eog_idx=i,\n",
    "                    snr_db=snr,\n",
    "                    lambda_used=lam\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ---- testing ----\n",
    "        for i in eeg_test_idx:\n",
    "            snr = rng.choice(snr_grid)\n",
    "            noisy, lam = noisy_eeg_eog(EEG[i], EOG[i], snr)\n",
    "\n",
    "            test_samples.append(\n",
    "                make_noisy_sample(\n",
    "                    noisy,\n",
    "                    eeg_idx=i,\n",
    "                    eog_idx=i,\n",
    "                    snr_db=snr,\n",
    "                    lambda_used=lam\n",
    "                )\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'exhaustive' or 'random'\")\n",
    "\n",
    "    return train_samples, test_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad1c55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_eeg_emg_paper(\n",
    "    EEG,\n",
    "    EMG,\n",
    "    seed=42,\n",
    "    mode=\"random\"  # \"exhaustive\" | \"random\"\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    eeg_indices = rng.choice(len(EEG), size=len(EMG), replace=True)\n",
    "    emg_indices = np.arange(len(EMG))\n",
    "\n",
    "    pairs = list(zip(eeg_indices, emg_indices))\n",
    "    rng.shuffle(pairs)\n",
    "\n",
    "    train_pairs = pairs[:5000]\n",
    "    test_pairs  = pairs[5000:5598]\n",
    "\n",
    "    snr_grid = np.array([-7, -6, -5, -4, -3, -2, -1, 0, 1, 2])\n",
    "\n",
    "    train_samples = []\n",
    "    test_samples = []\n",
    "\n",
    "    if mode == \"exhaustive\":\n",
    "        # ---- training: 10x ----\n",
    "        for _ in range(10):\n",
    "            for eeg_i, emg_i in train_pairs:\n",
    "                snr = sample_snr_uniform(-7, 2, rng)\n",
    "                noisy, lam = noisy_eeg_emg(EEG[eeg_i], EMG[emg_i], snr)\n",
    "\n",
    "                train_samples.append(\n",
    "                    make_noisy_sample(\n",
    "                        noisy,\n",
    "                        eeg_idx=eeg_i,\n",
    "                        emg_idx=emg_i,\n",
    "                        snr_db=snr,\n",
    "                        lambda_used=lam\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # ---- testing: fixed grid ----\n",
    "        for snr in snr_grid:\n",
    "            for eeg_i, emg_i in test_pairs:\n",
    "                noisy, lam = noisy_eeg_emg(EEG[eeg_i], EMG[emg_i], snr)\n",
    "\n",
    "                test_samples.append(\n",
    "                    make_noisy_sample(\n",
    "                        noisy,\n",
    "                        eeg_idx=eeg_i,\n",
    "                        emg_idx=emg_i,\n",
    "                        snr_db=snr,\n",
    "                        lambda_used=lam\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    elif mode == \"random\":\n",
    "        # ---- training ----\n",
    "        for eeg_i, emg_i in train_pairs:\n",
    "            snr = rng.choice(snr_grid)\n",
    "            noisy, lam = noisy_eeg_emg(EEG[eeg_i], EMG[emg_i], snr)\n",
    "\n",
    "            train_samples.append(\n",
    "                make_noisy_sample(\n",
    "                    noisy,\n",
    "                    eeg_idx=eeg_i,\n",
    "                    emg_idx=emg_i,\n",
    "                    snr_db=snr,\n",
    "                    lambda_used=lam\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # ---- testing ----\n",
    "        for eeg_i, emg_i in test_pairs:\n",
    "            snr = rng.choice(snr_grid)\n",
    "            noisy, lam = noisy_eeg_emg(EEG[eeg_i], EMG[emg_i], snr)\n",
    "\n",
    "            test_samples.append(\n",
    "                make_noisy_sample(\n",
    "                    noisy,\n",
    "                    eeg_idx=eeg_i,\n",
    "                    emg_idx=emg_i,\n",
    "                    snr_db=snr,\n",
    "                    lambda_used=lam\n",
    "                )\n",
    "            )\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'exhaustive' or 'random'\")\n",
    "\n",
    "    return train_samples, test_samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1068cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mix_custom(\n",
    "    EEG,\n",
    "    EOG=None,\n",
    "    EMG=None,\n",
    "    n_train=10000,\n",
    "    n_test=2000,\n",
    "    snr_range=(-7, 2),\n",
    "    seed=42,\n",
    "    eog_weight=1.0,\n",
    "    emg_weight=1.0\n",
    "):\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    samples = []\n",
    "\n",
    "    def make_one():\n",
    "        eeg_i = rng.integers(len(EEG))\n",
    "        snr = rng.uniform(*snr_range)\n",
    "\n",
    "        eog_i = None\n",
    "        emg_i = None\n",
    "\n",
    "        if EOG is not None and EMG is not None:\n",
    "            eog_i = rng.integers(len(EOG))\n",
    "            emg_i = rng.integers(len(EMG))\n",
    "            noisy, lam = noisy_eeg_eog_emg(\n",
    "                EEG[eeg_i],\n",
    "                EOG[eog_i],\n",
    "                EMG[emg_i],\n",
    "                snr,\n",
    "                eog_weight=eog_weight,\n",
    "                emg_weight=emg_weight\n",
    "            )\n",
    "\n",
    "        elif EOG is not None:\n",
    "            eog_i = rng.integers(len(EOG))\n",
    "            noisy, lam = noisy_eeg_eog(EEG[eeg_i], EOG[eog_i], snr)\n",
    "\n",
    "        elif EMG is not None:\n",
    "            emg_i = rng.integers(len(EMG))\n",
    "            noisy, lam = noisy_eeg_emg(EEG[eeg_i], EMG[emg_i], snr)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"At least one of EOG or EMG must be provided.\")\n",
    "\n",
    "        return make_noisy_sample(\n",
    "            noisy,\n",
    "            eeg_idx=eeg_i,\n",
    "            eog_idx=eog_i,\n",
    "            emg_idx=emg_i,\n",
    "            snr_db=snr,\n",
    "            lambda_used=lam\n",
    "        )\n",
    "\n",
    "    for _ in range(n_train + n_test):\n",
    "        samples.append(make_one())\n",
    "\n",
    "    return samples[:n_train], samples[n_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abd775e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_noisy_sample(sample, EEG):\n",
    "    \"\"\"\n",
    "    Normalize one noisy EEG sample according to EEGdenoiseNet protocol.\n",
    "    \n",
    "    Args:\n",
    "        sample: dict produced by make_noisy_sample\n",
    "        EEG: clean EEG array (for ground truth lookup)\n",
    "    \n",
    "    Returns:\n",
    "        normalized_sample (new dict)\n",
    "    \"\"\"\n",
    "    y = sample[\"noisy\"]\n",
    "    x = EEG[sample[\"eeg_idx\"]]\n",
    "\n",
    "    sigma_y = np.std(y)\n",
    "    if sigma_y == 0:\n",
    "        raise ValueError(\"Standard deviation of noisy signal is zero.\")\n",
    "\n",
    "    normalized_sample = sample.copy()\n",
    "    normalized_sample.update({\n",
    "        \"noisy_norm\": y / sigma_y,     # ŷ\n",
    "        \"clean_norm\": x / sigma_y,     # x̂\n",
    "        \"sigma_y\": sigma_y             # stored for rescaling later\n",
    "    })\n",
    "\n",
    "    return normalized_sample\n",
    "\n",
    "def normalize_dataset(samples, EEG):\n",
    "    \"\"\"\n",
    "    Normalize a list of noisy EEG samples.\n",
    "    \n",
    "    Args:\n",
    "        samples: list of noisy sample dicts\n",
    "        EEG: clean EEG array\n",
    "    \n",
    "    Returns:\n",
    "        list of normalized sample dicts\n",
    "    \"\"\"\n",
    "    return [normalize_noisy_sample(s, EEG) for s in samples]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e71207",
   "metadata": {},
   "source": [
    "# Data Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83e91144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 = EEG + EOG Paper\n",
    "# 2 = EEG + EMG Paper\n",
    "# 3 = Custom EEG + EOG\n",
    "# 4 = Custom EEG + EMG\n",
    "# else Custom EEG + EOG + EMG\n",
    "DATA_MODE = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "380303e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_MODE == 1: # EEG + EOG Paper\n",
    "    # --- generate noisy datasets (raw, unnormalized) ---\n",
    "    train_samples, test_samples = mix_eeg_eog_paper(\n",
    "        EEG=EEG,\n",
    "        EOG=EOG,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # --- normalize according to EEGdenoiseNet protocol ---\n",
    "    train_samples_norm = normalize_dataset(train_samples, EEG)\n",
    "    test_samples_norm  = normalize_dataset(test_samples, EEG)\n",
    "elif DATA_MODE == 2: # EEG + EMG Paper\n",
    "    # --- generate noisy datasets (raw, unnormalized) ---\n",
    "    train_samples, test_samples = mix_eeg_emg_paper(\n",
    "        EEG=EEG,\n",
    "        EMG=EMG,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # --- normalize ---\n",
    "    train_samples_norm = normalize_dataset(train_samples, EEG)\n",
    "    test_samples_norm  = normalize_dataset(test_samples, EEG)\n",
    "elif DATA_MODE == 3:\n",
    "    train_samples, test_samples = mix_custom(\n",
    "        EEG=EEG,\n",
    "        EOG=EOG,\n",
    "        n_train=10000,\n",
    "        n_test=2000,\n",
    "        snr_range=(-7, 2),\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    train_samples_norm = normalize_dataset(train_samples, EEG)\n",
    "    test_samples_norm  = normalize_dataset(test_samples, EEG)\n",
    "elif DATA_MODE == 4:\n",
    "    train_samples, test_samples = mix_custom(\n",
    "        EEG=EEG,\n",
    "        EMG=EMG,\n",
    "        n_train=10000,\n",
    "        n_test=2000,\n",
    "        snr_range=(-7, 2),\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    train_samples_norm = normalize_dataset(train_samples, EEG)\n",
    "    test_samples_norm  = normalize_dataset(test_samples, EEG)\n",
    "\n",
    "else:\n",
    "    train_samples, test_samples = mix_custom(\n",
    "        EEG=EEG,\n",
    "        EOG=EOG,\n",
    "        EMG=EMG,\n",
    "        n_train=10000,\n",
    "        n_test=2000,\n",
    "        snr_range=(-7, 2),\n",
    "        seed=42,\n",
    "        eog_weight=1.0,\n",
    "        emg_weight=1.0\n",
    "    )\n",
    "\n",
    "    train_samples_norm = normalize_dataset(train_samples, EEG)\n",
    "    test_samples_norm  = normalize_dataset(test_samples, EEG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b38140",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "078ccecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv1d -> BatchNorm1d -> Activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=15, s=2, p=7, bias=True, act=\"lrelu\"):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=bias)\n",
    "        self.norm = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        if act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act must be 'lrelu' or 'relu'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "class DeconvBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvTranspose1d -> BatchNorm1d -> Activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=4, s=2, p=1, bias=True, act=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=bias)\n",
    "        self.norm = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act must be 'relu' or 'lrelu'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.deconv(x)))\n",
    "\n",
    "\n",
    "class ResBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block: (Conv -> BN -> ReLU) x2 + skip\n",
    "    Keeps same channel count and length.\n",
    "    \"\"\"\n",
    "    def __init__(self, ch, k=7, p=3, bias=True):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv1d(ch, ch, kernel_size=k, stride=1, padding=p, bias=bias)\n",
    "        self.n1 = nn.BatchNorm1d(ch)\n",
    "        self.c2 = nn.Conv1d(ch, ch, kernel_size=k, stride=1, padding=p, bias=bias)\n",
    "        self.n2 = nn.BatchNorm1d(ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.n1(self.c1(x)))\n",
    "        h = self.n2(self.c2(h))\n",
    "        return F.relu(x + h)\n",
    "    \n",
    "class MultiScaleResBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale residual block: parallel conv branches (k=3,5,7) then fuse.\n",
    "    Keeps same channel count and length.\n",
    "    \"\"\"\n",
    "    def __init__(self, ch, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=3, padding=1, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.b5 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=5, padding=2, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.b7 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=7, padding=3, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=1, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.b3(x) + self.b5(x) + self.b7(x)\n",
    "        h = self.fuse(h)\n",
    "        return F.relu(x + h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db6f02e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN Generator (U-Net-ish + Res bottleneck)\n",
    "class GeneratorCNNWGAN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN U-Net-ish generator for EEG denoising (WGAN).\n",
    "    Input : (B, 1, 512) noisy_norm\n",
    "    Output: (B, 1, 512) clean_norm_hat\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=32, bottleneck_blocks=4, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.e1 = ConvBlock1D(1, base_ch,       k=16, s=2, p=7, bias=bias, act=\"lrelu\")      # 512 -> 256\n",
    "        self.e2 = ConvBlock1D(base_ch, base_ch*2, k=16, s=2, p=7, bias=bias, act=\"lrelu\")    # 256 -> 128\n",
    "        self.e3 = ConvBlock1D(base_ch*2, base_ch*4, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 128 -> 64\n",
    "        self.e4 = ConvBlock1D(base_ch*4, base_ch*8, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 64 -> 32\n",
    "\n",
    "        # Bottleneck\n",
    "        bn_ch = base_ch * 8\n",
    "        self.bottleneck = nn.Sequential(*[\n",
    "            ResBlock1D(bn_ch, k=7, p=3, bias=bias) for _ in range(bottleneck_blocks)\n",
    "        ])\n",
    "\n",
    "        # Decoder (concat doubles channels)\n",
    "        self.d1 = DeconvBlock1D(bn_ch, base_ch*4,   k=4, s=2, p=1, bias=bias, act=\"relu\")     # 32 -> 64\n",
    "        self.d2 = DeconvBlock1D(base_ch*8, base_ch*2, k=4, s=2, p=1,bias=bias, act=\"relu\")   # 64 -> 128\n",
    "        self.d3 = DeconvBlock1D(base_ch*4, base_ch,   k=4, s=2, p=1, bias=bias, act=\"relu\")   # 128 -> 256\n",
    "        self.d4 = DeconvBlock1D(base_ch*2, base_ch//2, k=4, s=2, p=1, bias=bias, act=\"relu\")  # 256 -> 512\n",
    "\n",
    "        # Head (linear output recommended for normalized signals)\n",
    "        self.out = nn.Conv1d(base_ch//2, 1, kernel_size=7, stride=1, padding=3, bias=bias)\n",
    "\n",
    "    def forward(self, y):\n",
    "        # Encoder\n",
    "        s1 = self.e1(y)   # (B, base, 256)\n",
    "        s2 = self.e2(s1)  # (B, 2b, 128)\n",
    "        s3 = self.e3(s2)  # (B, 4b, 64)\n",
    "        s4 = self.e4(s3)  # (B, 8b, 32)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(s4)\n",
    "\n",
    "        # Decoder + skip connections\n",
    "        d1 = self.d1(b)                  # (B, 4b, 64)\n",
    "        d1 = torch.cat([d1, s3], dim=1)  # (B, 8b, 64)\n",
    "\n",
    "        d2 = self.d2(d1)                 # (B, 2b, 128)\n",
    "        d2 = torch.cat([d2, s2], dim=1)  # (B, 4b, 128)\n",
    "\n",
    "        d3 = self.d3(d2)                 # (B, b, 256)\n",
    "        d3 = torch.cat([d3, s1], dim=1)  # (B, 2b, 256)\n",
    "\n",
    "        d4 = self.d4(d3)                 # (B, b/2, 512)\n",
    "\n",
    "        return self.out(d4)              # (B, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c375ef13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch Critic (shared by CNN/ResCNN)\n",
    "class CriticPatch1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional PatchGAN critic for WGAN:\n",
    "      D(y, x) -> patch scores\n",
    "    y,x: (B,1,512)\n",
    "    output: (B,1,32)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=32, bias=True):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv1d(2, base_ch, kernel_size=16, stride=2, padding=7, bias=bias)  # 512 -> 256\n",
    "        self.c2 = ConvBlock1D(base_ch, base_ch*2, k=16, s=2, p=7, bias=bias, act=\"lrelu\")    # 256 -> 128\n",
    "        self.c3 = ConvBlock1D(base_ch*2, base_ch*4, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 128 -> 64\n",
    "        self.c4 = ConvBlock1D(base_ch*4, base_ch*8, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 64 -> 32\n",
    "        self.out = nn.Conv1d(base_ch*8, 1, kernel_size=7, stride=1, padding=3, bias=bias)   # 32 -> 32\n",
    "\n",
    "    def forward(self, y, x):\n",
    "        h = torch.cat([y, x], dim=1)  # (B,2,512)\n",
    "        h = F.leaky_relu(self.c1(h), 0.2, inplace=True)\n",
    "        h = self.c2(h)\n",
    "        h = self.c3(h)\n",
    "        h = self.c4(h)\n",
    "        return self.out(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81390dd2",
   "metadata": {},
   "source": [
    "# Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "132e8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIAS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7a0615d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d2_b\n",
      "G: c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d2_b\\cnn_G_20260113_175122.pth\n",
      "D: c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d2_b\\cnn_D_20260113_175122.pth\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.abspath(f\"../models/main3_d{DATA_MODE}_{\"b\" if BIAS else \"nb\"}/\")\n",
    "print(data_path)\n",
    "\n",
    "pattern = re.compile(r\"^cnn_([DG])_\\d{8}_\\d{6}\\.pth$\")\n",
    "\n",
    "cnn_G_path = None\n",
    "cnn_D_path = None\n",
    "\n",
    "for f in os.listdir(data_path):\n",
    "    m = pattern.match(f)\n",
    "    if m:\n",
    "        full = os.path.join(data_path, f)\n",
    "        if m.group(1) == \"G\":\n",
    "            cnn_G_path = full\n",
    "        else:\n",
    "            cnn_D_path = full\n",
    "\n",
    "print(\"G:\", cnn_G_path)\n",
    "print(\"D:\", cnn_D_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac653b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratorCNNWGAN(\n",
      "  (e1): ConvBlock1D(\n",
      "    (conv): Conv1d(1, 32, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e2): ConvBlock1D(\n",
      "    (conv): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e3): ConvBlock1D(\n",
      "    (conv): Conv1d(64, 128, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e4): ConvBlock1D(\n",
      "    (conv): Conv1d(128, 256, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (0): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (d1): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d2): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(256, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d3): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(128, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d4): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(64, 16, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (out): Conv1d(16, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      ")\n",
      "CriticPatch1D(\n",
      "  (c1): Conv1d(2, 32, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "  (c2): ConvBlock1D(\n",
      "    (conv): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (c3): ConvBlock1D(\n",
      "    (conv): Conv1d(64, 128, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (c4): ConvBlock1D(\n",
      "    (conv): Conv1d(128, 256, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (out): Conv1d(256, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aryo\\AppData\\Local\\Temp\\ipykernel_9112\\2181062752.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  G.load_state_dict(torch.load(cnn_G_path, map_location=device))\n",
      "C:\\Users\\Aryo\\AppData\\Local\\Temp\\ipykernel_9112\\2181062752.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  D.load_state_dict(torch.load(cnn_D_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "G = GeneratorCNNWGAN(bias=BIAS).to(device)\n",
    "D = CriticPatch1D(bias=BIAS).to(device)\n",
    "\n",
    "G.load_state_dict(torch.load(cnn_G_path, map_location=device))\n",
    "D.load_state_dict(torch.load(cnn_D_path, map_location=device))\n",
    "\n",
    "print(G.eval())\n",
    "print(D.eval())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5aa43328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> GeneratorCNNWGAN\n",
      "e1 -> ConvBlock1D\n",
      "e1.conv -> Conv1d\n",
      "e1.norm -> BatchNorm1d\n",
      "e1.act -> LeakyReLU\n",
      "e2 -> ConvBlock1D\n",
      "e2.conv -> Conv1d\n",
      "e2.norm -> BatchNorm1d\n",
      "e2.act -> LeakyReLU\n",
      "e3 -> ConvBlock1D\n",
      "e3.conv -> Conv1d\n",
      "e3.norm -> BatchNorm1d\n",
      "e3.act -> LeakyReLU\n",
      "e4 -> ConvBlock1D\n",
      "e4.conv -> Conv1d\n",
      "e4.norm -> BatchNorm1d\n",
      "e4.act -> LeakyReLU\n",
      "bottleneck -> Sequential\n",
      "bottleneck.0 -> ResBlock1D\n",
      "bottleneck.0.c1 -> Conv1d\n",
      "bottleneck.0.n1 -> BatchNorm1d\n",
      "bottleneck.0.c2 -> Conv1d\n",
      "bottleneck.0.n2 -> BatchNorm1d\n",
      "bottleneck.1 -> ResBlock1D\n",
      "bottleneck.1.c1 -> Conv1d\n",
      "bottleneck.1.n1 -> BatchNorm1d\n",
      "bottleneck.1.c2 -> Conv1d\n",
      "bottleneck.1.n2 -> BatchNorm1d\n",
      "bottleneck.2 -> ResBlock1D\n",
      "bottleneck.2.c1 -> Conv1d\n",
      "bottleneck.2.n1 -> BatchNorm1d\n",
      "bottleneck.2.c2 -> Conv1d\n",
      "bottleneck.2.n2 -> BatchNorm1d\n",
      "bottleneck.3 -> ResBlock1D\n",
      "bottleneck.3.c1 -> Conv1d\n",
      "bottleneck.3.n1 -> BatchNorm1d\n",
      "bottleneck.3.c2 -> Conv1d\n",
      "bottleneck.3.n2 -> BatchNorm1d\n",
      "d1 -> DeconvBlock1D\n",
      "d1.deconv -> ConvTranspose1d\n",
      "d1.norm -> BatchNorm1d\n",
      "d1.act -> ReLU\n",
      "d2 -> DeconvBlock1D\n",
      "d2.deconv -> ConvTranspose1d\n",
      "d2.norm -> BatchNorm1d\n",
      "d2.act -> ReLU\n",
      "d3 -> DeconvBlock1D\n",
      "d3.deconv -> ConvTranspose1d\n",
      "d3.norm -> BatchNorm1d\n",
      "d3.act -> ReLU\n",
      "d4 -> DeconvBlock1D\n",
      "d4.deconv -> ConvTranspose1d\n",
      "d4.norm -> BatchNorm1d\n",
      "d4.act -> ReLU\n",
      "out -> Conv1d\n",
      "\n",
      "================================================================================\n",
      "\n",
      " -> CriticPatch1D\n",
      "c1 -> Conv1d\n",
      "c2 -> ConvBlock1D\n",
      "c2.conv -> Conv1d\n",
      "c2.norm -> BatchNorm1d\n",
      "c2.act -> LeakyReLU\n",
      "c3 -> ConvBlock1D\n",
      "c3.conv -> Conv1d\n",
      "c3.norm -> BatchNorm1d\n",
      "c3.act -> LeakyReLU\n",
      "c4 -> ConvBlock1D\n",
      "c4.conv -> Conv1d\n",
      "c4.norm -> BatchNorm1d\n",
      "c4.act -> LeakyReLU\n",
      "out -> Conv1d\n",
      "\n",
      "================================================================================\n",
      "\n",
      "e1.conv.weight torch.Size([32, 1, 16]) True\n",
      "e1.conv.bias torch.Size([32]) True\n",
      "e1.norm.weight torch.Size([32]) True\n",
      "e1.norm.bias torch.Size([32]) True\n",
      "e2.conv.weight torch.Size([64, 32, 16]) True\n",
      "e2.conv.bias torch.Size([64]) True\n",
      "e2.norm.weight torch.Size([64]) True\n",
      "e2.norm.bias torch.Size([64]) True\n",
      "e3.conv.weight torch.Size([128, 64, 16]) True\n",
      "e3.conv.bias torch.Size([128]) True\n",
      "e3.norm.weight torch.Size([128]) True\n",
      "e3.norm.bias torch.Size([128]) True\n",
      "e4.conv.weight torch.Size([256, 128, 16]) True\n",
      "e4.conv.bias torch.Size([256]) True\n",
      "e4.norm.weight torch.Size([256]) True\n",
      "e4.norm.bias torch.Size([256]) True\n",
      "bottleneck.0.c1.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.0.c1.bias torch.Size([256]) True\n",
      "bottleneck.0.n1.weight torch.Size([256]) True\n",
      "bottleneck.0.n1.bias torch.Size([256]) True\n",
      "bottleneck.0.c2.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.0.c2.bias torch.Size([256]) True\n",
      "bottleneck.0.n2.weight torch.Size([256]) True\n",
      "bottleneck.0.n2.bias torch.Size([256]) True\n",
      "bottleneck.1.c1.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.1.c1.bias torch.Size([256]) True\n",
      "bottleneck.1.n1.weight torch.Size([256]) True\n",
      "bottleneck.1.n1.bias torch.Size([256]) True\n",
      "bottleneck.1.c2.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.1.c2.bias torch.Size([256]) True\n",
      "bottleneck.1.n2.weight torch.Size([256]) True\n",
      "bottleneck.1.n2.bias torch.Size([256]) True\n",
      "bottleneck.2.c1.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.2.c1.bias torch.Size([256]) True\n",
      "bottleneck.2.n1.weight torch.Size([256]) True\n",
      "bottleneck.2.n1.bias torch.Size([256]) True\n",
      "bottleneck.2.c2.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.2.c2.bias torch.Size([256]) True\n",
      "bottleneck.2.n2.weight torch.Size([256]) True\n",
      "bottleneck.2.n2.bias torch.Size([256]) True\n",
      "bottleneck.3.c1.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.3.c1.bias torch.Size([256]) True\n",
      "bottleneck.3.n1.weight torch.Size([256]) True\n",
      "bottleneck.3.n1.bias torch.Size([256]) True\n",
      "bottleneck.3.c2.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.3.c2.bias torch.Size([256]) True\n",
      "bottleneck.3.n2.weight torch.Size([256]) True\n",
      "bottleneck.3.n2.bias torch.Size([256]) True\n",
      "d1.deconv.weight torch.Size([256, 128, 4]) True\n",
      "d1.deconv.bias torch.Size([128]) True\n",
      "d1.norm.weight torch.Size([128]) True\n",
      "d1.norm.bias torch.Size([128]) True\n",
      "d2.deconv.weight torch.Size([256, 64, 4]) True\n",
      "d2.deconv.bias torch.Size([64]) True\n",
      "d2.norm.weight torch.Size([64]) True\n",
      "d2.norm.bias torch.Size([64]) True\n",
      "d3.deconv.weight torch.Size([128, 32, 4]) True\n",
      "d3.deconv.bias torch.Size([32]) True\n",
      "d3.norm.weight torch.Size([32]) True\n",
      "d3.norm.bias torch.Size([32]) True\n",
      "d4.deconv.weight torch.Size([64, 16, 4]) True\n",
      "d4.deconv.bias torch.Size([16]) True\n",
      "d4.norm.weight torch.Size([16]) True\n",
      "d4.norm.bias torch.Size([16]) True\n",
      "out.weight torch.Size([1, 16, 7]) True\n",
      "out.bias torch.Size([1]) True\n",
      "\n",
      "================================================================================\n",
      "\n",
      "c1.weight torch.Size([32, 2, 16]) True\n",
      "c1.bias torch.Size([32]) True\n",
      "c2.conv.weight torch.Size([64, 32, 16]) True\n",
      "c2.conv.bias torch.Size([64]) True\n",
      "c2.norm.weight torch.Size([64]) True\n",
      "c2.norm.bias torch.Size([64]) True\n",
      "c3.conv.weight torch.Size([128, 64, 16]) True\n",
      "c3.conv.bias torch.Size([128]) True\n",
      "c3.norm.weight torch.Size([128]) True\n",
      "c3.norm.bias torch.Size([128]) True\n",
      "c4.conv.weight torch.Size([256, 128, 16]) True\n",
      "c4.conv.bias torch.Size([256]) True\n",
      "c4.norm.weight torch.Size([256]) True\n",
      "c4.norm.bias torch.Size([256]) True\n",
      "out.weight torch.Size([1, 256, 7]) True\n",
      "out.bias torch.Size([1]) True\n",
      "\n",
      "================================================================================\n",
      "\n",
      "e1.norm.running_mean torch.Size([32])\n",
      "e1.norm.running_var torch.Size([32])\n",
      "e1.norm.num_batches_tracked torch.Size([])\n",
      "e2.norm.running_mean torch.Size([64])\n",
      "e2.norm.running_var torch.Size([64])\n",
      "e2.norm.num_batches_tracked torch.Size([])\n",
      "e3.norm.running_mean torch.Size([128])\n",
      "e3.norm.running_var torch.Size([128])\n",
      "e3.norm.num_batches_tracked torch.Size([])\n",
      "e4.norm.running_mean torch.Size([256])\n",
      "e4.norm.running_var torch.Size([256])\n",
      "e4.norm.num_batches_tracked torch.Size([])\n",
      "bottleneck.0.n1.running_mean torch.Size([256])\n",
      "bottleneck.0.n1.running_var torch.Size([256])\n",
      "bottleneck.0.n1.num_batches_tracked torch.Size([])\n",
      "bottleneck.0.n2.running_mean torch.Size([256])\n",
      "bottleneck.0.n2.running_var torch.Size([256])\n",
      "bottleneck.0.n2.num_batches_tracked torch.Size([])\n",
      "bottleneck.1.n1.running_mean torch.Size([256])\n",
      "bottleneck.1.n1.running_var torch.Size([256])\n",
      "bottleneck.1.n1.num_batches_tracked torch.Size([])\n",
      "bottleneck.1.n2.running_mean torch.Size([256])\n",
      "bottleneck.1.n2.running_var torch.Size([256])\n",
      "bottleneck.1.n2.num_batches_tracked torch.Size([])\n",
      "bottleneck.2.n1.running_mean torch.Size([256])\n",
      "bottleneck.2.n1.running_var torch.Size([256])\n",
      "bottleneck.2.n1.num_batches_tracked torch.Size([])\n",
      "bottleneck.2.n2.running_mean torch.Size([256])\n",
      "bottleneck.2.n2.running_var torch.Size([256])\n",
      "bottleneck.2.n2.num_batches_tracked torch.Size([])\n",
      "bottleneck.3.n1.running_mean torch.Size([256])\n",
      "bottleneck.3.n1.running_var torch.Size([256])\n",
      "bottleneck.3.n1.num_batches_tracked torch.Size([])\n",
      "bottleneck.3.n2.running_mean torch.Size([256])\n",
      "bottleneck.3.n2.running_var torch.Size([256])\n",
      "bottleneck.3.n2.num_batches_tracked torch.Size([])\n",
      "d1.norm.running_mean torch.Size([128])\n",
      "d1.norm.running_var torch.Size([128])\n",
      "d1.norm.num_batches_tracked torch.Size([])\n",
      "d2.norm.running_mean torch.Size([64])\n",
      "d2.norm.running_var torch.Size([64])\n",
      "d2.norm.num_batches_tracked torch.Size([])\n",
      "d3.norm.running_mean torch.Size([32])\n",
      "d3.norm.running_var torch.Size([32])\n",
      "d3.norm.num_batches_tracked torch.Size([])\n",
      "d4.norm.running_mean torch.Size([16])\n",
      "d4.norm.running_var torch.Size([16])\n",
      "d4.norm.num_batches_tracked torch.Size([])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "c2.norm.running_mean torch.Size([64])\n",
      "c2.norm.running_var torch.Size([64])\n",
      "c2.norm.num_batches_tracked torch.Size([])\n",
      "c3.norm.running_mean torch.Size([128])\n",
      "c3.norm.running_var torch.Size([128])\n",
      "c3.norm.num_batches_tracked torch.Size([])\n",
      "c4.norm.running_mean torch.Size([256])\n",
      "c4.norm.running_var torch.Size([256])\n",
      "c4.norm.num_batches_tracked torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "for name, module in G.named_modules():\n",
    "    print(name, \"->\", module.__class__.__name__)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 +\"\\n\")\n",
    "\n",
    "for name, module in D.named_modules():\n",
    "    print(name, \"->\", module.__class__.__name__)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 +\"\\n\")\n",
    "\n",
    "for name, param in G.named_parameters():\n",
    "    print(name, param.shape, param.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 +\"\\n\")\n",
    "\n",
    "for name, param in D.named_parameters():\n",
    "    print(name, param.shape, param.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 +\"\\n\")\n",
    "\n",
    "for name, buf in G.named_buffers():\n",
    "    print(name, buf.shape)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 +\"\\n\")\n",
    "\n",
    "for name, buf in D.named_buffers():\n",
    "    print(name, buf.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21901abc",
   "metadata": {},
   "source": [
    "# Fuse Weigths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9ab98c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_bn_1d(\n",
    "    conv_weight,        # (Cout, Cin, K)\n",
    "    conv_bias,          # (Cout,) or None\n",
    "    running_mean,       # (Cout,)\n",
    "    running_var,        # (Cout,)\n",
    "    bn_weight,          # (Cout,) or None (gamma)\n",
    "    bn_bias,            # (Cout,) or None (beta)\n",
    "    eps=1e-5\n",
    "):\n",
    "    Cout = conv_weight.shape[0]\n",
    "\n",
    "    if bn_weight is None:\n",
    "        bn_weight = torch.ones(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "    if bn_bias is None:\n",
    "        bn_bias = torch.zeros(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "    if conv_bias is None:\n",
    "        conv_bias = torch.zeros(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "\n",
    "    denom = torch.sqrt(running_var + eps)          # (Cout,)\n",
    "    scale = bn_weight / denom                      # (Cout,)\n",
    "\n",
    "    # Fuse weight\n",
    "    fused_weight = conv_weight * scale[:, None, None]\n",
    "\n",
    "    # Fuse bias\n",
    "    fused_bias = (conv_bias - running_mean) * scale + bn_bias\n",
    "\n",
    "    return fused_weight, fused_bias\n",
    "\n",
    "def fuse_deconv_bn_1d(\n",
    "    deconv_weight,     # (Cin, Cout, K)\n",
    "    deconv_bias,       # (Cout,) or None\n",
    "    running_mean,      # (Cout,)\n",
    "    running_var,       # (Cout,)\n",
    "    bn_weight,         # (Cout,)\n",
    "    bn_bias,           # (Cout,)\n",
    "    eps\n",
    "):\n",
    "    Cin, Cout, K = deconv_weight.shape\n",
    "\n",
    "    if deconv_bias is None:\n",
    "        deconv_bias = torch.zeros(\n",
    "            Cout,\n",
    "            device=deconv_weight.device,\n",
    "            dtype=deconv_weight.dtype\n",
    "        )\n",
    "\n",
    "    # BN scale\n",
    "    scale = bn_weight / torch.sqrt(running_var + eps)  # (Cout,)\n",
    "\n",
    "    # Fuse weights (scale on Cout dimension)\n",
    "    fused_weight = deconv_weight * scale.view(1, Cout, 1)\n",
    "\n",
    "    # Fuse bias\n",
    "    fused_bias = (deconv_bias - running_mean) * scale + bn_bias\n",
    "\n",
    "    return fused_weight, fused_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "543d8959",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedConvBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s, p, bias=True, act=\"lrelu\"):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, k, s, p, bias=bias)\n",
    "\n",
    "        if act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "\n",
    "\n",
    "class FusedDeconvBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s, p, bias=True, act=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(in_ch, out_ch, k, s, p, bias=bias)\n",
    "\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.deconv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "efd1bb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorCNNWGAN_Fused(nn.Module):\n",
    "    def __init__(self, base_ch=32, bottleneck_blocks=4, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.e1 = FusedConvBlock1D(1, base_ch, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e2 = FusedConvBlock1D(base_ch, base_ch*2, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e3 = FusedConvBlock1D(base_ch*2, base_ch*4, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e4 = FusedConvBlock1D(base_ch*4, base_ch*8, 16, 2, 7, bias, \"lrelu\")\n",
    "\n",
    "        self.bottleneck = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(base_ch*8, base_ch*8, 7, 1, 3, bias=bias),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv1d(base_ch*8, base_ch*8, 7, 1, 3, bias=bias),\n",
    "            )\n",
    "            for _ in range(bottleneck_blocks)\n",
    "        ])\n",
    "\n",
    "        self.d1 = FusedDeconvBlock1D(base_ch*8, base_ch*4, 4, 2, 1, bias)\n",
    "        self.d2 = FusedDeconvBlock1D(base_ch*8, base_ch*2, 4, 2, 1, bias)\n",
    "        self.d3 = FusedDeconvBlock1D(base_ch*4, base_ch, 4, 2, 1, bias)\n",
    "        self.d4 = FusedDeconvBlock1D(base_ch*2, base_ch//2, 4, 2, 1, bias)\n",
    "\n",
    "        self.out = nn.Conv1d(base_ch//2, 1, 7, 1, 3, bias=bias)\n",
    "\n",
    "    def forward(self, y):\n",
    "        s1 = self.e1(y)\n",
    "        s2 = self.e2(s1)\n",
    "        s3 = self.e3(s2)\n",
    "        s4 = self.e4(s3)\n",
    "\n",
    "        b = s4\n",
    "        for blk in self.bottleneck:\n",
    "            b = F.relu(b + blk(b))\n",
    "\n",
    "        d1 = torch.cat([self.d1(b), s3], dim=1)\n",
    "        d2 = torch.cat([self.d2(d1), s2], dim=1)\n",
    "        d3 = torch.cat([self.d3(d2), s1], dim=1)\n",
    "        d4 = self.d4(d3)\n",
    "\n",
    "        return self.out(d4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "72c46b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_generator(G):\n",
    "    G_fused = GeneratorCNNWGAN_Fused(bias=True).to(device)\n",
    "    G_fused.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encoder\n",
    "        for i in range(1, 5):\n",
    "            e = getattr(G, f\"e{i}\")\n",
    "            fe = getattr(G_fused, f\"e{i}\")\n",
    "\n",
    "            w, b = fuse_conv_bn_1d(\n",
    "                e.conv.weight, e.conv.bias,\n",
    "                e.norm.running_mean, e.norm.running_var,\n",
    "                e.norm.weight, e.norm.bias\n",
    "            )\n",
    "            fe.conv.weight.copy_(w)\n",
    "            fe.conv.bias.copy_(b)\n",
    "\n",
    "        # Bottleneck\n",
    "        for i, blk in enumerate(G.bottleneck):\n",
    "            fblk = G_fused.bottleneck[i]\n",
    "\n",
    "            for j, (c, n) in enumerate([(blk.c1, blk.n1), (blk.c2, blk.n2)]):\n",
    "                w, b = fuse_conv_bn_1d(\n",
    "                    c.weight, c.bias,\n",
    "                    n.running_mean, n.running_var,\n",
    "                    n.weight, n.bias\n",
    "                )\n",
    "                fblk[j*2].weight.copy_(w)\n",
    "                fblk[j*2].bias.copy_(b)\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(1, 5):\n",
    "            d = getattr(G, f\"d{i}\")\n",
    "            fd = getattr(G_fused, f\"d{i}\")\n",
    "\n",
    "            w, b = fuse_deconv_bn_1d(\n",
    "                d.deconv.weight, d.deconv.bias,\n",
    "                d.norm.running_mean, d.norm.running_var,\n",
    "                d.norm.weight, d.norm.bias,\n",
    "                d.norm.eps\n",
    "            )\n",
    "            fd.deconv.weight.copy_(w)\n",
    "            fd.deconv.bias.copy_(b)\n",
    "\n",
    "        # Output\n",
    "        G_fused.out.weight.copy_(G.out.weight)\n",
    "        G_fused.out.bias.copy_(G.out.bias)\n",
    "\n",
    "    return G_fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6092e01f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> GeneratorCNNWGAN_Fused\n",
      "e1 -> FusedConvBlock1D\n",
      "e1.conv -> Conv1d\n",
      "e1.act -> LeakyReLU\n",
      "e2 -> FusedConvBlock1D\n",
      "e2.conv -> Conv1d\n",
      "e2.act -> LeakyReLU\n",
      "e3 -> FusedConvBlock1D\n",
      "e3.conv -> Conv1d\n",
      "e3.act -> LeakyReLU\n",
      "e4 -> FusedConvBlock1D\n",
      "e4.conv -> Conv1d\n",
      "e4.act -> LeakyReLU\n",
      "bottleneck -> Sequential\n",
      "bottleneck.0 -> Sequential\n",
      "bottleneck.0.0 -> Conv1d\n",
      "bottleneck.0.1 -> ReLU\n",
      "bottleneck.0.2 -> Conv1d\n",
      "bottleneck.1 -> Sequential\n",
      "bottleneck.1.0 -> Conv1d\n",
      "bottleneck.1.1 -> ReLU\n",
      "bottleneck.1.2 -> Conv1d\n",
      "bottleneck.2 -> Sequential\n",
      "bottleneck.2.0 -> Conv1d\n",
      "bottleneck.2.1 -> ReLU\n",
      "bottleneck.2.2 -> Conv1d\n",
      "bottleneck.3 -> Sequential\n",
      "bottleneck.3.0 -> Conv1d\n",
      "bottleneck.3.1 -> ReLU\n",
      "bottleneck.3.2 -> Conv1d\n",
      "d1 -> FusedDeconvBlock1D\n",
      "d1.deconv -> ConvTranspose1d\n",
      "d1.act -> ReLU\n",
      "d2 -> FusedDeconvBlock1D\n",
      "d2.deconv -> ConvTranspose1d\n",
      "d2.act -> ReLU\n",
      "d3 -> FusedDeconvBlock1D\n",
      "d3.deconv -> ConvTranspose1d\n",
      "d3.act -> ReLU\n",
      "d4 -> FusedDeconvBlock1D\n",
      "d4.deconv -> ConvTranspose1d\n",
      "d4.act -> ReLU\n",
      "out -> Conv1d\n"
     ]
    }
   ],
   "source": [
    "G_fused = fuse_generator(G)\n",
    "\n",
    "for name, module in G_fused.named_modules():\n",
    "    print(name, \"->\", module.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53258f13",
   "metadata": {},
   "source": [
    "# Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aee774d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len: 512\n",
      "Max: 5.067047251915556\n",
      "Min: -4.0315014453972156\n",
      "Scale: 5.067047251915556\n"
     ]
    }
   ],
   "source": [
    "sample = test_samples_norm[0]\n",
    "sample_scale = max(abs(sample[\"noisy_norm\"].min()), abs(sample[\"noisy_norm\"].max()))\n",
    "x = torch.tensor(sample[\"noisy_norm\"], dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
    "\n",
    "print(\"Len:\", len(test_samples_norm[0][\"noisy_norm\"]))\n",
    "print(\"Max:\", sample[\"noisy_norm\"].max())\n",
    "print(\"Min:\", sample[\"noisy_norm\"].min())\n",
    "print(\"Scale:\", sample_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5f2b7a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLOBAL min: -10.46939142750513\n",
      "GLOBAL max: 14.82895621443052\n"
     ]
    }
   ],
   "source": [
    "all_vals = np.concatenate(\n",
    "    [s[\"noisy_norm\"] for s in test_samples_norm]\n",
    ")\n",
    "\n",
    "print(\"GLOBAL min:\", all_vals.min())\n",
    "print(\"GLOBAL max:\", all_vals.max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fe0a19",
   "metadata": {},
   "source": [
    "# Data and Weight Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db71e3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# All 32 bit float (torch)\n",
    "print(x.dtype)\n",
    "print(G_fused.e1.conv.weight.dtype)\n",
    "print(G_fused.e1.conv.bias.dtype)\n",
    "print(G_fused.d1.deconv.weight.dtype)\n",
    "print(G_fused.d1.deconv.bias.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d3ee7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== G_fused parameter ranges ===\n",
      "Weights: min = -5.299317e-01, max = 5.558594e-01\n",
      "Biases : min = -2.609087e+00, max = 2.154806e+00\n"
     ]
    }
   ],
   "source": [
    "def scan_weight_bias_ranges(model):\n",
    "    w_min, w_max = float(\"inf\"), float(\"-inf\")\n",
    "    b_min, b_max = float(\"inf\"), float(\"-inf\")\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "            if module.weight is not None:\n",
    "                w_min = min(w_min, module.weight.min().item())\n",
    "                w_max = max(w_max, module.weight.max().item())\n",
    "\n",
    "            if module.bias is not None:\n",
    "                b_min = min(b_min, module.bias.min().item())\n",
    "                b_max = max(b_max, module.bias.max().item())\n",
    "\n",
    "    return w_min, w_max, b_min, b_max\n",
    "\n",
    "w_min, w_max, b_min, b_max = scan_weight_bias_ranges(G_fused)\n",
    "\n",
    "print(\"=== G_fused parameter ranges ===\")\n",
    "print(f\"Weights: min = {w_min:.6e}, max = {w_max:.6e}\")\n",
    "print(f\"Biases : min = {b_min:.6e}, max = {b_max:.6e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb614997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_to_q1_15(x):\n",
    "    # Q1.15 (int16)\n",
    "    # Raw int range   : [-32768, 32767]\n",
    "    # Real value range: [-1.0, +0.999969482421875]\n",
    "    scale = 1 << 15\n",
    "    x = np.round(x * scale)\n",
    "    x = np.clip(x, -32768, 32767) \n",
    "    return x.astype(np.int16)\n",
    "\n",
    "def float_to_q4_12(x):\n",
    "    # Q4.12 (int16)\n",
    "    # Raw int range   : [-32768, 32767]\n",
    "    # Real value range: [-8.0, +7.999755859375]\n",
    "    scale = 1 << 12\n",
    "    x = np.round(x * scale)\n",
    "    x = np.clip(x, -32768, 32767) \n",
    "    return x.astype(np.int16)\n",
    "\n",
    "def float_to_q10_10(x):\n",
    "    # Q10.10 (int32, 20-bit signed)\n",
    "    # Raw int range   : [-524288, 524287]\n",
    "    # Real value range: [-512.0, +511.9990234375]\n",
    "    scale = 1 << 10\n",
    "    x = np.round(x * scale)\n",
    "    x = np.clip(x, -524288,  524287)\n",
    "    return x.astype(np.int32)\n",
    "\n",
    "def float_to_q(x, frac_bits, int_bits, dtype):\n",
    "    scale = 1 << frac_bits\n",
    "    total_bits = int_bits + frac_bits\n",
    "    min_val = -(1 << (total_bits - 1))\n",
    "    max_val = (1 << (total_bits - 1)) - 1\n",
    "\n",
    "    xq = np.round(x * scale)\n",
    "    xq = np.clip(xq, min_val, max_val)\n",
    "    return xq.astype(dtype)\n",
    "\n",
    "def q_to_float(x, frac_bits):\n",
    "    return x.astype(np.float32) / (1 << frac_bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b948ab7a",
   "metadata": {},
   "source": [
    "# Modification Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d095ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_CONFIGS = {\n",
    "    \"Q4.12\": dict(frac_bits=12, int_bits=4, dtype=np.int16),\n",
    "    \"Q10.10\": dict(frac_bits=10, int_bits=10, dtype=np.int32),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627fdbd3",
   "metadata": {},
   "source": [
    "This:\n",
    "- walks Conv1d + ConvTranspose1d\n",
    "- quantizes weights and bias\n",
    "- writes them back as float32 dequantized\n",
    "- prints fixed-point min/max per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "faa73c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each layer\n",
    "# FP32 weights\n",
    "#     ↓ quantize (round + clip)\n",
    "# INT representation (Q format)\n",
    "#     ↓ dequantize\n",
    "# FP32 tensor with quantization error\n",
    "\n",
    "def quantize_model_fixed_point(model, qcfg, verbose=True):\n",
    "    model_q = deepcopy(model)\n",
    "\n",
    "    frac = qcfg[\"frac_bits\"]\n",
    "    itg  = qcfg[\"int_bits\"]\n",
    "    dt   = qcfg[\"dtype\"]\n",
    "\n",
    "    scale = 1 << frac\n",
    "    \n",
    "    print(f\"\\n=== Quantizing model to Q{itg}.{frac} ===\")\n",
    "\n",
    "    for name, module in model_q.named_modules():\n",
    "        if isinstance(module, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "\n",
    "            # ---- weights ----\n",
    "            w = module.weight.detach().cpu().numpy()\n",
    "            w_q = float_to_q(w, frac, itg, dt)\n",
    "            w_f = q_to_float(w_q, frac)\n",
    "\n",
    "            module.weight.data = torch.from_numpy(w_f).to(module.weight.device)\n",
    "\n",
    "            # ---- bias ----\n",
    "            if module.bias is not None:\n",
    "                b = module.bias.detach().cpu().numpy()\n",
    "                b_q = float_to_q(b, frac, itg, dt)\n",
    "                b_f = q_to_float(b_q, frac)\n",
    "                module.bias.data = torch.from_numpy(b_f).to(module.bias.device)\n",
    "\n",
    "            if verbose:\n",
    "                wq_min, wq_max = w_q.min(), w_q.max()\n",
    "                print(f\"[{name}]\")\n",
    "                print(\n",
    "                    f\"  W_q min/max : {wq_min:>8} {wq_max:>8} \"\n",
    "                    f\"(≈ {wq_min/scale:+.6f}, {wq_max/scale:+.6f})\"\n",
    "                )\n",
    "\n",
    "                if module.bias is not None:\n",
    "                    bq_min, bq_max = b_q.min(), b_q.max()\n",
    "                    print(\n",
    "                        f\"  B_q min/max : {bq_min:>8} {bq_max:>8} \"\n",
    "                        f\"(≈ {bq_min/scale:+.6f}, {bq_max/scale:+.6f})\"\n",
    "                    )\n",
    "\n",
    "\n",
    "    return model_q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "201563ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_input_fixed_point(x, qcfg):\n",
    "    frac = qcfg[\"frac_bits\"]\n",
    "    itg  = qcfg[\"int_bits\"]\n",
    "    dt   = qcfg[\"dtype\"]\n",
    "\n",
    "    x_np = x.detach().cpu().numpy()\n",
    "    x_q = float_to_q(x_np, frac, itg, dt)\n",
    "    x_f = q_to_float(x_q, frac)\n",
    "\n",
    "    print(\"\\n=== Input quantization ===\")\n",
    "    print(f\"x_q min/max: {x_q.min()} {x_q.max()}\")\n",
    "\n",
    "    return torch.from_numpy(x_f).to(x.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b3aa9e",
   "metadata": {},
   "source": [
    "# Reference Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0aaec5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rrmse(x_hat, x):\n",
    "    return torch.sqrt(torch.mean((x_hat - x) ** 2)) / torch.sqrt(torch.mean(x ** 2))\n",
    "\n",
    "\n",
    "def corrcoef(x_hat, x):\n",
    "    x_hat = x_hat - x_hat.mean()\n",
    "    x = x - x.mean()\n",
    "    return torch.sum(x_hat * x) / (\n",
    "        torch.sqrt(torch.sum(x_hat ** 2)) * torch.sqrt(torch.sum(x ** 2)) + 1e-8\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3e73bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== G vs G_fused ===\n",
      "Max abs diff : 1.086950e-03\n",
      "Mean abs diff: 2.378545e-04\n",
      "RRMSE        : 5.670033e-04\n",
      "Corr coef    : 1.000000\n"
     ]
    }
   ],
   "source": [
    "G.eval()\n",
    "G_fused.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_ref = G(x)\n",
    "    y_fused = G_fused(x)\n",
    "\n",
    "    assert y_ref.shape == y_fused.shape\n",
    "\n",
    "    max_abs_diff = torch.max(torch.abs(y_ref - y_fused)).item()\n",
    "    mean_abs_diff = torch.mean(torch.abs(y_ref - y_fused)).item()\n",
    "    rrmse_val = rrmse(y_fused, y_ref).item()\n",
    "    corr_val = corrcoef(y_fused, y_ref).item()\n",
    "\n",
    "    print(\"=== G vs G_fused ===\")\n",
    "    print(f\"Max abs diff : {max_abs_diff:.6e}\")\n",
    "    print(f\"Mean abs diff: {mean_abs_diff:.6e}\")\n",
    "    print(f\"RRMSE        : {rrmse_val:.6e}\")\n",
    "    print(f\"Corr coef    : {corr_val:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3cd897",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f4e605fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_and_compare(G_fp32, G_q, x_fp32, x_q):\n",
    "    G_fp32.eval()\n",
    "    G_q.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_ref = G_fp32(x_fp32)\n",
    "        y_q   = G_q(x_q)\n",
    "\n",
    "        max_abs = torch.max(torch.abs(y_ref - y_q)).item()\n",
    "        mean_abs = torch.mean(torch.abs(y_ref - y_q)).item()\n",
    "        rrmse_v = rrmse(y_q, y_ref).item()\n",
    "        corr_v  = corrcoef(y_q, y_ref).item()\n",
    "\n",
    "    print(\"\\n=== Fixed-point vs FP32 ===\")\n",
    "    print(f\"Max abs diff : {max_abs:.6e}\")\n",
    "    print(f\"Mean abs diff: {mean_abs:.6e}\")\n",
    "    print(f\"RRMSE        : {rrmse_v:.6e}\")\n",
    "    print(f\"Corr coef    : {corr_v:.6f}\")\n",
    "\n",
    "    return y_q\n",
    "\n",
    "def print_qcfg_limits(qcfg):\n",
    "    itg  = qcfg[\"int_bits\"]   # includes sign\n",
    "    frac = qcfg[\"frac_bits\"]\n",
    "    dt   = qcfg[\"dtype\"]\n",
    "\n",
    "    total_bits = np.dtype(dt).itemsize * 8\n",
    "    scale = 1 << frac\n",
    "\n",
    "    # signed integer limits from dtype\n",
    "    int_min = -(1 << (total_bits - 1))\n",
    "    int_max =  (1 << (total_bits - 1)) - 1\n",
    "\n",
    "    real_min = int_min / scale\n",
    "    real_max = int_max / scale\n",
    "\n",
    "    # sanity: m = total_bits - frac\n",
    "    # assert itg == total_bits - frac, \\\n",
    "    #     f\"Mismatch: Q{itg}.{frac} but dtype implies Q{total_bits-frac}.{frac}\"\n",
    "\n",
    "    print(\"=== Fixed-point format ===\")\n",
    "    print(f\"Q{itg}.{frac} ({dt.__name__})\")\n",
    "    print(f\"Total bits         : {total_bits}\")\n",
    "    print(f\"Scale factor       : 2^{frac} = {scale}\")\n",
    "    print(f\"Integer range      : [{int_min}, {int_max}]\")\n",
    "    print(f\"Real value range   : [{real_min:.6f}, {real_max:.6f}]\")\n",
    "    print()\n",
    "\n",
    "def attach_activation_range_hooks(model, qcfg):\n",
    "    hooks = []\n",
    "\n",
    "    itg  = qcfg[\"int_bits\"]   # includes sign\n",
    "    frac = qcfg[\"frac_bits\"]\n",
    "\n",
    "    real_min = -(1 << (itg - 1))\n",
    "    real_max =  (1 << (itg - 1)) - (1 / (1 << frac))\n",
    "\n",
    "    def hook_fn(name):\n",
    "        def fn(module, inp, out):\n",
    "            out_min = out.min().item()\n",
    "            out_max = out.max().item()\n",
    "\n",
    "            print(f\"[{name}] output min/max: \"\n",
    "                  f\"{out_min:+.6f}, {out_max:+.6f}\", end=\"\")\n",
    "\n",
    "            if out_min < real_min or out_max > real_max:\n",
    "                print(\"  ⚠️  OUT OF RANGE\")\n",
    "            else:\n",
    "                print(\"  ✓\")\n",
    "\n",
    "        return fn\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (nn.Conv1d, nn.ConvTranspose1d)):\n",
    "            hooks.append(module.register_forward_hook(hook_fn(name)))\n",
    "\n",
    "    return hooks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1176013d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fixed-point format ===\n",
      "Q10.10 (int32)\n",
      "Total bits         : 32\n",
      "Scale factor       : 2^10 = 1024\n",
      "Integer range      : [-2147483648, 2147483647]\n",
      "Real value range   : [-2097152.000000, 2097151.999023]\n",
      "\n",
      "\n",
      "=== Quantizing model to Q10.10 ===\n",
      "\n",
      "=== Input quantization ===\n",
      "x_q min/max: -4128 5189\n",
      "[e1.conv] output min/max: -5.064058, +5.926579  ✓\n",
      "[e2.conv] output min/max: -5.975633, +4.874141  ✓\n",
      "[e3.conv] output min/max: -4.102404, +3.927611  ✓\n",
      "[e4.conv] output min/max: -3.861659, +3.760465  ✓\n",
      "[bottleneck.0.0] output min/max: -3.759048, +3.915655  ✓\n",
      "[bottleneck.0.2] output min/max: -3.849937, +3.924349  ✓\n",
      "[bottleneck.1.0] output min/max: -3.774267, +4.282374  ✓\n",
      "[bottleneck.1.2] output min/max: -4.456840, +3.892466  ✓\n",
      "[bottleneck.2.0] output min/max: -4.465076, +3.977906  ✓\n",
      "[bottleneck.2.2] output min/max: -3.880110, +4.034831  ✓\n",
      "[bottleneck.3.0] output min/max: -3.649748, +3.954165  ✓\n",
      "[bottleneck.3.2] output min/max: -4.985993, +3.991021  ✓\n",
      "[d1.deconv] output min/max: -3.993756, +4.326971  ✓\n",
      "[d2.deconv] output min/max: -4.186068, +4.847568  ✓\n",
      "[d3.deconv] output min/max: -4.684648, +5.548031  ✓\n",
      "[d4.deconv] output min/max: -4.844221, +6.583823  ✓\n",
      "[out] output min/max: -1.448663, +1.638826  ✓\n",
      "\n",
      "=== Fixed-point vs FP32 ===\n",
      "Max abs diff : 2.597484e-02\n",
      "Mean abs diff: 5.744908e-03\n",
      "RRMSE        : 1.395665e-02\n",
      "Corr coef    : 0.999913\n"
     ]
    }
   ],
   "source": [
    "# \"Q4.12\" or \"Q10.10\"\n",
    "qcfg = Q_CONFIGS[\"Q10.10\"]\n",
    "\n",
    "print_qcfg_limits(qcfg)\n",
    "\n",
    "G_q = quantize_model_fixed_point(G_fused, qcfg, verbose=False)\n",
    "x_q = quantize_input_fixed_point(x, qcfg)\n",
    "hooks = attach_activation_range_hooks(G_q, qcfg)\n",
    "\n",
    "y_q = run_and_compare(G_fused, G_q, x, x_q)\n",
    "\n",
    "for h in hooks:\n",
    "    h.remove()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
