{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e95c8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5f73d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbc9ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 1234\n",
    "\n",
    "# Python & NumPy\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Determinism flags\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c89c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = torch.randn(1, 512, device=device)\n",
    "bottleneck_input = torch.randn(256, 32, device=device)\n",
    "decoder_first_input = torch.randn(256, 32, device=device)\n",
    "decoder_last_input = torch.randn(64, 256, device=device)\n",
    "out_input = torch.randn(16, 512, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c498f9f",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ce1ba420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv1d -> BatchNorm1d -> Activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=15, s=2, p=7, bias=True, act=\"lrelu\"):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=bias)\n",
    "        self.norm = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        if act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act must be 'lrelu' or 'relu'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "class DeconvBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvTranspose1d -> BatchNorm1d -> Activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=4, s=2, p=1, bias=True, act=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=bias)\n",
    "        self.norm = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act must be 'relu' or 'lrelu'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.deconv(x)))\n",
    "\n",
    "\n",
    "class ResBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block: (Conv -> BN -> ReLU) x2 + skip\n",
    "    Keeps same channel count and length.\n",
    "    \"\"\"\n",
    "    def __init__(self, ch, k=7, p=3, bias=True):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv1d(ch, ch, kernel_size=k, stride=1, padding=p, bias=bias)\n",
    "        self.n1 = nn.BatchNorm1d(ch)\n",
    "        self.c2 = nn.Conv1d(ch, ch, kernel_size=k, stride=1, padding=p, bias=bias)\n",
    "        self.n2 = nn.BatchNorm1d(ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.n1(self.c1(x)))\n",
    "        h = self.n2(self.c2(h))\n",
    "        return F.relu(x + h)\n",
    "    \n",
    "class MultiScaleResBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale residual block: parallel conv branches (k=3,5,7) then fuse.\n",
    "    Keeps same channel count and length.\n",
    "    \"\"\"\n",
    "    def __init__(self, ch, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=3, padding=1, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.b5 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=5, padding=2, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.b7 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=7, padding=3, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=1, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.b3(x) + self.b5(x) + self.b7(x)\n",
    "        h = self.fuse(h)\n",
    "        return F.relu(x + h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19b6d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN Generator (U-Net-ish + Res bottleneck)\n",
    "class GeneratorCNNWGAN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN U-Net-ish generator for EEG denoising (WGAN).\n",
    "    Input : (B, 1, 512) noisy_norm\n",
    "    Output: (B, 1, 512) clean_norm_hat\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=32, bottleneck_blocks=4, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.e1 = ConvBlock1D(1, base_ch,       k=16, s=2, p=7, bias=bias, act=\"lrelu\")      # 512 -> 256\n",
    "        self.e2 = ConvBlock1D(base_ch, base_ch*2, k=16, s=2, p=7, bias=bias, act=\"lrelu\")    # 256 -> 128\n",
    "        self.e3 = ConvBlock1D(base_ch*2, base_ch*4, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 128 -> 64\n",
    "        self.e4 = ConvBlock1D(base_ch*4, base_ch*8, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 64 -> 32\n",
    "\n",
    "        # Bottleneck\n",
    "        bn_ch = base_ch * 8\n",
    "        self.bottleneck = nn.Sequential(*[\n",
    "            ResBlock1D(bn_ch, k=7, p=3, bias=bias) for _ in range(bottleneck_blocks)\n",
    "        ])\n",
    "\n",
    "        # Decoder (concat doubles channels)\n",
    "        self.d1 = DeconvBlock1D(bn_ch, base_ch*4,   k=4, s=2, p=1, bias=bias, act=\"relu\")     # 32 -> 64\n",
    "        self.d2 = DeconvBlock1D(base_ch*8, base_ch*2, k=4, s=2, p=1,bias=bias, act=\"relu\")   # 64 -> 128\n",
    "        self.d3 = DeconvBlock1D(base_ch*4, base_ch,   k=4, s=2, p=1, bias=bias, act=\"relu\")   # 128 -> 256\n",
    "        self.d4 = DeconvBlock1D(base_ch*2, base_ch//2, k=4, s=2, p=1, bias=bias, act=\"relu\")  # 256 -> 512\n",
    "\n",
    "        # Head (linear output recommended for normalized signals)\n",
    "        self.out = nn.Conv1d(base_ch//2, 1, kernel_size=7, stride=1, padding=3, bias=bias)\n",
    "\n",
    "    def forward(self, y):\n",
    "        # Encoder\n",
    "        s1 = self.e1(y)   # (B, base, 256)\n",
    "        s2 = self.e2(s1)  # (B, 2b, 128)\n",
    "        s3 = self.e3(s2)  # (B, 4b, 64)\n",
    "        s4 = self.e4(s3)  # (B, 8b, 32)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(s4)\n",
    "\n",
    "        # Decoder + skip connections\n",
    "        d1 = self.d1(b)                  # (B, 4b, 64)\n",
    "        d1 = torch.cat([d1, s3], dim=1)  # (B, 8b, 64)\n",
    "\n",
    "        d2 = self.d2(d1)                 # (B, 2b, 128)\n",
    "        d2 = torch.cat([d2, s2], dim=1)  # (B, 4b, 128)\n",
    "\n",
    "        d3 = self.d3(d2)                 # (B, b, 256)\n",
    "        d3 = torch.cat([d3, s1], dim=1)  # (B, 2b, 256)\n",
    "\n",
    "        d4 = self.d4(d3)                 # (B, b/2, 512)\n",
    "\n",
    "        return self.out(d4)              # (B, 1, 512)\n",
    "    \n",
    "# Patch Critic (shared by CNN/ResCNN)\n",
    "class CriticPatch1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional PatchGAN critic for WGAN:\n",
    "      D(y, x) -> patch scores\n",
    "    y,x: (B,1,512)\n",
    "    output: (B,1,32)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=32, bias=True):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv1d(2, base_ch, kernel_size=16, stride=2, padding=7, bias=bias)  # 512 -> 256\n",
    "        self.c2 = ConvBlock1D(base_ch, base_ch*2, k=16, s=2, p=7, bias=bias, act=\"lrelu\")    # 256 -> 128\n",
    "        self.c3 = ConvBlock1D(base_ch*2, base_ch*4, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 128 -> 64\n",
    "        self.c4 = ConvBlock1D(base_ch*4, base_ch*8, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 64 -> 32\n",
    "        self.out = nn.Conv1d(base_ch*8, 1, kernel_size=7, stride=1, padding=3, bias=bias)   # 32 -> 32\n",
    "\n",
    "    def forward(self, y, x):\n",
    "        h = torch.cat([y, x], dim=1)  # (B,2,512)\n",
    "        h = F.leaky_relu(self.c1(h), 0.2, inplace=True)\n",
    "        h = self.c2(h)\n",
    "        h = self.c3(h)\n",
    "        h = self.c4(h)\n",
    "        return self.out(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac198d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedConvBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s, p, bias=True, act=\"lrelu\"):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, k, s, p, bias=bias)\n",
    "\n",
    "        if act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "\n",
    "\n",
    "class FusedDeconvBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s, p, bias=True, act=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(in_ch, out_ch, k, s, p, bias=bias)\n",
    "\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.deconv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97b7ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorCNNWGAN_Fused(nn.Module):\n",
    "    def __init__(self, base_ch=32, bottleneck_blocks=4, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.e1 = FusedConvBlock1D(1, base_ch, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e2 = FusedConvBlock1D(base_ch, base_ch*2, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e3 = FusedConvBlock1D(base_ch*2, base_ch*4, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e4 = FusedConvBlock1D(base_ch*4, base_ch*8, 16, 2, 7, bias, \"lrelu\")\n",
    "\n",
    "        self.bottleneck = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(base_ch*8, base_ch*8, 7, 1, 3, bias=bias),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv1d(base_ch*8, base_ch*8, 7, 1, 3, bias=bias),\n",
    "            )\n",
    "            for _ in range(bottleneck_blocks)\n",
    "        ])\n",
    "\n",
    "        self.d1 = FusedDeconvBlock1D(base_ch*8, base_ch*4, 4, 2, 1, bias)\n",
    "        self.d2 = FusedDeconvBlock1D(base_ch*8, base_ch*2, 4, 2, 1, bias)\n",
    "        self.d3 = FusedDeconvBlock1D(base_ch*4, base_ch, 4, 2, 1, bias)\n",
    "        self.d4 = FusedDeconvBlock1D(base_ch*2, base_ch//2, 4, 2, 1, bias)\n",
    "\n",
    "        self.out = nn.Conv1d(base_ch//2, 1, 7, 1, 3, bias=bias)\n",
    "\n",
    "    def forward(self, y):\n",
    "        s1 = self.e1(y)\n",
    "        s2 = self.e2(s1)\n",
    "        s3 = self.e3(s2)\n",
    "        s4 = self.e4(s3)\n",
    "\n",
    "        b = s4\n",
    "        for blk in self.bottleneck:\n",
    "            b = F.relu(b + blk(b))\n",
    "\n",
    "        d1 = torch.cat([self.d1(b), s3], dim=1)\n",
    "        d2 = torch.cat([self.d2(d1), s2], dim=1)\n",
    "        d3 = torch.cat([self.d3(d2), s1], dim=1)\n",
    "        d4 = self.d4(d3)\n",
    "\n",
    "        return self.out(d4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d4b07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_bn_1d(\n",
    "    conv_weight,        # (Cout, Cin, K)\n",
    "    conv_bias,          # (Cout,) or None\n",
    "    running_mean,       # (Cout,)\n",
    "    running_var,        # (Cout,)\n",
    "    bn_weight,          # (Cout,) or None (gamma)\n",
    "    bn_bias,            # (Cout,) or None (beta)\n",
    "    eps=1e-5\n",
    "):\n",
    "    Cout = conv_weight.shape[0]\n",
    "\n",
    "    if bn_weight is None:\n",
    "        bn_weight = torch.ones(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "    if bn_bias is None:\n",
    "        bn_bias = torch.zeros(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "    if conv_bias is None:\n",
    "        conv_bias = torch.zeros(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "\n",
    "    denom = torch.sqrt(running_var + eps)          # (Cout,)\n",
    "    scale = bn_weight / denom                      # (Cout,)\n",
    "\n",
    "    # Fuse weight\n",
    "    fused_weight = conv_weight * scale[:, None, None]\n",
    "\n",
    "    # Fuse bias\n",
    "    fused_bias = (conv_bias - running_mean) * scale + bn_bias\n",
    "\n",
    "    return fused_weight, fused_bias\n",
    "\n",
    "def fuse_deconv_bn_1d(\n",
    "    deconv_weight,     # (Cin, Cout, K)\n",
    "    deconv_bias,       # (Cout,) or None\n",
    "    running_mean,      # (Cout,)\n",
    "    running_var,       # (Cout,)\n",
    "    bn_weight,         # (Cout,)\n",
    "    bn_bias,           # (Cout,)\n",
    "    eps\n",
    "):\n",
    "    Cin, Cout, K = deconv_weight.shape\n",
    "\n",
    "    if deconv_bias is None:\n",
    "        deconv_bias = torch.zeros(\n",
    "            Cout,\n",
    "            device=deconv_weight.device,\n",
    "            dtype=deconv_weight.dtype\n",
    "        )\n",
    "\n",
    "    # BN scale\n",
    "    scale = bn_weight / torch.sqrt(running_var + eps)  # (Cout,)\n",
    "\n",
    "    # Fuse weights (scale on Cout dimension)\n",
    "    fused_weight = deconv_weight * scale.view(1, Cout, 1)\n",
    "\n",
    "    # Fuse bias\n",
    "    fused_bias = (deconv_bias - running_mean) * scale + bn_bias\n",
    "\n",
    "    return fused_weight, fused_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "610993e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_generator(G):\n",
    "    G_fused = GeneratorCNNWGAN_Fused(bias=True).to(device)\n",
    "    G_fused.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encoder\n",
    "        for i in range(1, 5):\n",
    "            e = getattr(G, f\"e{i}\")\n",
    "            fe = getattr(G_fused, f\"e{i}\")\n",
    "\n",
    "            w, b = fuse_conv_bn_1d(\n",
    "                e.conv.weight, e.conv.bias,\n",
    "                e.norm.running_mean, e.norm.running_var,\n",
    "                e.norm.weight, e.norm.bias\n",
    "            )\n",
    "            fe.conv.weight.copy_(w)\n",
    "            fe.conv.bias.copy_(b)\n",
    "\n",
    "        # Bottleneck\n",
    "        for i, blk in enumerate(G.bottleneck):\n",
    "            fblk = G_fused.bottleneck[i]\n",
    "\n",
    "            for j, (c, n) in enumerate([(blk.c1, blk.n1), (blk.c2, blk.n2)]):\n",
    "                w, b = fuse_conv_bn_1d(\n",
    "                    c.weight, c.bias,\n",
    "                    n.running_mean, n.running_var,\n",
    "                    n.weight, n.bias\n",
    "                )\n",
    "                fblk[j*2].weight.copy_(w)\n",
    "                fblk[j*2].bias.copy_(b)\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(1, 5):\n",
    "            d = getattr(G, f\"d{i}\")\n",
    "            fd = getattr(G_fused, f\"d{i}\")\n",
    "\n",
    "            w, b = fuse_deconv_bn_1d(\n",
    "                d.deconv.weight, d.deconv.bias,\n",
    "                d.norm.running_mean, d.norm.running_var,\n",
    "                d.norm.weight, d.norm.bias,\n",
    "                d.norm.eps\n",
    "            )\n",
    "            fd.deconv.weight.copy_(w)\n",
    "            fd.deconv.bias.copy_(b)\n",
    "\n",
    "        # Output\n",
    "        G_fused.out.weight.copy_(G.out.weight)\n",
    "        G_fused.out.bias.copy_(G.out.bias)\n",
    "\n",
    "    return G_fused\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af16c6",
   "metadata": {},
   "source": [
    "# Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da315182",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIAS = True\n",
    "DATA_MODE = 5 # up to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eaf903ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d5_b\n",
      "G: c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d5_b\\cnn_G_20260114_024826.pth\n",
      "D: c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d5_b\\cnn_D_20260114_024826.pth\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.abspath(f\"../models/main3_d{DATA_MODE}_{\"b\" if BIAS else \"nb\"}/\")\n",
    "print(data_path)\n",
    "\n",
    "pattern = re.compile(r\"^cnn_([DG])_\\d{8}_\\d{6}\\.pth$\")\n",
    "\n",
    "cnn_G_path = None\n",
    "cnn_D_path = None\n",
    "\n",
    "for f in os.listdir(data_path):\n",
    "    m = pattern.match(f)\n",
    "    if m:\n",
    "        full = os.path.join(data_path, f)\n",
    "        if m.group(1) == \"G\":\n",
    "            cnn_G_path = full\n",
    "        else:\n",
    "            cnn_D_path = full\n",
    "\n",
    "print(\"G:\", cnn_G_path)\n",
    "print(\"D:\", cnn_D_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "005e7297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratorCNNWGAN(\n",
      "  (e1): ConvBlock1D(\n",
      "    (conv): Conv1d(1, 32, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e2): ConvBlock1D(\n",
      "    (conv): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e3): ConvBlock1D(\n",
      "    (conv): Conv1d(64, 128, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e4): ConvBlock1D(\n",
      "    (conv): Conv1d(128, 256, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (0): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (d1): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d2): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(256, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d3): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(128, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d4): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(64, 16, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (out): Conv1d(16, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      ")\n",
      "CriticPatch1D(\n",
      "  (c1): Conv1d(2, 32, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "  (c2): ConvBlock1D(\n",
      "    (conv): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (c3): ConvBlock1D(\n",
      "    (conv): Conv1d(64, 128, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (c4): ConvBlock1D(\n",
      "    (conv): Conv1d(128, 256, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (out): Conv1d(256, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aryo\\AppData\\Local\\Temp\\ipykernel_3076\\700924479.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  G.load_state_dict(torch.load(cnn_G_path, map_location=device))\n",
      "C:\\Users\\Aryo\\AppData\\Local\\Temp\\ipykernel_3076\\700924479.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  D.load_state_dict(torch.load(cnn_D_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "G = GeneratorCNNWGAN(bias=BIAS).to(device)\n",
    "D = CriticPatch1D(bias=BIAS).to(device)\n",
    "\n",
    "G.load_state_dict(torch.load(cnn_G_path, map_location=device))\n",
    "D.load_state_dict(torch.load(cnn_D_path, map_location=device))\n",
    "\n",
    "print(G.eval())\n",
    "print(D.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35a120e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_fused = fuse_generator(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3096f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_CONFIGS = {\n",
    "    \"Q4.12\": dict(frac_bits=12, int_bits=4, dtype=np.int16),\n",
    "    \"Q10.10\": dict(frac_bits=10, int_bits=10, dtype=np.int32),\n",
    "    \"Q9.14\": dict(frac_bits=14, int_bits=10, dtype=np.int32),\n",
    "}\n",
    "\n",
    "def float_to_q(x, frac_bits, int_bits, dtype):\n",
    "    scale = 1 << frac_bits\n",
    "    total_bits = int_bits + frac_bits\n",
    "    min_val = -(1 << (total_bits - 1))\n",
    "    max_val = (1 << (total_bits - 1)) - 1\n",
    "\n",
    "    xq = np.round(x * scale)\n",
    "    xq = np.clip(xq, min_val, max_val)\n",
    "    return xq.astype(dtype)\n",
    "\n",
    "def q_to_float(x, frac_bits):\n",
    "    return x.astype(np.float32) / (1 << frac_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c5d3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE =  \"Q9.14\" # \"Q4.12\", \"Q10.10\",\"Q9.14\", or \"FLOAT\"\n",
    "\n",
    "TIME_REPEAT = {\n",
    "    \"encoder\":    [256, 128, 64, 32],\n",
    "    \"bottleneck\": [32]*8,\n",
    "    \"decoder\":    [64, 128, 256, 512],\n",
    "    \"out\":        [512]\n",
    "}\n",
    "\n",
    "if TYPE == \"FLOAT\":\n",
    "    MODE = \"FLOAT\"\n",
    "    DTYPE = np.float32\n",
    "else:\n",
    "    MODE = \"FIXED\"\n",
    "    cfg = Q_CONFIGS[TYPE]\n",
    "    FRAC_BITS = cfg[\"frac_bits\"]\n",
    "    INT_BITS  = cfg[\"int_bits\"]\n",
    "    DTYPE     = cfg[\"dtype\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611a195",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52640b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = os.path.abspath(f\"../models/sample_output_{TYPE}/\")\n",
    "os.makedirs(output_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1a97d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== DECODER LAST OUTPUT =====\n",
      "Shape      : (1, 512)\n",
      "Total vals : 512\n"
     ]
    }
   ],
   "source": [
    "# Encoder input\n",
    "G.eval()\n",
    "encoder_input_batched = encoder_input.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_all = G(encoder_input_batched)\n",
    "\n",
    "out_2d_all = out_all.squeeze(0)\n",
    "out_f_all = out_2d_all.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "assert out_f_all.ndim == 2, \"Expected 2D output [x, y]\"\n",
    "X_all, Y_all = out_f_all.shape\n",
    "\n",
    "print(\"===== DECODER LAST OUTPUT =====\")\n",
    "print(f\"Shape      : ({X_all}, {Y_all})\")\n",
    "print(f\"Total vals : {X_all * Y_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2ddce75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== DECODER LAST OUTPUT =====\n",
      "Shape      : (16, 512)\n",
      "Total vals : 8192\n"
     ]
    }
   ],
   "source": [
    "# Decoder last output \n",
    "G.eval()\n",
    "decoder_last_input_batched = decoder_last_input.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_deconv_ref = G.d4.deconv(decoder_last_input_batched)\n",
    "    out_bn_ref     = G.d4.norm(out_deconv_ref)\n",
    "    out_d4_ref     = G.d4.act(out_bn_ref)\n",
    "\n",
    "out_2d_decoder_last = out_d4_ref.squeeze(0)\n",
    "out_f_decoder_last = out_2d_decoder_last.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "assert out_f_decoder_last.ndim == 2, \"Expected 2D output [x, y]\"\n",
    "X_decoder_last, Y_decoder_last = out_f_decoder_last.shape\n",
    "\n",
    "print(\"===== DECODER LAST OUTPUT =====\")\n",
    "print(f\"Shape      : ({X_decoder_last}, {Y_decoder_last})\")\n",
    "print(f\"Total vals : {X_decoder_last * Y_decoder_last}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "503f1c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved outputs:\n",
      "  HEX : encoder_output_d5_format.mem\n",
      "  RAW : encoder_output_d5_raw.mem\n",
      "  Shape: (1, 512), total entries = 512\n"
     ]
    }
   ],
   "source": [
    "# ---- QUANTIZE ----\n",
    "out_q_all = float_to_q(\n",
    "    out_f_all,\n",
    "    frac_bits=FRAC_BITS,\n",
    "    int_bits=INT_BITS,\n",
    "    dtype=DTYPE\n",
    ")\n",
    "\n",
    "mem_name_all_hex = f\"encoder_output_d{DATA_MODE}_format.mem\"\n",
    "mem_name_all_raw = f\"encoder_output_d{DATA_MODE}_raw.mem\"\n",
    "\n",
    "mem_path_all_hex = os.path.join(output_path, mem_name_all_hex)\n",
    "mem_path_all_raw = os.path.join(output_path, mem_name_all_raw)\n",
    "\n",
    "with open(mem_path_all_hex, \"w\") as f_hex, open(mem_path_all_raw, \"w\") as f_raw:\n",
    "    for y in range(Y_all):\n",
    "        for x in range(X_all):\n",
    "            v = int(out_q_all[x, y])\n",
    "\n",
    "            # HEX (two's complement, 32-bit)\n",
    "            f_hex.write(f\"{v & 0xFFFFFFFF:08X}\\n\")\n",
    "\n",
    "            # RAW signed decimal\n",
    "            f_raw.write(f\"{v}\\n\")\n",
    "\n",
    "print(\"Saved outputs:\")\n",
    "print(f\"  HEX : {mem_name_all_hex}\")\n",
    "print(f\"  RAW : {mem_name_all_raw}\")\n",
    "print(f\"  Shape: ({X_all}, {Y_all}), total entries = {X_all * Y_all}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e88070a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved outputs:\n",
      "  HEX : decoder_last_output_d5_format.mem\n",
      "  RAW : decoder_last_output_d5_raw.mem\n",
      "  Shape: (16, 512), total entries = 8192\n"
     ]
    }
   ],
   "source": [
    "out_q_decoder_last = float_to_q(\n",
    "    out_f_decoder_last,\n",
    "    frac_bits=FRAC_BITS,\n",
    "    int_bits=INT_BITS,\n",
    "    dtype=DTYPE\n",
    ")\n",
    "\n",
    "mem_name_decoder_last_hex = f\"decoder_last_output_d{DATA_MODE}_format.mem\"\n",
    "mem_name_decoder_last_raw = f\"decoder_last_output_d{DATA_MODE}_raw.mem\"\n",
    "\n",
    "mem_path_decoder_last_hex = os.path.join(output_path, mem_name_decoder_last_hex)\n",
    "mem_path_decoder_last_raw = os.path.join(output_path, mem_name_decoder_last_raw)\n",
    "\n",
    "with open(mem_path_decoder_last_hex, \"w\") as f_hex, open(mem_path_decoder_last_raw, \"w\") as f_raw:\n",
    "    for x in range(X_decoder_last):\n",
    "        for y in range(Y_decoder_last):\n",
    "            v = int(out_q_decoder_last[x, y])\n",
    "\n",
    "            # HEX (two's complement, 32-bit)\n",
    "            f_hex.write(f\"{v & 0xFFFFFFFF:08X}\\n\")\n",
    "\n",
    "            # RAW signed decimal\n",
    "            f_raw.write(f\"{v}\\n\")\n",
    "\n",
    "print(\"Saved outputs:\")\n",
    "print(f\"  HEX : {mem_name_decoder_last_hex}\")\n",
    "print(f\"  RAW : {mem_name_decoder_last_raw}\")\n",
    "print(f\"  Shape: ({X_decoder_last}, {Y_decoder_last}), total entries = {X_decoder_last * Y_decoder_last}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d70b4430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6341, -1.2250,  0.0230,  ...,  0.2211,  0.2900, -0.1713],\n",
      "        [-0.4107, -1.2087,  1.7604,  ..., -0.4288, -0.4538,  0.9026],\n",
      "        [-0.3861,  0.3538,  0.8009,  ...,  0.5962, -1.7025, -0.3186],\n",
      "        ...,\n",
      "        [ 0.4007, -0.3718, -1.2671,  ...,  1.2490,  0.5307, -1.2224],\n",
      "        [-0.6826,  0.6642,  1.0837,  ...,  0.5124, -0.3639,  0.7698],\n",
      "        [-0.7566, -0.0156, -0.9006,  ..., -1.7253,  0.0457,  0.4532]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(decoder_last_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fbe218d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[[ 0.0228,  0.1653,  0.0051, -0.1294],\n",
      "         [-0.0901, -0.0611,  0.1014,  0.0291],\n",
      "         [ 0.0008, -0.0930,  0.0647, -0.0078],\n",
      "         ...,\n",
      "         [-0.0497, -0.1010, -0.0566,  0.0056],\n",
      "         [-0.0846,  0.0546, -0.0636,  0.0020],\n",
      "         [-0.0536,  0.0287, -0.0322,  0.0185]],\n",
      "\n",
      "        [[ 0.0769, -0.0828,  0.1180,  0.0688],\n",
      "         [-0.0002, -0.0019,  0.0623, -0.1138],\n",
      "         [-0.0240, -0.0500, -0.0507, -0.0429],\n",
      "         ...,\n",
      "         [-0.1126,  0.0289, -0.1446,  0.1001],\n",
      "         [ 0.0340, -0.0484,  0.0286,  0.1241],\n",
      "         [ 0.0700, -0.0342,  0.0891,  0.0096]],\n",
      "\n",
      "        [[-0.0995,  0.0514, -0.1144,  0.0107],\n",
      "         [ 0.0730, -0.0616, -0.0276, -0.0601],\n",
      "         [ 0.0722, -0.0767,  0.0243, -0.0774],\n",
      "         ...,\n",
      "         [-0.0025,  0.0232, -0.0602, -0.0437],\n",
      "         [-0.1055, -0.0083, -0.1348, -0.0881],\n",
      "         [ 0.0371,  0.0029,  0.0282, -0.1540]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.0081,  0.1087, -0.0645, -0.0016],\n",
      "         [ 0.0173,  0.0834,  0.0496,  0.0190],\n",
      "         [ 0.0592,  0.0834, -0.0762,  0.0458],\n",
      "         ...,\n",
      "         [ 0.0504,  0.0523,  0.0136, -0.0155],\n",
      "         [-0.0684,  0.0750,  0.0947, -0.0784],\n",
      "         [-0.0677,  0.0136, -0.0877, -0.0751]],\n",
      "\n",
      "        [[-0.0170,  0.0655,  0.0228,  0.0407],\n",
      "         [-0.0652, -0.0048, -0.0377, -0.0049],\n",
      "         [-0.0536, -0.0200, -0.0516,  0.0238],\n",
      "         ...,\n",
      "         [ 0.0045,  0.0229,  0.0349,  0.0101],\n",
      "         [-0.0284, -0.0033, -0.0296,  0.0098],\n",
      "         [ 0.0460, -0.0626, -0.0033, -0.1069]],\n",
      "\n",
      "        [[-0.0236, -0.0022,  0.0235,  0.0928],\n",
      "         [-0.0710, -0.0260,  0.0749, -0.0163],\n",
      "         [-0.1043, -0.0205, -0.0234,  0.0348],\n",
      "         ...,\n",
      "         [-0.0744,  0.0381, -0.0641, -0.0003],\n",
      "         [-0.0714,  0.0239, -0.0363, -0.0559],\n",
      "         [ 0.0294, -0.0875,  0.0659,  0.0922]]], device='cuda:0',\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.2676,  0.3283,  0.0901, -0.2882,  0.2407,  0.3677, -0.1240,  0.0153,\n",
      "        -0.0107,  0.1948,  0.0551,  0.1733,  0.1010,  0.1766, -0.1708,  0.1769],\n",
      "       device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(G_fused.d4.deconv.weight)\n",
    "print(G_fused.d4.deconv.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d62372e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv1d_output_at(\n",
    "    x,          # input tensor, shape (Cin, Tin)\n",
    "    W,          # deconv weight, shape (Cin, Cout, K)\n",
    "    b,          # bias, shape (Cout,)\n",
    "    oc,         # output channel index (0-based)\n",
    "    t_out,      # output time index (0-based)\n",
    "    FRAC_BITS,\n",
    "    INT_BITS,\n",
    "    DTYPE,\n",
    "    stride=2,\n",
    "    padding=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute y[oc, t_out] for ConvTranspose1d using default stride/padding.\n",
    "\n",
    "    This is a GOLDEN reference for HW:\n",
    "    - No tiling\n",
    "    - No PE logic\n",
    "    - Pure math\n",
    "    \"\"\"\n",
    "\n",
    "    Cin, Tin = x.shape\n",
    "    _, Cout, K = W.shape\n",
    "\n",
    "    assert 0 <= oc < Cout\n",
    "\n",
    "    # --- FLOAT ACCUMULATION ---\n",
    "    y_float = b[oc].item()\n",
    "\n",
    "    for k_pos in range(K):\n",
    "        # Transposed-conv inverse mapping\n",
    "        t_in = t_out + padding - k_pos\n",
    "\n",
    "        # Must align with stride\n",
    "        if t_in % stride != 0:\n",
    "            continue\n",
    "\n",
    "        t_in //= stride\n",
    "\n",
    "        # Bounds check\n",
    "        if t_in < 0 or t_in >= Tin:\n",
    "            continue\n",
    "\n",
    "        for cin in range(Cin):\n",
    "            y_float += (\n",
    "                x[cin, t_in].item()\n",
    "                * W[cin, oc, k_pos].item()\n",
    "            )\n",
    "\n",
    "    # --- FIXED-POINT ---\n",
    "    y_fixed = float_to_q(\n",
    "        y_float,\n",
    "        frac_bits=FRAC_BITS,\n",
    "        int_bits=INT_BITS,\n",
    "        dtype=DTYPE\n",
    "    )\n",
    "\n",
    "    return y_float, y_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "27c1afee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_float[0,0] = 0\n",
      "y_fixed[0,0] = 0\n"
     ]
    }
   ],
   "source": [
    "y_float, y_fixed = deconv1d_output_at(\n",
    "    x=decoder_last_input,\n",
    "    W=G_fused.d4.deconv.weight,\n",
    "    b=G_fused.d4.deconv.bias,\n",
    "    oc=0,\n",
    "    t_out=2,\n",
    "    FRAC_BITS=FRAC_BITS,\n",
    "    INT_BITS=INT_BITS,\n",
    "    DTYPE=DTYPE,\n",
    ")\n",
    "\n",
    "print(\"y_float[0,0] =\", y_float if y_float > 0 else 0)\n",
    "print(\"y_fixed[0,0] =\", y_fixed if y_fixed > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e259e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
