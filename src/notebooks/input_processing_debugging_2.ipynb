{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "e95c8b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b5f73d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "bbc9ad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "SEED = 1234\n",
    "\n",
    "# Python & NumPy\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# PyTorch\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Determinism flags\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "4c89c5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = torch.randn(1, 512, device=device)\n",
    "bottleneck_input = torch.randn(256, 32, device=device)\n",
    "decoder_first_input = torch.randn(256, 32, device=device)\n",
    "decoder_last_input = torch.randn(64, 256, device=device)\n",
    "out_input = torch.randn(16, 512, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c498f9f",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "ce1ba420",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv1d -> BatchNorm1d -> Activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=15, s=2, p=7, bias=True, act=\"lrelu\"):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=bias)\n",
    "        self.norm = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        if act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act must be 'lrelu' or 'relu'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "class DeconvBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvTranspose1d -> BatchNorm1d -> Activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=4, s=2, p=1, bias=True, act=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=bias)\n",
    "        self.norm = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act must be 'relu' or 'lrelu'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.deconv(x)))\n",
    "\n",
    "\n",
    "class ResBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block: (Conv -> BN -> ReLU) x2 + skip\n",
    "    Keeps same channel count and length.\n",
    "    \"\"\"\n",
    "    def __init__(self, ch, k=7, p=3, bias=True):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv1d(ch, ch, kernel_size=k, stride=1, padding=p, bias=bias)\n",
    "        self.n1 = nn.BatchNorm1d(ch)\n",
    "        self.c2 = nn.Conv1d(ch, ch, kernel_size=k, stride=1, padding=p, bias=bias)\n",
    "        self.n2 = nn.BatchNorm1d(ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.n1(self.c1(x)))\n",
    "        h = self.n2(self.c2(h))\n",
    "        return F.relu(x + h)\n",
    "    \n",
    "class MultiScaleResBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale residual block: parallel conv branches (k=3,5,7) then fuse.\n",
    "    Keeps same channel count and length.\n",
    "    \"\"\"\n",
    "    def __init__(self, ch, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=3, padding=1, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.b5 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=5, padding=2, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.b7 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=7, padding=3, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=1, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.b3(x) + self.b5(x) + self.b7(x)\n",
    "        h = self.fuse(h)\n",
    "        return F.relu(x + h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "id": "19b6d55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN Generator (U-Net-ish + Res bottleneck)\n",
    "class GeneratorCNNWGAN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN U-Net-ish generator for EEG denoising (WGAN).\n",
    "    Input : (B, 1, 512) noisy_norm\n",
    "    Output: (B, 1, 512) clean_norm_hat\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=32, bottleneck_blocks=4, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.e1 = ConvBlock1D(1, base_ch,       k=16, s=2, p=7, bias=bias, act=\"lrelu\")      # 512 -> 256\n",
    "        self.e2 = ConvBlock1D(base_ch, base_ch*2, k=16, s=2, p=7, bias=bias, act=\"lrelu\")    # 256 -> 128\n",
    "        self.e3 = ConvBlock1D(base_ch*2, base_ch*4, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 128 -> 64\n",
    "        self.e4 = ConvBlock1D(base_ch*4, base_ch*8, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 64 -> 32\n",
    "\n",
    "        # Bottleneck\n",
    "        bn_ch = base_ch * 8\n",
    "        self.bottleneck = nn.Sequential(*[\n",
    "            ResBlock1D(bn_ch, k=7, p=3, bias=bias) for _ in range(bottleneck_blocks)\n",
    "        ])\n",
    "\n",
    "        # Decoder (concat doubles channels)\n",
    "        self.d1 = DeconvBlock1D(bn_ch, base_ch*4,   k=4, s=2, p=1, bias=bias, act=\"relu\")     # 32 -> 64\n",
    "        self.d2 = DeconvBlock1D(base_ch*8, base_ch*2, k=4, s=2, p=1,bias=bias, act=\"relu\")   # 64 -> 128\n",
    "        self.d3 = DeconvBlock1D(base_ch*4, base_ch,   k=4, s=2, p=1, bias=bias, act=\"relu\")   # 128 -> 256\n",
    "        self.d4 = DeconvBlock1D(base_ch*2, base_ch//2, k=4, s=2, p=1, bias=bias, act=\"relu\")  # 256 -> 512\n",
    "\n",
    "        # Head (linear output recommended for normalized signals)\n",
    "        self.out = nn.Conv1d(base_ch//2, 1, kernel_size=7, stride=1, padding=3, bias=bias)\n",
    "\n",
    "    def forward(self, y):\n",
    "        # Encoder\n",
    "        s1 = self.e1(y)   # (B, base, 256)\n",
    "        s2 = self.e2(s1)  # (B, 2b, 128)\n",
    "        s3 = self.e3(s2)  # (B, 4b, 64)\n",
    "        s4 = self.e4(s3)  # (B, 8b, 32)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(s4)\n",
    "\n",
    "        # Decoder + skip connections\n",
    "        d1 = self.d1(b)                  # (B, 4b, 64)\n",
    "        d1 = torch.cat([d1, s3], dim=1)  # (B, 8b, 64)\n",
    "\n",
    "        d2 = self.d2(d1)                 # (B, 2b, 128)\n",
    "        d2 = torch.cat([d2, s2], dim=1)  # (B, 4b, 128)\n",
    "\n",
    "        d3 = self.d3(d2)                 # (B, b, 256)\n",
    "        d3 = torch.cat([d3, s1], dim=1)  # (B, 2b, 256)\n",
    "\n",
    "        d4 = self.d4(d3)                 # (B, b/2, 512)\n",
    "\n",
    "        return self.out(d4)              # (B, 1, 512)\n",
    "    \n",
    "# Patch Critic (shared by CNN/ResCNN)\n",
    "class CriticPatch1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional PatchGAN critic for WGAN:\n",
    "      D(y, x) -> patch scores\n",
    "    y,x: (B,1,512)\n",
    "    output: (B,1,32)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=32, bias=True):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv1d(2, base_ch, kernel_size=16, stride=2, padding=7, bias=bias)  # 512 -> 256\n",
    "        self.c2 = ConvBlock1D(base_ch, base_ch*2, k=16, s=2, p=7, bias=bias, act=\"lrelu\")    # 256 -> 128\n",
    "        self.c3 = ConvBlock1D(base_ch*2, base_ch*4, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 128 -> 64\n",
    "        self.c4 = ConvBlock1D(base_ch*4, base_ch*8, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 64 -> 32\n",
    "        self.out = nn.Conv1d(base_ch*8, 1, kernel_size=7, stride=1, padding=3, bias=bias)   # 32 -> 32\n",
    "\n",
    "    def forward(self, y, x):\n",
    "        h = torch.cat([y, x], dim=1)  # (B,2,512)\n",
    "        h = F.leaky_relu(self.c1(h), 0.2, inplace=True)\n",
    "        h = self.c2(h)\n",
    "        h = self.c3(h)\n",
    "        h = self.c4(h)\n",
    "        return self.out(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "id": "ac198d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedConvBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s, p, bias=True, act=\"lrelu\"):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, k, s, p, bias=bias)\n",
    "\n",
    "        if act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "\n",
    "\n",
    "class FusedDeconvBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s, p, bias=True, act=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(in_ch, out_ch, k, s, p, bias=bias)\n",
    "\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.deconv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "id": "97b7ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorCNNWGAN_Fused(nn.Module):\n",
    "    def __init__(self, base_ch=32, bottleneck_blocks=4, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.e1 = FusedConvBlock1D(1, base_ch, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e2 = FusedConvBlock1D(base_ch, base_ch*2, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e3 = FusedConvBlock1D(base_ch*2, base_ch*4, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e4 = FusedConvBlock1D(base_ch*4, base_ch*8, 16, 2, 7, bias, \"lrelu\")\n",
    "\n",
    "        self.bottleneck = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(base_ch*8, base_ch*8, 7, 1, 3, bias=bias),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv1d(base_ch*8, base_ch*8, 7, 1, 3, bias=bias),\n",
    "            )\n",
    "            for _ in range(bottleneck_blocks)\n",
    "        ])\n",
    "\n",
    "        self.d1 = FusedDeconvBlock1D(base_ch*8, base_ch*4, 4, 2, 1, bias)\n",
    "        self.d2 = FusedDeconvBlock1D(base_ch*8, base_ch*2, 4, 2, 1, bias)\n",
    "        self.d3 = FusedDeconvBlock1D(base_ch*4, base_ch, 4, 2, 1, bias)\n",
    "        self.d4 = FusedDeconvBlock1D(base_ch*2, base_ch//2, 4, 2, 1, bias)\n",
    "\n",
    "        self.out = nn.Conv1d(base_ch//2, 1, 7, 1, 3, bias=bias)\n",
    "\n",
    "    def forward(self, y):\n",
    "        s1 = self.e1(y)\n",
    "        s2 = self.e2(s1)\n",
    "        s3 = self.e3(s2)\n",
    "        s4 = self.e4(s3)\n",
    "\n",
    "        b = s4\n",
    "        for blk in self.bottleneck:\n",
    "            b = F.relu(b + blk(b))\n",
    "\n",
    "        d1 = torch.cat([self.d1(b), s3], dim=1)\n",
    "        d2 = torch.cat([self.d2(d1), s2], dim=1)\n",
    "        d3 = torch.cat([self.d3(d2), s1], dim=1)\n",
    "        d4 = self.d4(d3)\n",
    "\n",
    "        return self.out(d4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "id": "4d4b07b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_bn_1d(\n",
    "    conv_weight,        # (Cout, Cin, K)\n",
    "    conv_bias,          # (Cout,) or None\n",
    "    running_mean,       # (Cout,)\n",
    "    running_var,        # (Cout,)\n",
    "    bn_weight,          # (Cout,) or None (gamma)\n",
    "    bn_bias,            # (Cout,) or None (beta)\n",
    "    eps=1e-5\n",
    "):\n",
    "    Cout = conv_weight.shape[0]\n",
    "\n",
    "    if bn_weight is None:\n",
    "        bn_weight = torch.ones(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "    if bn_bias is None:\n",
    "        bn_bias = torch.zeros(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "    if conv_bias is None:\n",
    "        conv_bias = torch.zeros(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "\n",
    "    denom = torch.sqrt(running_var + eps)          # (Cout,)\n",
    "    scale = bn_weight / denom                      # (Cout,)\n",
    "\n",
    "    # Fuse weight\n",
    "    fused_weight = conv_weight * scale[:, None, None]\n",
    "\n",
    "    # Fuse bias\n",
    "    fused_bias = (conv_bias - running_mean) * scale + bn_bias\n",
    "\n",
    "    return fused_weight, fused_bias\n",
    "\n",
    "def fuse_deconv_bn_1d(\n",
    "    deconv_weight,     # (Cin, Cout, K)\n",
    "    deconv_bias,       # (Cout,) or None\n",
    "    running_mean,      # (Cout,)\n",
    "    running_var,       # (Cout,)\n",
    "    bn_weight,         # (Cout,)\n",
    "    bn_bias,           # (Cout,)\n",
    "    eps\n",
    "):\n",
    "    Cin, Cout, K = deconv_weight.shape\n",
    "\n",
    "    if deconv_bias is None:\n",
    "        deconv_bias = torch.zeros(\n",
    "            Cout,\n",
    "            device=deconv_weight.device,\n",
    "            dtype=deconv_weight.dtype\n",
    "        )\n",
    "\n",
    "    # BN scale\n",
    "    scale = bn_weight / torch.sqrt(running_var + eps)  # (Cout,)\n",
    "\n",
    "    # Fuse weights (scale on Cout dimension)\n",
    "    fused_weight = deconv_weight * scale.view(1, Cout, 1)\n",
    "\n",
    "    # Fuse bias\n",
    "    fused_bias = (deconv_bias - running_mean) * scale + bn_bias\n",
    "\n",
    "    return fused_weight, fused_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "610993e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_generator(G):\n",
    "    G_fused = GeneratorCNNWGAN_Fused(bias=True).to(device)\n",
    "    G_fused.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encoder\n",
    "        for i in range(1, 5):\n",
    "            e = getattr(G, f\"e{i}\")\n",
    "            fe = getattr(G_fused, f\"e{i}\")\n",
    "\n",
    "            w, b = fuse_conv_bn_1d(\n",
    "                e.conv.weight, e.conv.bias,\n",
    "                e.norm.running_mean, e.norm.running_var,\n",
    "                e.norm.weight, e.norm.bias\n",
    "            )\n",
    "            fe.conv.weight.copy_(w)\n",
    "            fe.conv.bias.copy_(b)\n",
    "\n",
    "        # Bottleneck\n",
    "        for i, blk in enumerate(G.bottleneck):\n",
    "            fblk = G_fused.bottleneck[i]\n",
    "\n",
    "            for j, (c, n) in enumerate([(blk.c1, blk.n1), (blk.c2, blk.n2)]):\n",
    "                w, b = fuse_conv_bn_1d(\n",
    "                    c.weight, c.bias,\n",
    "                    n.running_mean, n.running_var,\n",
    "                    n.weight, n.bias\n",
    "                )\n",
    "                fblk[j*2].weight.copy_(w)\n",
    "                fblk[j*2].bias.copy_(b)\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(1, 5):\n",
    "            d = getattr(G, f\"d{i}\")\n",
    "            fd = getattr(G_fused, f\"d{i}\")\n",
    "\n",
    "            w, b = fuse_deconv_bn_1d(\n",
    "                d.deconv.weight, d.deconv.bias,\n",
    "                d.norm.running_mean, d.norm.running_var,\n",
    "                d.norm.weight, d.norm.bias,\n",
    "                d.norm.eps\n",
    "            )\n",
    "            fd.deconv.weight.copy_(w)\n",
    "            fd.deconv.bias.copy_(b)\n",
    "\n",
    "        # Output\n",
    "        G_fused.out.weight.copy_(G.out.weight)\n",
    "        G_fused.out.bias.copy_(G.out.bias)\n",
    "\n",
    "    return G_fused\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af16c6",
   "metadata": {},
   "source": [
    "# Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "da315182",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIAS = True\n",
    "DATA_MODE = 5 # up to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "eaf903ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d5_b\n",
      "G: c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d5_b\\cnn_G_20260114_024826.pth\n",
      "D: c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d5_b\\cnn_D_20260114_024826.pth\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.abspath(f\"../models/main3_d{DATA_MODE}_{\"b\" if BIAS else \"nb\"}/\")\n",
    "print(data_path)\n",
    "\n",
    "pattern = re.compile(r\"^cnn_([DG])_\\d{8}_\\d{6}\\.pth$\")\n",
    "\n",
    "cnn_G_path = None\n",
    "cnn_D_path = None\n",
    "\n",
    "for f in os.listdir(data_path):\n",
    "    m = pattern.match(f)\n",
    "    if m:\n",
    "        full = os.path.join(data_path, f)\n",
    "        if m.group(1) == \"G\":\n",
    "            cnn_G_path = full\n",
    "        else:\n",
    "            cnn_D_path = full\n",
    "\n",
    "print(\"G:\", cnn_G_path)\n",
    "print(\"D:\", cnn_D_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "005e7297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratorCNNWGAN(\n",
      "  (e1): ConvBlock1D(\n",
      "    (conv): Conv1d(1, 32, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e2): ConvBlock1D(\n",
      "    (conv): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e3): ConvBlock1D(\n",
      "    (conv): Conv1d(64, 128, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e4): ConvBlock1D(\n",
      "    (conv): Conv1d(128, 256, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (0): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (d1): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d2): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(256, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d3): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(128, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d4): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(64, 16, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (out): Conv1d(16, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      ")\n",
      "CriticPatch1D(\n",
      "  (c1): Conv1d(2, 32, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "  (c2): ConvBlock1D(\n",
      "    (conv): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (c3): ConvBlock1D(\n",
      "    (conv): Conv1d(64, 128, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (c4): ConvBlock1D(\n",
      "    (conv): Conv1d(128, 256, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (out): Conv1d(256, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aryo\\AppData\\Local\\Temp\\ipykernel_3556\\700924479.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  G.load_state_dict(torch.load(cnn_G_path, map_location=device))\n",
      "C:\\Users\\Aryo\\AppData\\Local\\Temp\\ipykernel_3556\\700924479.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  D.load_state_dict(torch.load(cnn_D_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "G = GeneratorCNNWGAN(bias=BIAS).to(device)\n",
    "D = CriticPatch1D(bias=BIAS).to(device)\n",
    "\n",
    "G.load_state_dict(torch.load(cnn_G_path, map_location=device))\n",
    "D.load_state_dict(torch.load(cnn_D_path, map_location=device))\n",
    "\n",
    "print(G.eval())\n",
    "print(D.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "35a120e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_fused = fuse_generator(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "c3096f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_CONFIGS = {\n",
    "    \"Q4.12\": dict(frac_bits=12, int_bits=4, dtype=np.int16),\n",
    "    \"Q10.10\": dict(frac_bits=10, int_bits=10, dtype=np.int32),\n",
    "    \"Q9.14\": dict(frac_bits=14, int_bits=10, dtype=np.int32),\n",
    "}\n",
    "\n",
    "def float_to_q(x, frac_bits, int_bits, dtype):\n",
    "    scale = 1 << frac_bits\n",
    "    total_bits = int_bits + frac_bits\n",
    "    min_val = -(1 << (total_bits - 1))\n",
    "    max_val = (1 << (total_bits - 1)) - 1\n",
    "\n",
    "    xq = np.round(x * scale)\n",
    "    xq = np.clip(xq, min_val, max_val)\n",
    "    return xq.astype(dtype)\n",
    "\n",
    "def q_to_float(x, frac_bits):\n",
    "    return x.astype(np.float32) / (1 << frac_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "0c5d3c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE =  \"Q9.14\" # \"Q4.12\", \"Q10.10\",\"Q9.14\", or \"FLOAT\"\n",
    "\n",
    "TIME_REPEAT = {\n",
    "    \"encoder\":    [256, 128, 64, 32],\n",
    "    \"bottleneck\": [32]*8,\n",
    "    \"decoder\":    [64, 128, 256, 512],\n",
    "    \"out\":        [512]\n",
    "}\n",
    "\n",
    "if TYPE == \"FLOAT\":\n",
    "    MODE = \"FLOAT\"\n",
    "    DTYPE = np.float32\n",
    "else:\n",
    "    MODE = \"FIXED\"\n",
    "    cfg = Q_CONFIGS[TYPE]\n",
    "    FRAC_BITS = cfg[\"frac_bits\"]\n",
    "    INT_BITS  = cfg[\"int_bits\"]\n",
    "    DTYPE     = cfg[\"dtype\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886261cc",
   "metadata": {},
   "source": [
    "# Mem Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "bded771b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_part(param_name):\n",
    "    if param_name.startswith((\"e1.\", \"e2.\", \"e3.\", \"e4.\")):\n",
    "        return \"encoder\"\n",
    "    elif param_name.startswith(\"bottleneck.\"):\n",
    "        return \"bottleneck\"\n",
    "    elif param_name.startswith((\"d1.\", \"d2.\", \"d3.\", \"d4.\")):\n",
    "        return \"decoder\"\n",
    "    elif param_name.startswith((\"out.\")):\n",
    "        return \"out\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown param group: {param_name}\")\n",
    "    \n",
    "def is_bias(name):\n",
    "    return name.endswith(\".bias\")\n",
    "\n",
    "def is_weight(name):\n",
    "    return name.endswith(\".weight\")\n",
    "\n",
    "def expand_bias(bias, repeat_n):\n",
    "    # bias shape: [C]\n",
    "    # output shape: [C * repeat_n]\n",
    "    return np.repeat(bias, repeat_n)\n",
    "\n",
    "def flatten_conv_weight(w):\n",
    "    # shape [x, y, z]\n",
    "    return w.reshape(-1)\n",
    "\n",
    "def flatten_deconv_weight(w):\n",
    "    # Original shape: (x, y, z)\n",
    "    # Target iteration order (Slowest to Fastest): y -> z -> x\n",
    "    # This matches the indices: [0,0,0], [1,0,0], [0,0,1], [1,0,1]...\n",
    "    w_perm = np.transpose(w, (1, 2, 0)) \n",
    "    return w_perm.reshape(-1)\n",
    "\n",
    "def write_mem(path, data):\n",
    "    with open(path, \"w\") as f:\n",
    "        for v in data:\n",
    "            if data.dtype == np.int16:\n",
    "                f.write(f\"{int(v) & 0xFFFF:04X}\\n\")\n",
    "            elif data.dtype == np.int32:\n",
    "                f.write(f\"{int(v) & 0xFFFFFFFF:08X}\\n\")\n",
    "            elif data.dtype == np.float32:\n",
    "                bits = np.frombuffer(\n",
    "                    np.float32(v).tobytes(), dtype=np.uint32\n",
    "                )[0]\n",
    "                f.write(f\"{bits:08X}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "id": "d168d4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {\n",
    "    part: {\n",
    "        \"weight\": {\"data\": [], \"offsets\": {}, \"cursor\": 0},\n",
    "        \"bias\":   {\"data\": [], \"offsets\": {}, \"cursor\": 0}\n",
    "    }\n",
    "    for part in [\"encoder\", \"bottleneck\", \"decoder\", \"out\"]\n",
    "}\n",
    "\n",
    "bias_layer_counter = {\n",
    "    \"encoder\": 0,\n",
    "    \"bottleneck\": 0,\n",
    "    \"decoder\": 0,\n",
    "    \"out\": 0\n",
    "}\n",
    "\n",
    "ref_param_sum = 0\n",
    "\n",
    "# ---- Global statistics (NO bias repetition) ----\n",
    "w_min = +np.inf\n",
    "w_max = -np.inf\n",
    "w_sum = 0\n",
    "w_cnt = 0\n",
    "\n",
    "b_min = +np.inf\n",
    "b_max = -np.inf\n",
    "b_sum = 0\n",
    "b_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "e1572c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[encoder   ][weight] e1.conv.weight                 -> flat_size=512\n",
      "[encoder   ][bias  ] e1.conv.bias                   -> flat_size=8192 (repeat=256)\n",
      "[encoder   ][weight] e2.conv.weight                 -> flat_size=32768\n",
      "[encoder   ][bias  ] e2.conv.bias                   -> flat_size=8192 (repeat=128)\n",
      "[encoder   ][weight] e3.conv.weight                 -> flat_size=131072\n",
      "[encoder   ][bias  ] e3.conv.bias                   -> flat_size=8192 (repeat=64)\n",
      "[encoder   ][weight] e4.conv.weight                 -> flat_size=524288\n",
      "[encoder   ][bias  ] e4.conv.bias                   -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.0.0.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.0.0.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.0.2.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.0.2.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.1.0.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.1.0.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.1.2.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.1.2.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.2.0.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.2.0.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.2.2.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.2.2.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.3.0.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.3.0.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.3.2.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.3.2.bias            -> flat_size=8192 (repeat=32)\n",
      "[decoder   ][weight] d1.deconv.weight               -> flat_size=131072 (deconv reorder)\n",
      "[decoder   ][bias  ] d1.deconv.bias                 -> flat_size=8192 (repeat=64)\n",
      "[decoder   ][weight] d2.deconv.weight               -> flat_size=65536 (deconv reorder)\n",
      "[decoder   ][bias  ] d2.deconv.bias                 -> flat_size=8192 (repeat=128)\n",
      "[decoder   ][weight] d3.deconv.weight               -> flat_size=16384 (deconv reorder)\n",
      "[decoder   ][bias  ] d3.deconv.bias                 -> flat_size=8192 (repeat=256)\n",
      "[decoder   ][weight] d4.deconv.weight               -> flat_size=4096 (deconv reorder)\n",
      "[decoder   ][bias  ] d4.deconv.bias                 -> flat_size=8192 (repeat=512)\n",
      "[out       ][weight] out.weight                     -> flat_size=112\n",
      "[out       ][bias  ] out.bias                       -> flat_size=512 (repeat=512)\n",
      "\n",
      "===== BIAS LAYER CONSUMPTION CHECK =====\n",
      "encoder   : used 4 / expected 4  [OK]\n",
      "bottleneck: used 8 / expected 8  [OK]\n",
      "decoder   : used 4 / expected 4  [OK]\n",
      "out       : used 1 / expected 1  [OK]\n"
     ]
    }
   ],
   "source": [
    "for name, param in G_fused.state_dict().items():\n",
    "    arr_f = param.cpu().numpy().astype(np.float32)\n",
    "    part = classify_part(name) # \"encoder\", \"bottneck\", \"decoder\", or \"out\"\n",
    "    wtype = \"bias\" if is_bias(name) else \"weight\"\n",
    "\n",
    "    # Quantize or float\n",
    "    if MODE == \"FLOAT\":\n",
    "        arr = arr_f.astype(np.float32)\n",
    "        dtype_str = \"float32\"\n",
    "        meta = {}\n",
    "    else:\n",
    "        arr = float_to_q(\n",
    "            arr_f,\n",
    "            frac_bits=FRAC_BITS,\n",
    "            int_bits=INT_BITS,\n",
    "            dtype=DTYPE\n",
    "        )\n",
    "        dtype_str = TYPE\n",
    "        meta = {\"frac_bits\": FRAC_BITS, \"int_bits\": INT_BITS}\n",
    "\n",
    "    # ---- Apply ordering rules ----\n",
    "    if wtype == \"weight\":\n",
    "        ref_param_sum += arr.size\n",
    "        w_min = min(w_min, arr.min())\n",
    "        w_max = max(w_max, arr.max())\n",
    "        w_sum += arr.sum()\n",
    "        w_cnt += arr.size\n",
    "\n",
    "        if part == \"decoder\":\n",
    "            arr_flat = flatten_deconv_weight(arr)\n",
    "            reorder_note = \" (deconv reorder)\"\n",
    "        else:\n",
    "            arr_flat = flatten_conv_weight(arr)\n",
    "            reorder_note = \"\"\n",
    "    else:\n",
    "        # bias\n",
    "        ref_param_sum += arr.size\n",
    "        b_min = min(b_min, arr.min())\n",
    "        b_max = max(b_max, arr.max())\n",
    "        b_sum += arr.sum()\n",
    "        b_cnt += arr.size\n",
    "\n",
    "        layer_idx = bias_layer_counter[part]\n",
    "\n",
    "        if layer_idx >= len(TIME_REPEAT[part]):\n",
    "            raise RuntimeError(\n",
    "                f\"Too many bias layers in {part}: \"\n",
    "                f\"expected {len(TIME_REPEAT[part])}, got more\"\n",
    "            )\n",
    "\n",
    "        repeat_n = TIME_REPEAT[part][layer_idx]\n",
    "        bias_layer_counter[part] += 1\n",
    "        \n",
    "        arr_flat = expand_bias(arr, repeat_n)\n",
    "        reorder_note = f\" (repeat={repeat_n})\"\n",
    "\n",
    "    # ---- PRINT SIZE CHECK (NEW) ----\n",
    "    print(\n",
    "        f\"[{part:10}][{wtype:6}] {name:30} -> \"\n",
    "        f\"flat_size={arr_flat.size}{reorder_note}\"\n",
    "    )\n",
    "\n",
    "    c = containers[part][wtype]\n",
    "    size = arr_flat.size\n",
    "\n",
    "    c[\"offsets\"][name] = {\n",
    "        \"offset\": int(c[\"cursor\"]),\n",
    "        \"shape\": list(arr_flat.shape),\n",
    "        \"dtype\": dtype_str,\n",
    "        **meta\n",
    "    }\n",
    "\n",
    "    c[\"data\"].append(arr_flat)\n",
    "    c[\"cursor\"] += size\n",
    "\n",
    "print(\"\\n===== BIAS LAYER CONSUMPTION CHECK =====\")\n",
    "for part, cnt in bias_layer_counter.items():\n",
    "    expected = len(TIME_REPEAT[part])\n",
    "    status = \"OK\" if cnt == expected else \"MISMATCH\"\n",
    "    print(f\"{part:10}: used {cnt} / expected {expected}  [{status}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "id": "9401ca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(\n",
    "    os.path.join(data_path, \"..\",\n",
    "    f\"Test_G_d{DATA_MODE}_{TYPE}\")\n",
    ")\n",
    "os.makedirs(base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "id": "203ba66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved encoder/weight: 688640\n",
      "Saved encoder/bias: 32768\n",
      "Saved bottleneck/weight: 3670016\n",
      "Saved bottleneck/bias: 65536\n",
      "Saved decoder/weight: 217088\n",
      "Saved decoder/bias: 32768\n",
      "Saved out/weight: 112\n",
      "Saved out/bias: 512\n"
     ]
    }
   ],
   "source": [
    "for part in containers:\n",
    "    for wtype in [\"weight\", \"bias\"]:\n",
    "        c = containers[part][wtype]\n",
    "        data = np.concatenate(c[\"data\"])\n",
    "\n",
    "        json_path = os.path.join(\n",
    "            base_dir,\n",
    "            f\"G_d{DATA_MODE}_{TYPE}_{part}_{wtype}.json\"\n",
    "        )\n",
    "        mem_path = os.path.join(\n",
    "            base_dir,\n",
    "            f\"G_d{DATA_MODE}_{TYPE}_{part}_{wtype}.mem\"\n",
    "        )\n",
    "\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(c[\"offsets\"], f, indent=2)\n",
    "\n",
    "        write_mem(mem_path, data)\n",
    "\n",
    "        print(f\"Saved {part}/{wtype}: {data.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9788ac",
   "metadata": {},
   "source": [
    "# Mem Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "a10df2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_mem_fixed(path, dtype):\n",
    "    with open(path, \"r\") as f:\n",
    "        lines = [l.strip() for l in f if l.strip()]\n",
    "\n",
    "    hex_arr = np.array(lines)\n",
    "\n",
    "    if dtype == np.int16:\n",
    "        # np.uint16 captures the bits, .view(np.int16) handles the negative sign\n",
    "        val_arr = np.array([int(h, 16) for h in hex_arr], dtype=np.uint16).view(np.int16)\n",
    "    elif dtype == np.int32:\n",
    "        val_arr = np.array([int(h, 16) for h in hex_arr], dtype=np.uint32).view(np.int32)\n",
    "    else:\n",
    "        raise ValueError(\"dtype must be np.int16 or np.int32\")\n",
    "\n",
    "    return hex_arr, val_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "5d0ebc55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder weight:\n",
      "  hex shape : (217088,)\n",
      "  q   shape : (217088,)\n",
      "Decoder bias:\n",
      "  hex shape : (32768,)\n",
      "  q   shape : (32768,)\n"
     ]
    }
   ],
   "source": [
    "decoder_weight_mem = os.path.join(\n",
    "    base_dir,\n",
    "    f\"G_d{DATA_MODE}_{TYPE}_decoder_weight.mem\"\n",
    ")\n",
    "\n",
    "w_hex, w_q = read_mem_fixed(\n",
    "    decoder_weight_mem,\n",
    "    dtype=DTYPE   # np.int16 or np.int32\n",
    ")\n",
    "\n",
    "print(\"Decoder weight:\")\n",
    "print(\"  hex shape :\", w_hex.shape)\n",
    "print(\"  q   shape :\", w_q.shape)\n",
    "\n",
    "decoder_bias_mem = os.path.join(\n",
    "    base_dir,\n",
    "    f\"G_d{DATA_MODE}_{TYPE}_decoder_bias.mem\"\n",
    ")\n",
    "\n",
    "b_hex, b_q = read_mem_fixed(\n",
    "    decoder_bias_mem,\n",
    "    dtype=DTYPE\n",
    ")\n",
    "\n",
    "print(\"Decoder bias:\")\n",
    "print(\"  hex shape :\", b_hex.shape)\n",
    "print(\"  q   shape :\", b_q.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7611a195",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ab67f1",
   "metadata": {},
   "source": [
    "## Original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "2ddce75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder last output \n",
    "G.eval()\n",
    "decoder_last_input_batched = decoder_last_input.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_deconv_ref = G.d4.deconv(decoder_last_input_batched)\n",
    "    out_bn_ref     = G.d4.norm(out_deconv_ref)\n",
    "    out_d4_ref_original     = G.d4.act(out_bn_ref)\n",
    "\n",
    "out_2d_original = out_d4_ref_original.squeeze(0)\n",
    "out_f_original = out_2d_original.detach().cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "id": "90e259e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# float32 input\n",
    "inp_f = decoder_last_input.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "# quantize to Q9.14\n",
    "inp_q = float_to_q(\n",
    "    inp_f,\n",
    "    frac_bits=14,\n",
    "    int_bits=10,\n",
    "    dtype=np.int32\n",
    ")\n",
    "\n",
    "out_q_original = float_to_q(\n",
    "    out_f_original,\n",
    "    frac_bits=14,\n",
    "    int_bits=10,\n",
    "    dtype=np.int32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "fc623972",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10389 -20070    377 ...   3623   4751  -2806]\n",
      " [ -6728 -19803  28842 ...  -7025  -7435  14789]\n",
      " [ -6326   5797  13122 ...   9767 -27894  -5219]\n",
      " ...\n",
      " [  6566  -6092 -20761 ...  20464   8695 -20028]\n",
      " [-11184  10882  17755 ...   8395  -5962  12613]\n",
      " [-12396   -255 -14755 ... -28268    749   7425]]\n",
      "[[  542  4028     0 ...     0  3798 14952]\n",
      " [ 3118     0    77 ...     0  4133     0]\n",
      " [    0     0  3371 ...  7146     0   157]\n",
      " ...\n",
      " [    0     0 13750 ...  7376  4594     0]\n",
      " [ 9272  5095     0 ...     0     0     0]\n",
      " [ 1178     0  6759 ...     0  2655 13680]]\n"
     ]
    }
   ],
   "source": [
    "print(inp_q)\n",
    "print(out_q_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f91ba",
   "metadata": {},
   "source": [
    "## Fused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "e32c1d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder last output \n",
    "G.eval()\n",
    "decoder_last_input_batched = decoder_last_input.unsqueeze(0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out_deconv_ref = G_fused.d4.deconv(decoder_last_input_batched)\n",
    "    out_d4_ref_fused     = G_fused.d4.act(out_bn_ref)\n",
    "\n",
    "out_2d_fused = out_d4_ref_fused.squeeze(0)\n",
    "out_f_fused = out_2d_fused.detach().cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "ff66e30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# float32 input\n",
    "inp_f = decoder_last_input.detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "# quantize to Q9.14\n",
    "inp_q = float_to_q(\n",
    "    inp_f,\n",
    "    frac_bits=14,\n",
    "    int_bits=10,\n",
    "    dtype=np.int32\n",
    ")\n",
    "\n",
    "out_q_fused = float_to_q(\n",
    "    out_f_fused,\n",
    "    frac_bits=14,\n",
    "    int_bits=10,\n",
    "    dtype=np.int32\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "24d984df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10389 -20070    377 ...   3623   4751  -2806]\n",
      " [ -6728 -19803  28842 ...  -7025  -7435  14789]\n",
      " [ -6326   5797  13122 ...   9767 -27894  -5219]\n",
      " ...\n",
      " [  6566  -6092 -20761 ...  20464   8695 -20028]\n",
      " [-11184  10882  17755 ...   8395  -5962  12613]\n",
      " [-12396   -255 -14755 ... -28268    749   7425]]\n",
      "[[  542  4028     0 ...     0  3798 14952]\n",
      " [ 3118     0    77 ...     0  4133     0]\n",
      " [    0     0  3371 ...  7146     0   157]\n",
      " ...\n",
      " [    0     0 13750 ...  7376  4594     0]\n",
      " [ 9272  5095     0 ...     0     0     0]\n",
      " [ 1178     0  6759 ...     0  2655 13680]]\n"
     ]
    }
   ],
   "source": [
    "print(inp_q)\n",
    "print(out_q_fused)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e8a157",
   "metadata": {},
   "source": [
    "## Mem derived"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "41795fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d4 weight shape: (16, 4, 64)\n",
      "d4 bias shape  : (16,)\n"
     ]
    }
   ],
   "source": [
    "# from your JSON / export\n",
    "D4_W_OFFSET = 212992\n",
    "D4_W_SIZE   = 4096   # 64 * 1 * 64\n",
    "D4_BIAS_OFFSET = 24576\n",
    "TIME_REPEAT_D4 = 512\n",
    "Cout        = 16\n",
    "Cin         = 64\n",
    "K           = 4\n",
    "\n",
    "# weights (already deconv-flattened in export)\n",
    "d4_w_flat = w_q[-D4_W_SIZE:]\n",
    "d4_w_reshaped = d4_w_flat.reshape(Cout, K, Cin)\n",
    "\n",
    "# bias (expanded during export â†’ just take first Cout)\n",
    "d4_b_first = b_q[D4_BIAS_OFFSET:D4_BIAS_OFFSET+Cout-1]   # [cout]\n",
    "d4_b_sliced = np.array([\n",
    "    b_q[D4_BIAS_OFFSET + oc * TIME_REPEAT_D4]\n",
    "    for oc in range(Cout)\n",
    "])\n",
    "\n",
    "print(\"d4 weight shape:\", d4_w_reshaped.shape)\n",
    "print(\"d4 bias shape  :\", d4_b_sliced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "666a5a7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All values are the same\n",
      "Last index: False\n"
     ]
    }
   ],
   "source": [
    "base_val = b_q[D4_BIAS_OFFSET]\n",
    "diff = False\n",
    "\n",
    "for i in range(D4_BIAS_OFFSET,D4_BIAS_OFFSET+TIME_REPEAT_D4):\n",
    "    if b_q[i] != base_val:\n",
    "        print(f\"Not the same at i = {i}\")\n",
    "        diff = True\n",
    "        break\n",
    "\n",
    "if not diff:\n",
    "    print(\"All values are the same\")\n",
    "\n",
    "print(F\"Last index: {base_val == b_q[D4_BIAS_OFFSET+TIME_REPEAT_D4]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "a9a8d414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 256)\n",
      "(64, 256)\n"
     ]
    }
   ],
   "source": [
    "input_f = decoder_last_input.detach().cpu().numpy().astype(np.float32)\n",
    "print(input_f.shape)\n",
    "\n",
    "arr_q = float_to_q(\n",
    "        input_f,\n",
    "        frac_bits=FRAC_BITS,\n",
    "        int_bits=INT_BITS,\n",
    "        dtype=DTYPE      # np.int32\n",
    "    )\n",
    "print(arr_q.shape)\n",
    "\n",
    "X, Y = arr_q.shape\n",
    "\n",
    "hex_arr = []\n",
    "for y in range(Y): # x then y for decoder\n",
    "    for x in range(X):\n",
    "        v = int(arr_q[x, y]) & 0xFFFFFFFF\n",
    "        hex_arr.append(f\"{v:08X}\\n\")\n",
    "\n",
    "val_arr = np.array(\n",
    "            [(int(h, 16) & 0xFFFFFFFF) for h in hex_arr],\n",
    "            dtype=np.uint32\n",
    "        ).view(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "7d701b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Cin, Tin = decoder_last_input.shape\n",
    "Cout = 16\n",
    "K = 4\n",
    "stride = 2\n",
    "padding = 1\n",
    "\n",
    "# output length\n",
    "Tout = (Tin - 1) * stride - 2 * padding + K # (256 - 1) * 2 - 2 * 1 + 4 = 512\n",
    "\n",
    "# output buffer\n",
    "out_int = np.zeros((Cout, Tout), dtype=np.int32)\n",
    "\n",
    "# ---- ConvTranspose1d math ----\n",
    "\n",
    "for cout in range(Cout):\n",
    "    for t_out in range(Tout):\n",
    "        acc = int(d4_b_sliced[cout]) << FRAC_BITS\n",
    "\n",
    "        for k in range(K):\n",
    "            t_in_val = t_out + padding - k\n",
    "            if t_in_val % stride != 0:\n",
    "                continue\n",
    "            t_in = t_in_val // stride\n",
    "            if t_in < 0 or t_in >= Tin:\n",
    "                continue\n",
    "\n",
    "            for cin in range(Cin):\n",
    "                idx = (t_in * Cin) + cin\n",
    "\n",
    "                weight = d4_w_reshaped[cout, k, cin]\n",
    "                acc += weight * val_arr[idx]\n",
    "\n",
    "        rounding_factor = 1 << (FRAC_BITS - 1)\n",
    "        final_val = (acc + rounding_factor) >> FRAC_BITS\n",
    "        out_int[cout, t_out] = final_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "794d2754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   540   4028  -7946 ...  -3355   3800  14946]\n",
      " [  3114   -585     79 ...  -2533   4126   -160]\n",
      " [ -9749  -1488   3366 ...   7148 -11849    156]\n",
      " ...\n",
      " [ -1618 -12669  13750 ...   7378   4598  -7433]\n",
      " [  9268   5089 -12069 ...  -6294  -8993  -9798]\n",
      " [  1183  -9179   6757 ...  -2869   2648  13679]]\n"
     ]
    }
   ],
   "source": [
    "print(out_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2db4d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
