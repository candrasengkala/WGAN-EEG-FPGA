{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566e83bb",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "466b6077",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import json\n",
    "import re\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import random\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f995e8f",
   "metadata": {},
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "7f29c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Conv1d -> BatchNorm1d -> Activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=15, s=2, p=7, bias=True, act=\"lrelu\"):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=bias)\n",
    "        self.norm = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        if act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act must be 'lrelu' or 'relu'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.conv(x)))\n",
    "\n",
    "\n",
    "class DeconvBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    ConvTranspose1d -> BatchNorm1d -> Activation\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch, out_ch, k=4, s=2, p=1, bias=True, act=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(in_ch, out_ch, kernel_size=k, stride=s, padding=p, bias=bias)\n",
    "        self.norm = nn.BatchNorm1d(out_ch)\n",
    "\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        else:\n",
    "            raise ValueError(\"act must be 'relu' or 'lrelu'\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.norm(self.deconv(x)))\n",
    "\n",
    "\n",
    "class ResBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block: (Conv -> BN -> ReLU) x2 + skip\n",
    "    Keeps same channel count and length.\n",
    "    \"\"\"\n",
    "    def __init__(self, ch, k=7, p=3, bias=True):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv1d(ch, ch, kernel_size=k, stride=1, padding=p, bias=bias)\n",
    "        self.n1 = nn.BatchNorm1d(ch)\n",
    "        self.c2 = nn.Conv1d(ch, ch, kernel_size=k, stride=1, padding=p, bias=bias)\n",
    "        self.n2 = nn.BatchNorm1d(ch)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.n1(self.c1(x)))\n",
    "        h = self.n2(self.c2(h))\n",
    "        return F.relu(x + h)\n",
    "    \n",
    "class MultiScaleResBlock1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-scale residual block: parallel conv branches (k=3,5,7) then fuse.\n",
    "    Keeps same channel count and length.\n",
    "    \"\"\"\n",
    "    def __init__(self, ch, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.b3 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=3, padding=1, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.b5 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=5, padding=2, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.b7 = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=7, padding=3, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "        self.fuse = nn.Sequential(\n",
    "            nn.Conv1d(ch, ch, kernel_size=1, bias=bias),\n",
    "            nn.BatchNorm1d(ch),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.b3(x) + self.b5(x) + self.b7(x)\n",
    "        h = self.fuse(h)\n",
    "        return F.relu(x + h)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "56b22df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NN Generator (U-Net-ish + Res bottleneck)\n",
    "class GeneratorCNNWGAN(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN U-Net-ish generator for EEG denoising (WGAN).\n",
    "    Input : (B, 1, 512) noisy_norm\n",
    "    Output: (B, 1, 512) clean_norm_hat\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=32, bottleneck_blocks=4, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.e1 = ConvBlock1D(1, base_ch,       k=16, s=2, p=7, bias=bias, act=\"lrelu\")      # 512 -> 256\n",
    "        self.e2 = ConvBlock1D(base_ch, base_ch*2, k=16, s=2, p=7, bias=bias, act=\"lrelu\")    # 256 -> 128\n",
    "        self.e3 = ConvBlock1D(base_ch*2, base_ch*4, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 128 -> 64\n",
    "        self.e4 = ConvBlock1D(base_ch*4, base_ch*8, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 64 -> 32\n",
    "\n",
    "        # Bottleneck\n",
    "        bn_ch = base_ch * 8\n",
    "        self.bottleneck = nn.Sequential(*[\n",
    "            ResBlock1D(bn_ch, k=7, p=3, bias=bias) for _ in range(bottleneck_blocks)\n",
    "        ])\n",
    "\n",
    "        # Decoder (concat doubles channels)\n",
    "        self.d1 = DeconvBlock1D(bn_ch, base_ch*4,   k=4, s=2, p=1, bias=bias, act=\"relu\")     # 32 -> 64\n",
    "        self.d2 = DeconvBlock1D(base_ch*8, base_ch*2, k=4, s=2, p=1,bias=bias, act=\"relu\")   # 64 -> 128\n",
    "        self.d3 = DeconvBlock1D(base_ch*4, base_ch,   k=4, s=2, p=1, bias=bias, act=\"relu\")   # 128 -> 256\n",
    "        self.d4 = DeconvBlock1D(base_ch*2, base_ch//2, k=4, s=2, p=1, bias=bias, act=\"relu\")  # 256 -> 512\n",
    "\n",
    "        # Head (linear output recommended for normalized signals)\n",
    "        self.out = nn.Conv1d(base_ch//2, 1, kernel_size=7, stride=1, padding=3, bias=bias)\n",
    "\n",
    "    def forward(self, y):\n",
    "        # Encoder\n",
    "        s1 = self.e1(y)   # (B, base, 256)\n",
    "        s2 = self.e2(s1)  # (B, 2b, 128)\n",
    "        s3 = self.e3(s2)  # (B, 4b, 64)\n",
    "        s4 = self.e4(s3)  # (B, 8b, 32)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bottleneck(s4)\n",
    "\n",
    "        # Decoder + skip connections\n",
    "        d1 = self.d1(b)                  # (B, 4b, 64)\n",
    "        d1 = torch.cat([d1, s3], dim=1)  # (B, 8b, 64)\n",
    "\n",
    "        d2 = self.d2(d1)                 # (B, 2b, 128)\n",
    "        d2 = torch.cat([d2, s2], dim=1)  # (B, 4b, 128)\n",
    "\n",
    "        d3 = self.d3(d2)                 # (B, b, 256)\n",
    "        d3 = torch.cat([d3, s1], dim=1)  # (B, 2b, 256)\n",
    "\n",
    "        d4 = self.d4(d3)                 # (B, b/2, 512)\n",
    "\n",
    "        return self.out(d4)              # (B, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "0ef71ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch Critic (shared by CNN/ResCNN)\n",
    "class CriticPatch1D(nn.Module):\n",
    "    \"\"\"\n",
    "    Conditional PatchGAN critic for WGAN:\n",
    "      D(y, x) -> patch scores\n",
    "    y,x: (B,1,512)\n",
    "    output: (B,1,32)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ch=32, bias=True):\n",
    "        super().__init__()\n",
    "        self.c1 = nn.Conv1d(2, base_ch, kernel_size=16, stride=2, padding=7, bias=bias)  # 512 -> 256\n",
    "        self.c2 = ConvBlock1D(base_ch, base_ch*2, k=16, s=2, p=7, bias=bias, act=\"lrelu\")    # 256 -> 128\n",
    "        self.c3 = ConvBlock1D(base_ch*2, base_ch*4, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 128 -> 64\n",
    "        self.c4 = ConvBlock1D(base_ch*4, base_ch*8, k=16, s=2, p=7, bias=bias, act=\"lrelu\")  # 64 -> 32\n",
    "        self.out = nn.Conv1d(base_ch*8, 1, kernel_size=7, stride=1, padding=3, bias=bias)   # 32 -> 32\n",
    "\n",
    "    def forward(self, y, x):\n",
    "        h = torch.cat([y, x], dim=1)  # (B,2,512)\n",
    "        h = F.leaky_relu(self.c1(h), 0.2, inplace=True)\n",
    "        h = self.c2(h)\n",
    "        h = self.c3(h)\n",
    "        h = self.c4(h)\n",
    "        return self.out(h)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ea6ed",
   "metadata": {},
   "source": [
    "# Model Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "45b10b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "BIAS = True\n",
    "DATA_MODE = 4 # up to 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "3b2bbd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d4_b\n",
      "G: c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d4_b\\cnn_G_20260113_213937.pth\n",
      "D: c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d4_b\\cnn_D_20260113_213937.pth\n"
     ]
    }
   ],
   "source": [
    "data_path = os.path.abspath(f\"../models/main3_d{DATA_MODE}_{\"b\" if BIAS else \"nb\"}/\")\n",
    "print(data_path)\n",
    "\n",
    "pattern = re.compile(r\"^cnn_([DG])_\\d{8}_\\d{6}\\.pth$\")\n",
    "\n",
    "cnn_G_path = None\n",
    "cnn_D_path = None\n",
    "\n",
    "for f in os.listdir(data_path):\n",
    "    m = pattern.match(f)\n",
    "    if m:\n",
    "        full = os.path.join(data_path, f)\n",
    "        if m.group(1) == \"G\":\n",
    "            cnn_G_path = full\n",
    "        else:\n",
    "            cnn_D_path = full\n",
    "\n",
    "print(\"G:\", cnn_G_path)\n",
    "print(\"D:\", cnn_D_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "2e795bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeneratorCNNWGAN(\n",
      "  (e1): ConvBlock1D(\n",
      "    (conv): Conv1d(1, 32, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e2): ConvBlock1D(\n",
      "    (conv): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e3): ConvBlock1D(\n",
      "    (conv): Conv1d(64, 128, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (e4): ConvBlock1D(\n",
      "    (conv): Conv1d(128, 256, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (bottleneck): Sequential(\n",
      "    (0): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (3): ResBlock1D(\n",
      "      (c1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (c2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      "      (n2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (d1): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(256, 128, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d2): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(256, 64, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d3): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(128, 32, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (d4): DeconvBlock1D(\n",
      "    (deconv): ConvTranspose1d(64, 16, kernel_size=(4,), stride=(2,), padding=(1,))\n",
      "    (norm): BatchNorm1d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): ReLU(inplace=True)\n",
      "  )\n",
      "  (out): Conv1d(16, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      ")\n",
      "CriticPatch1D(\n",
      "  (c1): Conv1d(2, 32, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "  (c2): ConvBlock1D(\n",
      "    (conv): Conv1d(32, 64, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (c3): ConvBlock1D(\n",
      "    (conv): Conv1d(64, 128, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (c4): ConvBlock1D(\n",
      "    (conv): Conv1d(128, 256, kernel_size=(16,), stride=(2,), padding=(7,))\n",
      "    (norm): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (act): LeakyReLU(negative_slope=0.2, inplace=True)\n",
      "  )\n",
      "  (out): Conv1d(256, 1, kernel_size=(7,), stride=(1,), padding=(3,))\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aryo\\AppData\\Local\\Temp\\ipykernel_23412\\700924479.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  G.load_state_dict(torch.load(cnn_G_path, map_location=device))\n",
      "C:\\Users\\Aryo\\AppData\\Local\\Temp\\ipykernel_23412\\700924479.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  D.load_state_dict(torch.load(cnn_D_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "G = GeneratorCNNWGAN(bias=BIAS).to(device)\n",
    "D = CriticPatch1D(bias=BIAS).to(device)\n",
    "\n",
    "G.load_state_dict(torch.load(cnn_G_path, map_location=device))\n",
    "D.load_state_dict(torch.load(cnn_D_path, map_location=device))\n",
    "\n",
    "print(G.eval())\n",
    "print(D.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "a97fd7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> GeneratorCNNWGAN\n",
      "e1 -> ConvBlock1D\n",
      "e1.conv -> Conv1d\n",
      "e1.norm -> BatchNorm1d\n",
      "e1.act -> LeakyReLU\n",
      "e2 -> ConvBlock1D\n",
      "e2.conv -> Conv1d\n",
      "e2.norm -> BatchNorm1d\n",
      "e2.act -> LeakyReLU\n",
      "e3 -> ConvBlock1D\n",
      "e3.conv -> Conv1d\n",
      "e3.norm -> BatchNorm1d\n",
      "e3.act -> LeakyReLU\n",
      "e4 -> ConvBlock1D\n",
      "e4.conv -> Conv1d\n",
      "e4.norm -> BatchNorm1d\n",
      "e4.act -> LeakyReLU\n",
      "bottleneck -> Sequential\n",
      "bottleneck.0 -> ResBlock1D\n",
      "bottleneck.0.c1 -> Conv1d\n",
      "bottleneck.0.n1 -> BatchNorm1d\n",
      "bottleneck.0.c2 -> Conv1d\n",
      "bottleneck.0.n2 -> BatchNorm1d\n",
      "bottleneck.1 -> ResBlock1D\n",
      "bottleneck.1.c1 -> Conv1d\n",
      "bottleneck.1.n1 -> BatchNorm1d\n",
      "bottleneck.1.c2 -> Conv1d\n",
      "bottleneck.1.n2 -> BatchNorm1d\n",
      "bottleneck.2 -> ResBlock1D\n",
      "bottleneck.2.c1 -> Conv1d\n",
      "bottleneck.2.n1 -> BatchNorm1d\n",
      "bottleneck.2.c2 -> Conv1d\n",
      "bottleneck.2.n2 -> BatchNorm1d\n",
      "bottleneck.3 -> ResBlock1D\n",
      "bottleneck.3.c1 -> Conv1d\n",
      "bottleneck.3.n1 -> BatchNorm1d\n",
      "bottleneck.3.c2 -> Conv1d\n",
      "bottleneck.3.n2 -> BatchNorm1d\n",
      "d1 -> DeconvBlock1D\n",
      "d1.deconv -> ConvTranspose1d\n",
      "d1.norm -> BatchNorm1d\n",
      "d1.act -> ReLU\n",
      "d2 -> DeconvBlock1D\n",
      "d2.deconv -> ConvTranspose1d\n",
      "d2.norm -> BatchNorm1d\n",
      "d2.act -> ReLU\n",
      "d3 -> DeconvBlock1D\n",
      "d3.deconv -> ConvTranspose1d\n",
      "d3.norm -> BatchNorm1d\n",
      "d3.act -> ReLU\n",
      "d4 -> DeconvBlock1D\n",
      "d4.deconv -> ConvTranspose1d\n",
      "d4.norm -> BatchNorm1d\n",
      "d4.act -> ReLU\n",
      "out -> Conv1d\n",
      "\n",
      "================================================================================\n",
      "\n",
      " -> CriticPatch1D\n",
      "c1 -> Conv1d\n",
      "c2 -> ConvBlock1D\n",
      "c2.conv -> Conv1d\n",
      "c2.norm -> BatchNorm1d\n",
      "c2.act -> LeakyReLU\n",
      "c3 -> ConvBlock1D\n",
      "c3.conv -> Conv1d\n",
      "c3.norm -> BatchNorm1d\n",
      "c3.act -> LeakyReLU\n",
      "c4 -> ConvBlock1D\n",
      "c4.conv -> Conv1d\n",
      "c4.norm -> BatchNorm1d\n",
      "c4.act -> LeakyReLU\n",
      "out -> Conv1d\n"
     ]
    }
   ],
   "source": [
    "for name, module in G.named_modules():\n",
    "    print(name, \"->\", module.__class__.__name__)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 +\"\\n\")\n",
    "\n",
    "for name, module in D.named_modules():\n",
    "    print(name, \"->\", module.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "4663f064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1.conv.weight torch.Size([32, 1, 16]) True\n",
      "e1.conv.bias torch.Size([32]) True\n",
      "e1.norm.weight torch.Size([32]) True\n",
      "e1.norm.bias torch.Size([32]) True\n",
      "e2.conv.weight torch.Size([64, 32, 16]) True\n",
      "e2.conv.bias torch.Size([64]) True\n",
      "e2.norm.weight torch.Size([64]) True\n",
      "e2.norm.bias torch.Size([64]) True\n",
      "e3.conv.weight torch.Size([128, 64, 16]) True\n",
      "e3.conv.bias torch.Size([128]) True\n",
      "e3.norm.weight torch.Size([128]) True\n",
      "e3.norm.bias torch.Size([128]) True\n",
      "e4.conv.weight torch.Size([256, 128, 16]) True\n",
      "e4.conv.bias torch.Size([256]) True\n",
      "e4.norm.weight torch.Size([256]) True\n",
      "e4.norm.bias torch.Size([256]) True\n",
      "bottleneck.0.c1.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.0.c1.bias torch.Size([256]) True\n",
      "bottleneck.0.n1.weight torch.Size([256]) True\n",
      "bottleneck.0.n1.bias torch.Size([256]) True\n",
      "bottleneck.0.c2.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.0.c2.bias torch.Size([256]) True\n",
      "bottleneck.0.n2.weight torch.Size([256]) True\n",
      "bottleneck.0.n2.bias torch.Size([256]) True\n",
      "bottleneck.1.c1.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.1.c1.bias torch.Size([256]) True\n",
      "bottleneck.1.n1.weight torch.Size([256]) True\n",
      "bottleneck.1.n1.bias torch.Size([256]) True\n",
      "bottleneck.1.c2.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.1.c2.bias torch.Size([256]) True\n",
      "bottleneck.1.n2.weight torch.Size([256]) True\n",
      "bottleneck.1.n2.bias torch.Size([256]) True\n",
      "bottleneck.2.c1.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.2.c1.bias torch.Size([256]) True\n",
      "bottleneck.2.n1.weight torch.Size([256]) True\n",
      "bottleneck.2.n1.bias torch.Size([256]) True\n",
      "bottleneck.2.c2.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.2.c2.bias torch.Size([256]) True\n",
      "bottleneck.2.n2.weight torch.Size([256]) True\n",
      "bottleneck.2.n2.bias torch.Size([256]) True\n",
      "bottleneck.3.c1.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.3.c1.bias torch.Size([256]) True\n",
      "bottleneck.3.n1.weight torch.Size([256]) True\n",
      "bottleneck.3.n1.bias torch.Size([256]) True\n",
      "bottleneck.3.c2.weight torch.Size([256, 256, 7]) True\n",
      "bottleneck.3.c2.bias torch.Size([256]) True\n",
      "bottleneck.3.n2.weight torch.Size([256]) True\n",
      "bottleneck.3.n2.bias torch.Size([256]) True\n",
      "d1.deconv.weight torch.Size([256, 128, 4]) True\n",
      "d1.deconv.bias torch.Size([128]) True\n",
      "d1.norm.weight torch.Size([128]) True\n",
      "d1.norm.bias torch.Size([128]) True\n",
      "d2.deconv.weight torch.Size([256, 64, 4]) True\n",
      "d2.deconv.bias torch.Size([64]) True\n",
      "d2.norm.weight torch.Size([64]) True\n",
      "d2.norm.bias torch.Size([64]) True\n",
      "d3.deconv.weight torch.Size([128, 32, 4]) True\n",
      "d3.deconv.bias torch.Size([32]) True\n",
      "d3.norm.weight torch.Size([32]) True\n",
      "d3.norm.bias torch.Size([32]) True\n",
      "d4.deconv.weight torch.Size([64, 16, 4]) True\n",
      "d4.deconv.bias torch.Size([16]) True\n",
      "d4.norm.weight torch.Size([16]) True\n",
      "d4.norm.bias torch.Size([16]) True\n",
      "out.weight torch.Size([1, 16, 7]) True\n",
      "out.bias torch.Size([1]) True\n",
      "\n",
      "================================================================================\n",
      "\n",
      "c1.weight torch.Size([32, 2, 16]) True\n",
      "c1.bias torch.Size([32]) True\n",
      "c2.conv.weight torch.Size([64, 32, 16]) True\n",
      "c2.conv.bias torch.Size([64]) True\n",
      "c2.norm.weight torch.Size([64]) True\n",
      "c2.norm.bias torch.Size([64]) True\n",
      "c3.conv.weight torch.Size([128, 64, 16]) True\n",
      "c3.conv.bias torch.Size([128]) True\n",
      "c3.norm.weight torch.Size([128]) True\n",
      "c3.norm.bias torch.Size([128]) True\n",
      "c4.conv.weight torch.Size([256, 128, 16]) True\n",
      "c4.conv.bias torch.Size([256]) True\n",
      "c4.norm.weight torch.Size([256]) True\n",
      "c4.norm.bias torch.Size([256]) True\n",
      "out.weight torch.Size([1, 256, 7]) True\n",
      "out.bias torch.Size([1]) True\n"
     ]
    }
   ],
   "source": [
    "for name, param in G.named_parameters():\n",
    "    print(name, param.shape, param.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 +\"\\n\")\n",
    "\n",
    "for name, param in D.named_parameters():\n",
    "    print(name, param.shape, param.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "921e8840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e1.norm.running_mean torch.Size([32])\n",
      "e1.norm.running_var torch.Size([32])\n",
      "e1.norm.num_batches_tracked torch.Size([])\n",
      "e2.norm.running_mean torch.Size([64])\n",
      "e2.norm.running_var torch.Size([64])\n",
      "e2.norm.num_batches_tracked torch.Size([])\n",
      "e3.norm.running_mean torch.Size([128])\n",
      "e3.norm.running_var torch.Size([128])\n",
      "e3.norm.num_batches_tracked torch.Size([])\n",
      "e4.norm.running_mean torch.Size([256])\n",
      "e4.norm.running_var torch.Size([256])\n",
      "e4.norm.num_batches_tracked torch.Size([])\n",
      "bottleneck.0.n1.running_mean torch.Size([256])\n",
      "bottleneck.0.n1.running_var torch.Size([256])\n",
      "bottleneck.0.n1.num_batches_tracked torch.Size([])\n",
      "bottleneck.0.n2.running_mean torch.Size([256])\n",
      "bottleneck.0.n2.running_var torch.Size([256])\n",
      "bottleneck.0.n2.num_batches_tracked torch.Size([])\n",
      "bottleneck.1.n1.running_mean torch.Size([256])\n",
      "bottleneck.1.n1.running_var torch.Size([256])\n",
      "bottleneck.1.n1.num_batches_tracked torch.Size([])\n",
      "bottleneck.1.n2.running_mean torch.Size([256])\n",
      "bottleneck.1.n2.running_var torch.Size([256])\n",
      "bottleneck.1.n2.num_batches_tracked torch.Size([])\n",
      "bottleneck.2.n1.running_mean torch.Size([256])\n",
      "bottleneck.2.n1.running_var torch.Size([256])\n",
      "bottleneck.2.n1.num_batches_tracked torch.Size([])\n",
      "bottleneck.2.n2.running_mean torch.Size([256])\n",
      "bottleneck.2.n2.running_var torch.Size([256])\n",
      "bottleneck.2.n2.num_batches_tracked torch.Size([])\n",
      "bottleneck.3.n1.running_mean torch.Size([256])\n",
      "bottleneck.3.n1.running_var torch.Size([256])\n",
      "bottleneck.3.n1.num_batches_tracked torch.Size([])\n",
      "bottleneck.3.n2.running_mean torch.Size([256])\n",
      "bottleneck.3.n2.running_var torch.Size([256])\n",
      "bottleneck.3.n2.num_batches_tracked torch.Size([])\n",
      "d1.norm.running_mean torch.Size([128])\n",
      "d1.norm.running_var torch.Size([128])\n",
      "d1.norm.num_batches_tracked torch.Size([])\n",
      "d2.norm.running_mean torch.Size([64])\n",
      "d2.norm.running_var torch.Size([64])\n",
      "d2.norm.num_batches_tracked torch.Size([])\n",
      "d3.norm.running_mean torch.Size([32])\n",
      "d3.norm.running_var torch.Size([32])\n",
      "d3.norm.num_batches_tracked torch.Size([])\n",
      "d4.norm.running_mean torch.Size([16])\n",
      "d4.norm.running_var torch.Size([16])\n",
      "d4.norm.num_batches_tracked torch.Size([])\n",
      "\n",
      "================================================================================\n",
      "\n",
      "c2.norm.running_mean torch.Size([64])\n",
      "c2.norm.running_var torch.Size([64])\n",
      "c2.norm.num_batches_tracked torch.Size([])\n",
      "c3.norm.running_mean torch.Size([128])\n",
      "c3.norm.running_var torch.Size([128])\n",
      "c3.norm.num_batches_tracked torch.Size([])\n",
      "c4.norm.running_mean torch.Size([256])\n",
      "c4.norm.running_var torch.Size([256])\n",
      "c4.norm.num_batches_tracked torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "for name, buf in G.named_buffers():\n",
    "    print(name, buf.shape)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80 +\"\\n\")\n",
    "\n",
    "for name, buf in D.named_buffers():\n",
    "    print(name, buf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "2b761e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BN layer: e1.norm\n",
      " running_mean: torch.Size([32])\n",
      " running_var : torch.Size([32])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: e2.norm\n",
      " running_mean: torch.Size([64])\n",
      " running_var : torch.Size([64])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: e3.norm\n",
      " running_mean: torch.Size([128])\n",
      " running_var : torch.Size([128])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: e4.norm\n",
      " running_mean: torch.Size([256])\n",
      " running_var : torch.Size([256])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: bottleneck.0.n1\n",
      " running_mean: torch.Size([256])\n",
      " running_var : torch.Size([256])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: bottleneck.0.n2\n",
      " running_mean: torch.Size([256])\n",
      " running_var : torch.Size([256])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: bottleneck.1.n1\n",
      " running_mean: torch.Size([256])\n",
      " running_var : torch.Size([256])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: bottleneck.1.n2\n",
      " running_mean: torch.Size([256])\n",
      " running_var : torch.Size([256])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: bottleneck.2.n1\n",
      " running_mean: torch.Size([256])\n",
      " running_var : torch.Size([256])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: bottleneck.2.n2\n",
      " running_mean: torch.Size([256])\n",
      " running_var : torch.Size([256])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: bottleneck.3.n1\n",
      " running_mean: torch.Size([256])\n",
      " running_var : torch.Size([256])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: bottleneck.3.n2\n",
      " running_mean: torch.Size([256])\n",
      " running_var : torch.Size([256])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: d1.norm\n",
      " running_mean: torch.Size([128])\n",
      " running_var : torch.Size([128])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: d2.norm\n",
      " running_mean: torch.Size([64])\n",
      " running_var : torch.Size([64])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: d3.norm\n",
      " running_mean: torch.Size([32])\n",
      " running_var : torch.Size([32])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n",
      "\n",
      "BN layer: d4.norm\n",
      " running_mean: torch.Size([16])\n",
      " running_var : torch.Size([16])\n",
      " momentum    : 0.1\n",
      " eps         : 1e-05\n",
      " affine      : True\n"
     ]
    }
   ],
   "source": [
    "for name, module in G.named_modules():\n",
    "    if isinstance(module, torch.nn.BatchNorm1d):\n",
    "        print(f\"\\nBN layer: {name}\")\n",
    "        print(\" running_mean:\", module.running_mean.shape)\n",
    "        print(\" running_var :\", module.running_var.shape)\n",
    "        print(\" momentum    :\", module.momentum)\n",
    "        print(\" eps         :\", module.eps)\n",
    "        print(\" affine      :\", module.affine)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262ddfca",
   "metadata": {},
   "source": [
    "# Convolution and BatchNorm Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "5ee3a1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_conv_bn_1d(\n",
    "    conv_weight,        # (Cout, Cin, K)\n",
    "    conv_bias,          # (Cout,) or None\n",
    "    running_mean,       # (Cout,)\n",
    "    running_var,        # (Cout,)\n",
    "    bn_weight,          # (Cout,) or None (gamma)\n",
    "    bn_bias,            # (Cout,) or None (beta)\n",
    "    eps=1e-5\n",
    "):\n",
    "    Cout = conv_weight.shape[0]\n",
    "\n",
    "    if bn_weight is None:\n",
    "        bn_weight = torch.ones(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "    if bn_bias is None:\n",
    "        bn_bias = torch.zeros(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "    if conv_bias is None:\n",
    "        conv_bias = torch.zeros(Cout, device=conv_weight.device, dtype=conv_weight.dtype)\n",
    "\n",
    "    denom = torch.sqrt(running_var + eps)          # (Cout,)\n",
    "    scale = bn_weight / denom                      # (Cout,)\n",
    "\n",
    "    # Fuse weight\n",
    "    fused_weight = conv_weight * scale[:, None, None]\n",
    "\n",
    "    # Fuse bias\n",
    "    fused_bias = (conv_bias - running_mean) * scale + bn_bias\n",
    "\n",
    "    return fused_weight, fused_bias\n",
    "\n",
    "def fuse_deconv_bn_1d(\n",
    "    deconv_weight,     # (Cin, Cout, K)\n",
    "    deconv_bias,       # (Cout,) or None\n",
    "    running_mean,      # (Cout,)\n",
    "    running_var,       # (Cout,)\n",
    "    bn_weight,         # (Cout,)\n",
    "    bn_bias,           # (Cout,)\n",
    "    eps\n",
    "):\n",
    "    Cin, Cout, K = deconv_weight.shape\n",
    "\n",
    "    if deconv_bias is None:\n",
    "        deconv_bias = torch.zeros(\n",
    "            Cout,\n",
    "            device=deconv_weight.device,\n",
    "            dtype=deconv_weight.dtype\n",
    "        )\n",
    "\n",
    "    # BN scale\n",
    "    scale = bn_weight / torch.sqrt(running_var + eps)  # (Cout,)\n",
    "\n",
    "    # Fuse weights (scale on Cout dimension)\n",
    "    fused_weight = deconv_weight * scale.view(1, Cout, 1)\n",
    "\n",
    "    # Fuse bias\n",
    "    fused_bias = (deconv_bias - running_mean) * scale + bn_bias\n",
    "\n",
    "    return fused_weight, fused_bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "8751c44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FusedConvBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s, p, bias=True, act=\"lrelu\"):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_ch, out_ch, k, s, p, bias=bias)\n",
    "\n",
    "        if act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        elif act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.conv(x))\n",
    "\n",
    "\n",
    "class FusedDeconvBlock1D(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, k, s, p, bias=True, act=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.deconv = nn.ConvTranspose1d(in_ch, out_ch, k, s, p, bias=bias)\n",
    "\n",
    "        if act == \"relu\":\n",
    "            self.act = nn.ReLU(inplace=True)\n",
    "        elif act == \"lrelu\":\n",
    "            self.act = nn.LeakyReLU(0.2, inplace=True)\n",
    "        else:\n",
    "            raise ValueError\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.act(self.deconv(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "8c5afa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeneratorCNNWGAN_Fused(nn.Module):\n",
    "    def __init__(self, base_ch=32, bottleneck_blocks=4, bias=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.e1 = FusedConvBlock1D(1, base_ch, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e2 = FusedConvBlock1D(base_ch, base_ch*2, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e3 = FusedConvBlock1D(base_ch*2, base_ch*4, 16, 2, 7, bias, \"lrelu\")\n",
    "        self.e4 = FusedConvBlock1D(base_ch*4, base_ch*8, 16, 2, 7, bias, \"lrelu\")\n",
    "\n",
    "        self.bottleneck = nn.Sequential(*[\n",
    "            nn.Sequential(\n",
    "                nn.Conv1d(base_ch*8, base_ch*8, 7, 1, 3, bias=bias),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv1d(base_ch*8, base_ch*8, 7, 1, 3, bias=bias),\n",
    "            )\n",
    "            for _ in range(bottleneck_blocks)\n",
    "        ])\n",
    "\n",
    "        self.d1 = FusedDeconvBlock1D(base_ch*8, base_ch*4, 4, 2, 1, bias)\n",
    "        self.d2 = FusedDeconvBlock1D(base_ch*8, base_ch*2, 4, 2, 1, bias)\n",
    "        self.d3 = FusedDeconvBlock1D(base_ch*4, base_ch, 4, 2, 1, bias)\n",
    "        self.d4 = FusedDeconvBlock1D(base_ch*2, base_ch//2, 4, 2, 1, bias)\n",
    "\n",
    "        self.out = nn.Conv1d(base_ch//2, 1, 7, 1, 3, bias=bias)\n",
    "\n",
    "    def forward(self, y):\n",
    "        s1 = self.e1(y)\n",
    "        s2 = self.e2(s1)\n",
    "        s3 = self.e3(s2)\n",
    "        s4 = self.e4(s3)\n",
    "\n",
    "        b = s4\n",
    "        for blk in self.bottleneck:\n",
    "            b = F.relu(b + blk(b))\n",
    "\n",
    "        d1 = torch.cat([self.d1(b), s3], dim=1)\n",
    "        d2 = torch.cat([self.d2(d1), s2], dim=1)\n",
    "        d3 = torch.cat([self.d3(d2), s1], dim=1)\n",
    "        d4 = self.d4(d3)\n",
    "\n",
    "        return self.out(d4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "7523ee40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_generator(G):\n",
    "    G_fused = GeneratorCNNWGAN_Fused(bias=True).to(device)\n",
    "    G_fused.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Encoder\n",
    "        for i in range(1, 5):\n",
    "            e = getattr(G, f\"e{i}\")\n",
    "            fe = getattr(G_fused, f\"e{i}\")\n",
    "\n",
    "            w, b = fuse_conv_bn_1d(\n",
    "                e.conv.weight, e.conv.bias,\n",
    "                e.norm.running_mean, e.norm.running_var,\n",
    "                e.norm.weight, e.norm.bias\n",
    "            )\n",
    "            fe.conv.weight.copy_(w)\n",
    "            fe.conv.bias.copy_(b)\n",
    "\n",
    "        # Bottleneck\n",
    "        for i, blk in enumerate(G.bottleneck):\n",
    "            fblk = G_fused.bottleneck[i]\n",
    "\n",
    "            for j, (c, n) in enumerate([(blk.c1, blk.n1), (blk.c2, blk.n2)]):\n",
    "                w, b = fuse_conv_bn_1d(\n",
    "                    c.weight, c.bias,\n",
    "                    n.running_mean, n.running_var,\n",
    "                    n.weight, n.bias\n",
    "                )\n",
    "                fblk[j*2].weight.copy_(w)\n",
    "                fblk[j*2].bias.copy_(b)\n",
    "\n",
    "        # Decoder\n",
    "        for i in range(1, 5):\n",
    "            d = getattr(G, f\"d{i}\")\n",
    "            fd = getattr(G_fused, f\"d{i}\")\n",
    "\n",
    "            w, b = fuse_deconv_bn_1d(\n",
    "                d.deconv.weight, d.deconv.bias,\n",
    "                d.norm.running_mean, d.norm.running_var,\n",
    "                d.norm.weight, d.norm.bias,\n",
    "                d.norm.eps\n",
    "            )\n",
    "            fd.deconv.weight.copy_(w)\n",
    "            fd.deconv.bias.copy_(b)\n",
    "\n",
    "        # Output\n",
    "        G_fused.out.weight.copy_(G.out.weight)\n",
    "        G_fused.out.bias.copy_(G.out.bias)\n",
    "\n",
    "    return G_fused\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "356e4104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: c:\\Users\\Aryo\\PersonalMade\\Programming\\GAN\\repo\\src\\models\\main3_d4_b\\fused_cnn_G_20260113_213937.pth\n"
     ]
    }
   ],
   "source": [
    "G_fused = fuse_generator(G)\n",
    "\n",
    "fused_G_path = os.path.join(\n",
    "    data_path,\n",
    "    \"fused_\" + os.path.basename(cnn_G_path)\n",
    ")\n",
    "\n",
    "torch.save(G_fused.state_dict(), fused_G_path)\n",
    "print(\"Saved:\", fused_G_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "b9cd7baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " -> GeneratorCNNWGAN_Fused\n",
      "e1 -> FusedConvBlock1D\n",
      "e1.conv -> Conv1d\n",
      "e1.act -> LeakyReLU\n",
      "e2 -> FusedConvBlock1D\n",
      "e2.conv -> Conv1d\n",
      "e2.act -> LeakyReLU\n",
      "e3 -> FusedConvBlock1D\n",
      "e3.conv -> Conv1d\n",
      "e3.act -> LeakyReLU\n",
      "e4 -> FusedConvBlock1D\n",
      "e4.conv -> Conv1d\n",
      "e4.act -> LeakyReLU\n",
      "bottleneck -> Sequential\n",
      "bottleneck.0 -> Sequential\n",
      "bottleneck.0.0 -> Conv1d\n",
      "bottleneck.0.1 -> ReLU\n",
      "bottleneck.0.2 -> Conv1d\n",
      "bottleneck.1 -> Sequential\n",
      "bottleneck.1.0 -> Conv1d\n",
      "bottleneck.1.1 -> ReLU\n",
      "bottleneck.1.2 -> Conv1d\n",
      "bottleneck.2 -> Sequential\n",
      "bottleneck.2.0 -> Conv1d\n",
      "bottleneck.2.1 -> ReLU\n",
      "bottleneck.2.2 -> Conv1d\n",
      "bottleneck.3 -> Sequential\n",
      "bottleneck.3.0 -> Conv1d\n",
      "bottleneck.3.1 -> ReLU\n",
      "bottleneck.3.2 -> Conv1d\n",
      "d1 -> FusedDeconvBlock1D\n",
      "d1.deconv -> ConvTranspose1d\n",
      "d1.act -> ReLU\n",
      "d2 -> FusedDeconvBlock1D\n",
      "d2.deconv -> ConvTranspose1d\n",
      "d2.act -> ReLU\n",
      "d3 -> FusedDeconvBlock1D\n",
      "d3.deconv -> ConvTranspose1d\n",
      "d3.act -> ReLU\n",
      "d4 -> FusedDeconvBlock1D\n",
      "d4.deconv -> ConvTranspose1d\n",
      "d4.act -> ReLU\n",
      "out -> Conv1d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aryo\\AppData\\Local\\Temp\\ipykernel_23412\\830450021.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  G_check.load_state_dict(torch.load(fused_G_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "G_check = GeneratorCNNWGAN_Fused().to(device)\n",
    "G_check.load_state_dict(torch.load(fused_G_path, map_location=device))\n",
    "G_check.eval()\n",
    "\n",
    "for name, module in G_check.named_modules():\n",
    "    print(name, \"->\", module.__class__.__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "9e442771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Generator\n",
      "  Learnable parameters : 4,584,161\n",
      "  Buffers (BN stats)   : 5,552\n",
      "  TOTAL constants      : 4,589,713\n",
      "\n",
      "Fused Generator\n",
      "  Learnable parameters : 4,578,625\n",
      "  Buffers (BN stats)   : 0\n",
      "  TOTAL constants      : 4,578,625\n",
      "\n",
      "============================================================\n",
      "DIFFERENCE\n",
      "  Params removed  : 5,536\n",
      "  Buffers removed : 5,552\n",
      "  Total reduction : 11,088\n"
     ]
    }
   ],
   "source": [
    "def count_params_and_constants(model):\n",
    "    n_params = sum(p.numel() for p in model.parameters())\n",
    "    n_buffers = sum(b.numel() for b in model.buffers())\n",
    "    return n_params, n_buffers, n_params + n_buffers\n",
    "\n",
    "\n",
    "def pretty_count(name, model):\n",
    "    p, b, t = count_params_and_constants(model)\n",
    "    print(f\"{name}\")\n",
    "    print(f\"  Learnable parameters : {p:,}\")\n",
    "    print(f\"  Buffers (BN stats)   : {b:,}\")\n",
    "    print(f\"  TOTAL constants      : {t:,}\")\n",
    "    print()\n",
    "\n",
    "\n",
    "pretty_count(\"Original Generator\", G)\n",
    "pretty_count(\"Fused Generator\", G_fused)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "\n",
    "p0, b0, t0 = count_params_and_constants(G)\n",
    "p1, b1, t1 = count_params_and_constants(G_fused)\n",
    "\n",
    "print(\"DIFFERENCE\")\n",
    "print(f\"  Params removed  : {p0 - p1:,}\")\n",
    "print(f\"  Buffers removed : {b0 - b1:,}\")\n",
    "print(f\"  Total reduction : {t0 - t1:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d993a9",
   "metadata": {},
   "source": [
    "# Transform to Fixed Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "991a7978",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_CONFIGS = {\n",
    "    \"Q4.12\": dict(frac_bits=12, int_bits=4, dtype=np.int16),\n",
    "    \"Q10.10\": dict(frac_bits=10, int_bits=10, dtype=np.int32),\n",
    "}\n",
    "\n",
    "def float_to_q(x, frac_bits, int_bits, dtype):\n",
    "    scale = 1 << frac_bits\n",
    "    total_bits = int_bits + frac_bits\n",
    "    min_val = -(1 << (total_bits - 1))\n",
    "    max_val = (1 << (total_bits - 1)) - 1\n",
    "\n",
    "    xq = np.round(x * scale)\n",
    "    xq = np.clip(xq, min_val, max_val)\n",
    "    return xq.astype(dtype)\n",
    "\n",
    "def q_to_float(x, frac_bits):\n",
    "    return x.astype(np.float32) / (1 << frac_bits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6187c414",
   "metadata": {},
   "source": [
    "# Prepare for Hardware"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "e79e2bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_part(param_name):\n",
    "    if param_name.startswith((\"e1.\", \"e2.\", \"e3.\", \"e4.\")):\n",
    "        return \"encoder\"\n",
    "    elif param_name.startswith(\"bottleneck.\"):\n",
    "        return \"bottleneck\"\n",
    "    elif param_name.startswith((\"d1.\", \"d2.\", \"d3.\", \"d4.\")):\n",
    "        return \"decoder\"\n",
    "    elif param_name.startswith((\"out.\")):\n",
    "        return \"out\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown param group: {param_name}\")\n",
    "    \n",
    "def is_bias(name):\n",
    "    return name.endswith(\".bias\")\n",
    "\n",
    "def is_weight(name):\n",
    "    return name.endswith(\".weight\")\n",
    "\n",
    "def expand_bias(bias, repeat_n):\n",
    "    # bias shape: [C]\n",
    "    # output shape: [C * repeat_n]\n",
    "    return np.repeat(bias, repeat_n)\n",
    "\n",
    "def flatten_conv_weight(w):\n",
    "    # shape [x, y, z]\n",
    "    return w.reshape(-1)\n",
    "\n",
    "def flatten_deconv_weight(w):\n",
    "    # original shape [x, y, z]\n",
    "    # want order: x  z  y\n",
    "    w_perm = np.transpose(w, (0, 2, 1))\n",
    "    return w_perm.reshape(-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "376ea54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TYPE = \"Q10.10\" # \"Q4.12\", \"Q10.10\", or \"FLOAT\"\n",
    "\n",
    "TIME_REPEAT = {\n",
    "    \"encoder\":    [256, 128, 64, 32],\n",
    "    \"bottleneck\": [32]*8,\n",
    "    \"decoder\":    [64, 128, 256, 512],\n",
    "    \"out\":        [512]\n",
    "}\n",
    "\n",
    "if TYPE == \"FLOAT\":\n",
    "    MODE = \"FLOAT\"\n",
    "    DTYPE = np.float32\n",
    "else:\n",
    "    MODE = \"FIXED\"\n",
    "    cfg = Q_CONFIGS[TYPE]\n",
    "    FRAC_BITS = cfg[\"frac_bits\"]\n",
    "    INT_BITS  = cfg[\"int_bits\"]\n",
    "    DTYPE     = cfg[\"dtype\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "bb3ca370",
   "metadata": {},
   "outputs": [],
   "source": [
    "containers = {\n",
    "    part: {\n",
    "        \"weight\": {\"data\": [], \"offsets\": {}, \"cursor\": 0},\n",
    "        \"bias\":   {\"data\": [], \"offsets\": {}, \"cursor\": 0}\n",
    "    }\n",
    "    for part in [\"encoder\", \"bottleneck\", \"decoder\", \"out\"]\n",
    "}\n",
    "\n",
    "bias_layer_counter = {\n",
    "    \"encoder\": 0,\n",
    "    \"bottleneck\": 0,\n",
    "    \"decoder\": 0,\n",
    "    \"out\": 0\n",
    "}\n",
    "\n",
    "ref_param_sum = 0\n",
    "\n",
    "# ---- Global statistics (NO bias repetition) ----\n",
    "w_min = +np.inf\n",
    "w_max = -np.inf\n",
    "w_sum = 0\n",
    "w_cnt = 0\n",
    "\n",
    "b_min = +np.inf\n",
    "b_max = -np.inf\n",
    "b_sum = 0\n",
    "b_cnt = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd598c0",
   "metadata": {},
   "source": [
    "> .npy stores values using the container dtype, not the logical fixed-point width\n",
    "\n",
    "For FLOAT: [value0: 32 bits IEEE754][value1: 32 bits][value2: 32 bits]...\n",
    "\n",
    "For Q4.12: [value0: 16 bits][value1: 16 bits][value2: 16 bits]...\n",
    "\n",
    "For Q10.10: [value0: 32 bits][value1: 32 bits][value2: 32 bits]..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "1b0717c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[encoder   ][weight] e1.conv.weight                 -> flat_size=512\n",
      "[encoder   ][bias  ] e1.conv.bias                   -> flat_size=8192 (repeat=256)\n",
      "[encoder   ][weight] e2.conv.weight                 -> flat_size=32768\n",
      "[encoder   ][bias  ] e2.conv.bias                   -> flat_size=8192 (repeat=128)\n",
      "[encoder   ][weight] e3.conv.weight                 -> flat_size=131072\n",
      "[encoder   ][bias  ] e3.conv.bias                   -> flat_size=8192 (repeat=64)\n",
      "[encoder   ][weight] e4.conv.weight                 -> flat_size=524288\n",
      "[encoder   ][bias  ] e4.conv.bias                   -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.0.0.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.0.0.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.0.2.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.0.2.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.1.0.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.1.0.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.1.2.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.1.2.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.2.0.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.2.0.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.2.2.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.2.2.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.3.0.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.3.0.bias            -> flat_size=8192 (repeat=32)\n",
      "[bottleneck][weight] bottleneck.3.2.weight          -> flat_size=458752\n",
      "[bottleneck][bias  ] bottleneck.3.2.bias            -> flat_size=8192 (repeat=32)\n",
      "[decoder   ][weight] d1.deconv.weight               -> flat_size=131072 (deconv reorder)\n",
      "[decoder   ][bias  ] d1.deconv.bias                 -> flat_size=8192 (repeat=64)\n",
      "[decoder   ][weight] d2.deconv.weight               -> flat_size=65536 (deconv reorder)\n",
      "[decoder   ][bias  ] d2.deconv.bias                 -> flat_size=8192 (repeat=128)\n",
      "[decoder   ][weight] d3.deconv.weight               -> flat_size=16384 (deconv reorder)\n",
      "[decoder   ][bias  ] d3.deconv.bias                 -> flat_size=8192 (repeat=256)\n",
      "[decoder   ][weight] d4.deconv.weight               -> flat_size=4096 (deconv reorder)\n",
      "[decoder   ][bias  ] d4.deconv.bias                 -> flat_size=8192 (repeat=512)\n",
      "[out       ][weight] out.weight                     -> flat_size=112\n",
      "[out       ][bias  ] out.bias                       -> flat_size=512 (repeat=512)\n",
      "\n",
      "===== BIAS LAYER CONSUMPTION CHECK =====\n",
      "encoder   : used 4 / expected 4  [OK]\n",
      "bottleneck: used 8 / expected 8  [OK]\n",
      "decoder   : used 4 / expected 4  [OK]\n",
      "out       : used 1 / expected 1  [OK]\n"
     ]
    }
   ],
   "source": [
    "for name, param in G_fused.state_dict().items():\n",
    "    arr_f = param.cpu().numpy().astype(np.float32)\n",
    "    part = classify_part(name)\n",
    "    wtype = \"bias\" if is_bias(name) else \"weight\"\n",
    "\n",
    "    # Quantize or float\n",
    "    if MODE == \"FLOAT\":\n",
    "        arr = arr_f.astype(np.float32)\n",
    "        dtype_str = \"float32\"\n",
    "        meta = {}\n",
    "    else:\n",
    "        arr = float_to_q(\n",
    "            arr_f,\n",
    "            frac_bits=FRAC_BITS,\n",
    "            int_bits=INT_BITS,\n",
    "            dtype=DTYPE\n",
    "        )\n",
    "        dtype_str = TYPE\n",
    "        meta = {\"frac_bits\": FRAC_BITS, \"int_bits\": INT_BITS}\n",
    "\n",
    "    # ---- Apply ordering rules ----\n",
    "    if wtype == \"weight\":\n",
    "        ref_param_sum += arr.size\n",
    "        w_min = min(w_min, arr.min())\n",
    "        w_max = max(w_max, arr.max())\n",
    "        w_sum += arr.sum()\n",
    "        w_cnt += arr.size\n",
    "\n",
    "        if part == \"decoder\":\n",
    "            arr_flat = flatten_deconv_weight(arr)\n",
    "            reorder_note = \" (deconv reorder)\"\n",
    "        else:\n",
    "            arr_flat = flatten_conv_weight(arr)\n",
    "            reorder_note = \"\"\n",
    "    else:\n",
    "        # bias\n",
    "        ref_param_sum += arr.size\n",
    "        b_min = min(b_min, arr.min())\n",
    "        b_max = max(b_max, arr.max())\n",
    "        b_sum += arr.sum()\n",
    "        b_cnt += arr.size\n",
    "\n",
    "        layer_idx = bias_layer_counter[part]\n",
    "\n",
    "        if layer_idx >= len(TIME_REPEAT[part]):\n",
    "            raise RuntimeError(\n",
    "                f\"Too many bias layers in {part}: \"\n",
    "                f\"expected {len(TIME_REPEAT[part])}, got more\"\n",
    "            )\n",
    "\n",
    "        repeat_n = TIME_REPEAT[part][layer_idx]\n",
    "        bias_layer_counter[part] += 1\n",
    "        \n",
    "        arr_flat = expand_bias(arr, repeat_n)\n",
    "        reorder_note = f\" (repeat={repeat_n})\"\n",
    "\n",
    "    # ---- PRINT SIZE CHECK (NEW) ----\n",
    "    print(\n",
    "        f\"[{part:10}][{wtype:6}] {name:30} -> \"\n",
    "        f\"flat_size={arr_flat.size}{reorder_note}\"\n",
    "    )\n",
    "\n",
    "    c = containers[part][wtype]\n",
    "    size = arr_flat.size\n",
    "\n",
    "    c[\"offsets\"][name] = {\n",
    "        \"offset\": int(c[\"cursor\"]),\n",
    "        \"shape\": list(arr_flat.shape),\n",
    "        \"dtype\": dtype_str,\n",
    "        **meta\n",
    "    }\n",
    "\n",
    "    c[\"data\"].append(arr_flat)\n",
    "    c[\"cursor\"] += size\n",
    "\n",
    "print(\"\\n===== BIAS LAYER CONSUMPTION CHECK =====\")\n",
    "for part, cnt in bias_layer_counter.items():\n",
    "    expected = len(TIME_REPEAT[part])\n",
    "    status = \"OK\" if cnt == expected else \"MISMATCH\"\n",
    "    print(f\"{part:10}: used {cnt} / expected {expected}  [{status}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "a6540ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== PARAMETER CONSISTENCY CHECK =====\n",
      "Expected total (before split): 4578625\n",
      "Recomputed total (no bias repeat): 4578625\n",
      " PARAM COUNT MATCHES (model preserved)\n",
      "\n",
      "===== GLOBAL PARAMETER STATISTICS =====\n",
      "Weights:\n",
      "  min = -607\n",
      "  max = 651\n",
      "  avg = -0.3141536796612481\n",
      "  count = 4575856\n",
      "\n",
      "Biases:\n",
      "  min = -2853\n",
      "  max = 2280\n",
      "  avg = 56.026002166847235\n",
      "  count = 2769\n",
      "\n",
      "Combined:\n",
      "  min = -2853\n",
      "  max = 2280\n",
      "  avg = -0.280081028693112\n",
      "  count = 4578625\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n===== PARAMETER CONSISTENCY CHECK =====\")\n",
    "print(\"Expected total (before split): 4578625\")\n",
    "print(\"Recomputed total (no bias repeat):\", ref_param_sum)\n",
    "\n",
    "if ref_param_sum == 4578625:\n",
    "    print(\" PARAM COUNT MATCHES (model preserved)\")\n",
    "else:\n",
    "    print(\" PARAM COUNT MISMATCH  CHECK SPLIT LOGIC\")\n",
    "\n",
    "print(\"\\n===== GLOBAL PARAMETER STATISTICS =====\")\n",
    "\n",
    "print(f\"Weights:\")\n",
    "print(f\"  min = {w_min}\")\n",
    "print(f\"  max = {w_max}\")\n",
    "print(f\"  avg = {w_sum / w_cnt}\")\n",
    "print(f\"  count = {w_cnt}\")\n",
    "\n",
    "print(f\"\\nBiases:\")\n",
    "print(f\"  min = {b_min}\")\n",
    "print(f\"  max = {b_max}\")\n",
    "print(f\"  avg = {b_sum / b_cnt}\")\n",
    "print(f\"  count = {b_cnt}\")\n",
    "\n",
    "print(f\"\\nCombined:\")\n",
    "total_cnt = w_cnt + b_cnt\n",
    "total_sum = w_sum + b_sum\n",
    "print(f\"  min = {min(w_min, b_min)}\")\n",
    "print(f\"  max = {max(w_max, b_max)}\")\n",
    "print(f\"  avg = {total_sum / total_cnt}\")\n",
    "print(f\"  count = {total_cnt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "2030ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = os.path.abspath(\n",
    "    os.path.join(os.path.dirname(fused_G_path), \"..\",\n",
    "    f\"G_d{DATA_MODE}_{TYPE}\")\n",
    ")\n",
    "os.makedirs(base_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "0a737520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mem(path, data):\n",
    "    with open(path, \"w\") as f:\n",
    "        for v in data:\n",
    "            if data.dtype == np.int16:\n",
    "                f.write(f\"{int(v) & 0xFFFF:04X}\\n\")\n",
    "            elif data.dtype == np.int32:\n",
    "                f.write(f\"{int(v) & 0xFFFFFFFF:08X}\\n\")\n",
    "            elif data.dtype == np.float32:\n",
    "                bits = np.frombuffer(\n",
    "                    np.float32(v).tobytes(), dtype=np.uint32\n",
    "                )[0]\n",
    "                f.write(f\"{bits:08X}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "b2b80d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved encoder/weight: 688640\n",
      "Saved encoder/bias: 32768\n",
      "Saved bottleneck/weight: 3670016\n",
      "Saved bottleneck/bias: 65536\n",
      "Saved decoder/weight: 217088\n",
      "Saved decoder/bias: 32768\n",
      "Saved out/weight: 112\n",
      "Saved out/bias: 512\n"
     ]
    }
   ],
   "source": [
    "# Each line = 1 memory word\n",
    "# Each line is container size (16 or 32 based on int32, int16, or float32)\n",
    "# line 0  -> address 0\n",
    "# line 1  -> address 1\n",
    "# line 2  -> address 2\n",
    "# ...\n",
    "\n",
    "# For Q10.10, hardware does: real_value = signed_int / (1 << 10);\n",
    "# Masking int(v) & 0xFFFFFFFF is needed for twos complement truncation, so:\n",
    "# -1  FFFFFFFF\n",
    "# -512  \n",
    "\n",
    "# Value ordering\n",
    "# layer_0.weight\n",
    "# layer_0.bias\n",
    "# layer_1.weight\n",
    "# layer_1.bias\n",
    "# ...\n",
    "\n",
    "mem_path = os.path.join(\n",
    "    os.path.dirname(fused_G_path),\n",
    "    f\"fused_cnn_G_d{DATA_MODE}b_weights_{TYPE}.mem\"\n",
    ")\n",
    "\n",
    "for part in containers:\n",
    "    for wtype in [\"weight\", \"bias\"]:\n",
    "        c = containers[part][wtype]\n",
    "        data = np.concatenate(c[\"data\"])\n",
    "\n",
    "        json_path = os.path.join(\n",
    "            base_dir,\n",
    "            f\"G_d{DATA_MODE}_{TYPE}_{part}_{wtype}.json\"\n",
    "        )\n",
    "        mem_path = os.path.join(\n",
    "            base_dir,\n",
    "            f\"G_d{DATA_MODE}_{TYPE}_{part}_{wtype}.mem\"\n",
    "        )\n",
    "\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(c[\"offsets\"], f, indent=2)\n",
    "\n",
    "        write_mem(mem_path, data)\n",
    "\n",
    "        print(f\"Saved {part}/{wtype}: {data.size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "3d632165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JSON MAPPING\n",
    "\n",
    "# EXAMPLE 1\n",
    "# \"e1.conv.weight\": {\n",
    "#     \"offset\": 0,\n",
    "#     \"shape\": [\n",
    "#       32,\n",
    "#       1,\n",
    "#       16\n",
    "#     ],\n",
    "#     \"dtype\": \"Q10.10\",\n",
    "#     \"frac_bits\": 10,\n",
    "#     \"int_bits\": 10\n",
    "#   },\n",
    "\n",
    "# .mem line   Tensor index\n",
    "#    \n",
    "# 0           [0][0][0]\n",
    "# 1           [0][0][1]\n",
    "# ...\n",
    "# 15          [0][0][15]\n",
    "# 16          [1][0][0]\n",
    "# ...\n",
    "# 511         [31][0][15]\n",
    "\n",
    "# EXAMPLE 2\n",
    "# \"e2.conv.weight\": {\n",
    "#   \"offset\": 544,\n",
    "#   \"shape\": [64, 32, 16]\n",
    "# }\n",
    "\n",
    "# Starts at offset = 544 -> .mem line 544  e2.conv.weight[0][0][0]\n",
    "# Number of values -> 64  32  16 = 32768 values\n",
    "# Occupies:\n",
    "# .mem lines 544  544 + 32768  1\n",
    "# = lines 544  33311"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99854831",
   "metadata": {},
   "source": [
    "## How weights map to hardware\n",
    "```bash\n",
    "Tile 0:\n",
    "  BRAM 0: 256 values\n",
    "  BRAM 1: 256 values\n",
    "  ...\n",
    "  BRAM 15: 256 values\n",
    "\n",
    "Tile 1:\n",
    "  BRAM 0: 256 values\n",
    "  ...\n",
    "```\n",
    "\n",
    "```python\n",
    "weight_layer[batch][tile][bram_id][addr]\n",
    "  =\n",
    "weights_flat[\n",
    "    tile * (16*256)\n",
    "  + bram_id * 256\n",
    "  + addr\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "2c9afb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# int base = batch * (4 * 16 * 256);\n",
    "\n",
    "# for (int tile = 0; tile < 4; tile++) {\n",
    "#     int tile_base = base + tile * (16 * 256);\n",
    "#     int addr_offset = tile * 256;\n",
    "\n",
    "#     for (int bram = 0; bram < 16; bram++) {\n",
    "#         int bram_base = tile_base + bram * 256;\n",
    "\n",
    "#         for (int addr = 0; addr < 256; addr++) {\n",
    "#             uint32_t value = weights_flat[bram_base + addr];\n",
    "\n",
    "#             write_weight_bram(\n",
    "#                 bram,                 // BRAM ID\n",
    "#                 addr_offset + addr,   // BRAM address\n",
    "#                 value                 // int32 / int16 / float\n",
    "#             );\n",
    "#         }\n",
    "#     }\n",
    "# }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
